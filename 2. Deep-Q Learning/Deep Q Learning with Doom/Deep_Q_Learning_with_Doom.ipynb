{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Q Learning with Doom.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCGnulHacOLL",
        "colab_type": "code",
        "outputId": "d3bfeac7-ad2c-4c20-e439-0905edf598a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%bash\n",
        "# Install deps from \n",
        "# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n",
        "\n",
        "apt-get update\n",
        "\n",
        "apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n",
        "nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n",
        "libopenal-dev timidity libwildmidi-dev unzip\n",
        "\n",
        "# Boost libraries\n",
        "apt-get install libboost-all-dev\n",
        "\n",
        "# Lua binding dependencies\n",
        "apt-get install liblua5.1-dev"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [93.1 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [32.9 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [958 kB]\n",
            "Get:17 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,837 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,253 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,392 kB]\n",
            "Get:20 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [886 kB]\n",
            "Fetched 6,725 kB in 4s (1,853 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "libjpeg-dev is already the newest version (8c-2ubuntu8).\n",
            "libjpeg-dev set to manually installed.\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.7).\n",
            "libbz2-dev is already the newest version (1.0.6-8.1ubuntu0.2).\n",
            "libbz2-dev set to manually installed.\n",
            "tar is already the newest version (1.29b-2ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  autoconf automake autopoint autotools-dev debhelper dh-autoreconf\n",
            "  dh-strip-nondeterminism file freepats gettext gettext-base gir1.2-atk-1.0\n",
            "  gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-gtk-2.0 gir1.2-ibus-1.0\n",
            "  gir1.2-pango-1.0 intltool-debian libarchive-cpio-perl libarchive-zip-perl\n",
            "  libatk1.0-dev libaudio2 libcairo-script-interpreter2 libcairo2-dev\n",
            "  libcapnp-0.6.1 libdbus-1-dev libfile-stripnondeterminism-perl libfluidsynth1\n",
            "  libgdk-pixbuf2.0-dev libibus-1.0-5 libibus-1.0-dev libmagic-mgc libmagic1\n",
            "  libmail-sendmail-perl libmirclient-dev libmirclient9 libmircommon-dev\n",
            "  libmircommon7 libmircookie-dev libmircookie2 libmircore-dev libmircore1\n",
            "  libmirprotobuf3 libpango1.0-dev libpangoxft-1.0-0 libpixman-1-dev\n",
            "  libprotobuf-dev libprotobuf-lite10 libpulse-dev libpulse-mainloop-glib0\n",
            "  libsigsegv2 libsndio-dev libsys-hostname-long-perl libtimedate-perl libtool\n",
            "  libudev-dev libwildmidi-config libwildmidi2 libxcb-shm0-dev\n",
            "  libxcomposite-dev libxcursor-dev libxinerama-dev libxkbcommon-dev\n",
            "  libxml2-utils libxrandr-dev libxv-dev m4 po-debconf timidity-daemon\n",
            "  x11proto-composite-dev x11proto-randr-dev x11proto-xinerama-dev\n",
            "Suggested packages:\n",
            "  autoconf-archive gnu-standards autoconf-doc dh-make dwz gettext-doc\n",
            "  libasprintf-dev libgettextpo-dev nas libcairo2-doc fluidr3mono-gm-soundfont\n",
            "  | timgm6mb-soundfont | fluid-soundfont-gm libgtk2.0-doc imagemagick\n",
            "  libpango1.0-doc libtool-doc gcj-jdk m4-doc libmail-box-perl\n",
            "  fluid-soundfont-gm fluid-soundfont-gs pmidi\n",
            "The following NEW packages will be installed:\n",
            "  autoconf automake autopoint autotools-dev debhelper dh-autoreconf\n",
            "  dh-strip-nondeterminism file freepats gettext gettext-base gir1.2-atk-1.0\n",
            "  gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-gtk-2.0 gir1.2-ibus-1.0\n",
            "  gir1.2-pango-1.0 intltool-debian libarchive-cpio-perl libarchive-zip-perl\n",
            "  libatk1.0-dev libaudio2 libcairo-script-interpreter2 libcairo2-dev\n",
            "  libcapnp-0.6.1 libdbus-1-dev libfile-stripnondeterminism-perl\n",
            "  libfluidsynth-dev libfluidsynth1 libgdk-pixbuf2.0-dev libgme-dev\n",
            "  libgtk2.0-dev libibus-1.0-5 libibus-1.0-dev libmagic-mgc libmagic1\n",
            "  libmail-sendmail-perl libmirclient-dev libmirclient9 libmircommon-dev\n",
            "  libmircommon7 libmircookie-dev libmircookie2 libmircore-dev libmircore1\n",
            "  libmirprotobuf3 libopenal-dev libpango1.0-dev libpangoxft-1.0-0\n",
            "  libpixman-1-dev libprotobuf-dev libprotobuf-lite10 libpulse-dev\n",
            "  libpulse-mainloop-glib0 libsdl2-dev libsigsegv2 libsndio-dev\n",
            "  libsys-hostname-long-perl libtimedate-perl libtool libudev-dev\n",
            "  libwildmidi-config libwildmidi-dev libwildmidi2 libxcb-shm0-dev\n",
            "  libxcomposite-dev libxcursor-dev libxinerama-dev libxkbcommon-dev\n",
            "  libxml2-utils libxrandr-dev libxv-dev m4 nasm po-debconf timidity\n",
            "  timidity-daemon x11proto-composite-dev x11proto-randr-dev\n",
            "  x11proto-xinerama-dev\n",
            "0 upgraded, 80 newly installed, 0 to remove and 44 not upgraded.\n",
            "Need to get 43.2 MB of archives.\n",
            "After this operation, 114 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gettext-base amd64 0.19.8.1-6ubuntu0.3 [113 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigsegv2 amd64 2.12-1 [14.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 m4 amd64 1.4.18-1 [197 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 autoconf all 2.69-11 [322 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 automake all 1:1.15.1-3ubuntu2 [509 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 autopoint all 0.19.8.1-6ubuntu0.3 [426 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtool all 2.4.6-2 [194 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 dh-autoreconf all 17 [15.8 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libarchive-zip-perl all 1.60-1ubuntu0.1 [84.6 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfile-stripnondeterminism-perl all 0.040-1.1~build1 [13.8 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 dh-strip-nondeterminism all 0.040-1.1~build1 [5,208 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gettext amd64 0.19.8.1-6ubuntu0.3 [1,293 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 intltool-debian all 0.35.0+20060710.4 [24.9 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 po-debconf all 1.0.20 [232 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 debhelper all 11.1.6ubuntu2 [902 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/universe amd64 freepats all 20060219-1 [29.0 MB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-atk-1.0 amd64 2.28.1-1 [17.8 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-freedesktop amd64 1.56.1-1 [9,080 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-gdkpixbuf-2.0 amd64 2.36.11-2 [7,748 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpangoxft-1.0-0 amd64 1.40.14-1ubuntu0.1 [15.0 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gir1.2-pango-1.0 amd64 1.40.14-1ubuntu0.1 [21.6 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-gtk-2.0 amd64 2.24.32-1ubuntu1 [172 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libibus-1.0-5 amd64 1.5.17-3ubuntu5.3 [133 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gir1.2-ibus-1.0 amd64 1.5.17-3ubuntu5.3 [66.5 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 libarchive-cpio-perl all 0.10-1 [9,644 B]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk1.0-dev amd64 2.28.1-1 [79.9 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/main amd64 libaudio2 amd64 1.9.4-6 [50.3 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcairo-script-interpreter2 amd64 1.15.10-2ubuntu0.1 [53.5 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpixman-1-dev amd64 0.34.0-2 [244 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxcb-shm0-dev amd64 1.13-2~ubuntu18.04 [6,684 B]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcairo2-dev amd64 1.15.10-2ubuntu0.1 [626 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcapnp-0.6.1 amd64 0.6.1-1ubuntu1 [658 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdbus-1-dev amd64 1.12.2-1ubuntu1.1 [165 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libfluidsynth1 amd64 1.1.9-1 [137 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libfluidsynth-dev amd64 1.1.9-1 [19.7 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgdk-pixbuf2.0-dev amd64 2.36.11-2 [46.8 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgme-dev amd64 0.6.2-1 [5,796 B]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpango1.0-dev amd64 1.40.14-1ubuntu0.1 [288 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-xinerama-dev all 2018.4-4 [2,628 B]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxinerama-dev amd64 2:1.1.3-1 [8,404 B]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-randr-dev all 2018.4-4 [2,620 B]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxrandr-dev amd64 2:1.5.1-1 [24.0 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcursor-dev amd64 1:1.1.15-1 [26.5 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-composite-dev all 1:2018.4-4 [2,620 B]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcomposite-dev amd64 1:0.4.4-2 [9,136 B]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxml2-utils amd64 2.9.4+dfsg1-6.1ubuntu1.3 [35.9 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-dev amd64 2.24.32-1ubuntu1 [2,652 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libibus-1.0-dev amd64 1.5.17-3ubuntu5.3 [145 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsys-hostname-long-perl all 1.5-1 [11.7 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmail-sendmail-perl all 0.80-1 [22.6 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircore1 amd64 0.31.1-0ubuntu1 [26.5 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircommon7 amd64 0.31.1-0ubuntu1 [73.9 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf-lite10 amd64 3.0.0-9.1ubuntu1 [97.7 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirprotobuf3 amd64 0.31.1-0ubuntu1 [127 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirclient9 amd64 0.31.1-0ubuntu1 [199 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircore-dev amd64 0.31.1-0ubuntu1 [21.7 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf-dev amd64 3.0.0-9.1ubuntu1 [959 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxkbcommon-dev amd64 0.8.2-1~ubuntu18.04.1 [150 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircommon-dev amd64 0.31.1-0ubuntu1 [13.9 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircookie2 amd64 0.31.1-0ubuntu1 [19.7 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircookie-dev amd64 0.31.1-0ubuntu1 [4,392 B]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirclient-dev amd64 0.31.1-0ubuntu1 [47.8 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenal-dev amd64 1:1.18.2-2 [20.9 kB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-mainloop-glib0 amd64 1:11.1-1ubuntu7.8 [22.1 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-dev amd64 1:11.1-1ubuntu7.8 [81.5 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsndio-dev amd64 1.1.0-3 [13.3 kB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libudev-dev amd64 237-3ubuntu10.41 [19.1 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxv-dev amd64 2:1.0.11-1 [32.5 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsdl2-dev amd64 2.0.8+dfsg1-1ubuntu1.18.04.4 [683 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libwildmidi-config all 0.4.2-1 [7,212 B]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libwildmidi2 amd64 0.4.2-1 [55.8 kB]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libwildmidi-dev amd64 0.4.2-1 [86.4 kB]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu bionic/universe amd64 nasm amd64 2.13.02-0.1 [359 kB]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu bionic/universe amd64 timidity amd64 2.13.2-41 [585 kB]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu bionic/universe amd64 timidity-daemon all 2.13.2-41 [5,984 B]\n",
            "Fetched 43.2 MB in 4s (10.7 MB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 144328 files and directories currently installed.)\r\n",
            "Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\r\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\r\n",
            "Selecting previously unselected package libmagic1:amd64.\r\n",
            "Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\r\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\r\n",
            "Selecting previously unselected package file.\r\n",
            "Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\r\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\r\n",
            "Selecting previously unselected package gettext-base.\r\n",
            "Preparing to unpack .../03-gettext-base_0.19.8.1-6ubuntu0.3_amd64.deb ...\r\n",
            "Unpacking gettext-base (0.19.8.1-6ubuntu0.3) ...\r\n",
            "Selecting previously unselected package libsigsegv2:amd64.\r\n",
            "Preparing to unpack .../04-libsigsegv2_2.12-1_amd64.deb ...\r\n",
            "Unpacking libsigsegv2:amd64 (2.12-1) ...\r\n",
            "Selecting previously unselected package m4.\r\n",
            "Preparing to unpack .../05-m4_1.4.18-1_amd64.deb ...\r\n",
            "Unpacking m4 (1.4.18-1) ...\r\n",
            "Selecting previously unselected package autoconf.\r\n",
            "Preparing to unpack .../06-autoconf_2.69-11_all.deb ...\r\n",
            "Unpacking autoconf (2.69-11) ...\r\n",
            "Selecting previously unselected package autotools-dev.\r\n",
            "Preparing to unpack .../07-autotools-dev_20180224.1_all.deb ...\r\n",
            "Unpacking autotools-dev (20180224.1) ...\r\n",
            "Selecting previously unselected package automake.\r\n",
            "Preparing to unpack .../08-automake_1%3a1.15.1-3ubuntu2_all.deb ...\r\n",
            "Unpacking automake (1:1.15.1-3ubuntu2) ...\r\n",
            "Selecting previously unselected package autopoint.\r\n",
            "Preparing to unpack .../09-autopoint_0.19.8.1-6ubuntu0.3_all.deb ...\r\n",
            "Unpacking autopoint (0.19.8.1-6ubuntu0.3) ...\r\n",
            "Selecting previously unselected package libtool.\r\n",
            "Preparing to unpack .../10-libtool_2.4.6-2_all.deb ...\r\n",
            "Unpacking libtool (2.4.6-2) ...\r\n",
            "Selecting previously unselected package dh-autoreconf.\r\n",
            "Preparing to unpack .../11-dh-autoreconf_17_all.deb ...\r\n",
            "Unpacking dh-autoreconf (17) ...\r\n",
            "Selecting previously unselected package libarchive-zip-perl.\r\n",
            "Preparing to unpack .../12-libarchive-zip-perl_1.60-1ubuntu0.1_all.deb ...\r\n",
            "Unpacking libarchive-zip-perl (1.60-1ubuntu0.1) ...\r\n",
            "Selecting previously unselected package libfile-stripnondeterminism-perl.\r\n",
            "Preparing to unpack .../13-libfile-stripnondeterminism-perl_0.040-1.1~build1_all.deb ...\r\n",
            "Unpacking libfile-stripnondeterminism-perl (0.040-1.1~build1) ...\r\n",
            "Selecting previously unselected package libtimedate-perl.\r\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\r\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\r\n",
            "Selecting previously unselected package dh-strip-nondeterminism.\r\n",
            "Preparing to unpack .../15-dh-strip-nondeterminism_0.040-1.1~build1_all.deb ...\r\n",
            "Unpacking dh-strip-nondeterminism (0.040-1.1~build1) ...\r\n",
            "Selecting previously unselected package gettext.\r\n",
            "Preparing to unpack .../16-gettext_0.19.8.1-6ubuntu0.3_amd64.deb ...\r\n",
            "Unpacking gettext (0.19.8.1-6ubuntu0.3) ...\r\n",
            "Selecting previously unselected package intltool-debian.\r\n",
            "Preparing to unpack .../17-intltool-debian_0.35.0+20060710.4_all.deb ...\r\n",
            "Unpacking intltool-debian (0.35.0+20060710.4) ...\r\n",
            "Selecting previously unselected package po-debconf.\r\n",
            "Preparing to unpack .../18-po-debconf_1.0.20_all.deb ...\r\n",
            "Unpacking po-debconf (1.0.20) ...\r\n",
            "Selecting previously unselected package debhelper.\r\n",
            "Preparing to unpack .../19-debhelper_11.1.6ubuntu2_all.deb ...\r\n",
            "Unpacking debhelper (11.1.6ubuntu2) ...\r\n",
            "Selecting previously unselected package freepats.\r\n",
            "Preparing to unpack .../20-freepats_20060219-1_all.deb ...\r\n",
            "Unpacking freepats (20060219-1) ...\r\n",
            "Selecting previously unselected package gir1.2-atk-1.0:amd64.\r\n",
            "Preparing to unpack .../21-gir1.2-atk-1.0_2.28.1-1_amd64.deb ...\r\n",
            "Unpacking gir1.2-atk-1.0:amd64 (2.28.1-1) ...\r\n",
            "Selecting previously unselected package gir1.2-freedesktop:amd64.\r\n",
            "Preparing to unpack .../22-gir1.2-freedesktop_1.56.1-1_amd64.deb ...\r\n",
            "Unpacking gir1.2-freedesktop:amd64 (1.56.1-1) ...\r\n",
            "Selecting previously unselected package gir1.2-gdkpixbuf-2.0:amd64.\r\n",
            "Preparing to unpack .../23-gir1.2-gdkpixbuf-2.0_2.36.11-2_amd64.deb ...\r\n",
            "Unpacking gir1.2-gdkpixbuf-2.0:amd64 (2.36.11-2) ...\r\n",
            "Selecting previously unselected package libpangoxft-1.0-0:amd64.\r\n",
            "Preparing to unpack .../24-libpangoxft-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking libpangoxft-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\r\n",
            "Selecting previously unselected package gir1.2-pango-1.0:amd64.\r\n",
            "Preparing to unpack .../25-gir1.2-pango-1.0_1.40.14-1ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking gir1.2-pango-1.0:amd64 (1.40.14-1ubuntu0.1) ...\r\n",
            "Selecting previously unselected package gir1.2-gtk-2.0.\r\n",
            "Preparing to unpack .../26-gir1.2-gtk-2.0_2.24.32-1ubuntu1_amd64.deb ...\r\n",
            "Unpacking gir1.2-gtk-2.0 (2.24.32-1ubuntu1) ...\r\n",
            "Selecting previously unselected package libibus-1.0-5:amd64.\r\n",
            "Preparing to unpack .../27-libibus-1.0-5_1.5.17-3ubuntu5.3_amd64.deb ...\r\n",
            "Unpacking libibus-1.0-5:amd64 (1.5.17-3ubuntu5.3) ...\r\n",
            "Selecting previously unselected package gir1.2-ibus-1.0:amd64.\r\n",
            "Preparing to unpack .../28-gir1.2-ibus-1.0_1.5.17-3ubuntu5.3_amd64.deb ...\r\n",
            "Unpacking gir1.2-ibus-1.0:amd64 (1.5.17-3ubuntu5.3) ...\r\n",
            "Selecting previously unselected package libarchive-cpio-perl.\r\n",
            "Preparing to unpack .../29-libarchive-cpio-perl_0.10-1_all.deb ...\r\n",
            "Unpacking libarchive-cpio-perl (0.10-1) ...\r\n",
            "Selecting previously unselected package libatk1.0-dev:amd64.\r\n",
            "Preparing to unpack .../30-libatk1.0-dev_2.28.1-1_amd64.deb ...\r\n",
            "Unpacking libatk1.0-dev:amd64 (2.28.1-1) ...\r\n",
            "Selecting previously unselected package libaudio2:amd64.\r\n",
            "Preparing to unpack .../31-libaudio2_1.9.4-6_amd64.deb ...\r\n",
            "Unpacking libaudio2:amd64 (1.9.4-6) ...\r\n",
            "Selecting previously unselected package libcairo-script-interpreter2:amd64.\r\n",
            "Preparing to unpack .../32-libcairo-script-interpreter2_1.15.10-2ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking libcairo-script-interpreter2:amd64 (1.15.10-2ubuntu0.1) ...\r\n",
            "Selecting previously unselected package libpixman-1-dev:amd64.\r\n",
            "Preparing to unpack .../33-libpixman-1-dev_0.34.0-2_amd64.deb ...\r\n",
            "Unpacking libpixman-1-dev:amd64 (0.34.0-2) ...\r\n",
            "Selecting previously unselected package libxcb-shm0-dev:amd64.\r\n",
            "Preparing to unpack .../34-libxcb-shm0-dev_1.13-2~ubuntu18.04_amd64.deb ...\r\n",
            "Unpacking libxcb-shm0-dev:amd64 (1.13-2~ubuntu18.04) ...\r\n",
            "Selecting previously unselected package libcairo2-dev:amd64.\r\n",
            "Preparing to unpack .../35-libcairo2-dev_1.15.10-2ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking libcairo2-dev:amd64 (1.15.10-2ubuntu0.1) ...\r\n",
            "Selecting previously unselected package libcapnp-0.6.1:amd64.\r\n",
            "Preparing to unpack .../36-libcapnp-0.6.1_0.6.1-1ubuntu1_amd64.deb ...\r\n",
            "Unpacking libcapnp-0.6.1:amd64 (0.6.1-1ubuntu1) ...\r\n",
            "Selecting previously unselected package libdbus-1-dev:amd64.\r\n",
            "Preparing to unpack .../37-libdbus-1-dev_1.12.2-1ubuntu1.1_amd64.deb ...\r\n",
            "Unpacking libdbus-1-dev:amd64 (1.12.2-1ubuntu1.1) ...\r\n",
            "Selecting previously unselected package libfluidsynth1:amd64.\r\n",
            "Preparing to unpack .../38-libfluidsynth1_1.1.9-1_amd64.deb ...\r\n",
            "Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\r\n",
            "Selecting previously unselected package libfluidsynth-dev:amd64.\r\n",
            "Preparing to unpack .../39-libfluidsynth-dev_1.1.9-1_amd64.deb ...\r\n",
            "Unpacking libfluidsynth-dev:amd64 (1.1.9-1) ...\r\n",
            "Selecting previously unselected package libgdk-pixbuf2.0-dev.\r\n",
            "Preparing to unpack .../40-libgdk-pixbuf2.0-dev_2.36.11-2_amd64.deb ...\r\n",
            "Unpacking libgdk-pixbuf2.0-dev (2.36.11-2) ...\r\n",
            "Selecting previously unselected package libgme-dev:amd64.\r\n",
            "Preparing to unpack .../41-libgme-dev_0.6.2-1_amd64.deb ...\r\n",
            "Unpacking libgme-dev:amd64 (0.6.2-1) ...\r\n",
            "Selecting previously unselected package libpango1.0-dev.\r\n",
            "Preparing to unpack .../42-libpango1.0-dev_1.40.14-1ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking libpango1.0-dev (1.40.14-1ubuntu0.1) ...\r\n",
            "Selecting previously unselected package x11proto-xinerama-dev.\r\n",
            "Preparing to unpack .../43-x11proto-xinerama-dev_2018.4-4_all.deb ...\r\n",
            "Unpacking x11proto-xinerama-dev (2018.4-4) ...\r\n",
            "Selecting previously unselected package libxinerama-dev:amd64.\r\n",
            "Preparing to unpack .../44-libxinerama-dev_2%3a1.1.3-1_amd64.deb ...\r\n",
            "Unpacking libxinerama-dev:amd64 (2:1.1.3-1) ...\r\n",
            "Selecting previously unselected package x11proto-randr-dev.\r\n",
            "Preparing to unpack .../45-x11proto-randr-dev_2018.4-4_all.deb ...\r\n",
            "Unpacking x11proto-randr-dev (2018.4-4) ...\r\n",
            "Selecting previously unselected package libxrandr-dev:amd64.\r\n",
            "Preparing to unpack .../46-libxrandr-dev_2%3a1.5.1-1_amd64.deb ...\r\n",
            "Unpacking libxrandr-dev:amd64 (2:1.5.1-1) ...\r\n",
            "Selecting previously unselected package libxcursor-dev:amd64.\r\n",
            "Preparing to unpack .../47-libxcursor-dev_1%3a1.1.15-1_amd64.deb ...\r\n",
            "Unpacking libxcursor-dev:amd64 (1:1.1.15-1) ...\r\n",
            "Selecting previously unselected package x11proto-composite-dev.\r\n",
            "Preparing to unpack .../48-x11proto-composite-dev_1%3a2018.4-4_all.deb ...\r\n",
            "Unpacking x11proto-composite-dev (1:2018.4-4) ...\r\n",
            "Selecting previously unselected package libxcomposite-dev:amd64.\r\n",
            "Preparing to unpack .../49-libxcomposite-dev_1%3a0.4.4-2_amd64.deb ...\r\n",
            "Unpacking libxcomposite-dev:amd64 (1:0.4.4-2) ...\r\n",
            "Selecting previously unselected package libxml2-utils.\r\n",
            "Preparing to unpack .../50-libxml2-utils_2.9.4+dfsg1-6.1ubuntu1.3_amd64.deb ...\r\n",
            "Unpacking libxml2-utils (2.9.4+dfsg1-6.1ubuntu1.3) ...\r\n",
            "Selecting previously unselected package libgtk2.0-dev.\r\n",
            "Preparing to unpack .../51-libgtk2.0-dev_2.24.32-1ubuntu1_amd64.deb ...\r\n",
            "Unpacking libgtk2.0-dev (2.24.32-1ubuntu1) ...\r\n",
            "Selecting previously unselected package libibus-1.0-dev:amd64.\r\n",
            "Preparing to unpack .../52-libibus-1.0-dev_1.5.17-3ubuntu5.3_amd64.deb ...\r\n",
            "Unpacking libibus-1.0-dev:amd64 (1.5.17-3ubuntu5.3) ...\r\n",
            "Selecting previously unselected package libsys-hostname-long-perl.\r\n",
            "Preparing to unpack .../53-libsys-hostname-long-perl_1.5-1_all.deb ...\r\n",
            "Unpacking libsys-hostname-long-perl (1.5-1) ...\r\n",
            "Selecting previously unselected package libmail-sendmail-perl.\r\n",
            "Preparing to unpack .../54-libmail-sendmail-perl_0.80-1_all.deb ...\r\n",
            "Unpacking libmail-sendmail-perl (0.80-1) ...\r\n",
            "Selecting previously unselected package libmircore1:amd64.\r\n",
            "Preparing to unpack .../55-libmircore1_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmircore1:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libmircommon7:amd64.\r\n",
            "Preparing to unpack .../56-libmircommon7_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmircommon7:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libprotobuf-lite10:amd64.\r\n",
            "Preparing to unpack .../57-libprotobuf-lite10_3.0.0-9.1ubuntu1_amd64.deb ...\r\n",
            "Unpacking libprotobuf-lite10:amd64 (3.0.0-9.1ubuntu1) ...\r\n",
            "Selecting previously unselected package libmirprotobuf3:amd64.\r\n",
            "Preparing to unpack .../58-libmirprotobuf3_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmirprotobuf3:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libmirclient9:amd64.\r\n",
            "Preparing to unpack .../59-libmirclient9_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmirclient9:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libmircore-dev:amd64.\r\n",
            "Preparing to unpack .../60-libmircore-dev_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmircore-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libprotobuf-dev:amd64.\r\n",
            "Preparing to unpack .../61-libprotobuf-dev_3.0.0-9.1ubuntu1_amd64.deb ...\r\n",
            "Unpacking libprotobuf-dev:amd64 (3.0.0-9.1ubuntu1) ...\r\n",
            "Selecting previously unselected package libxkbcommon-dev:amd64.\r\n",
            "Preparing to unpack .../62-libxkbcommon-dev_0.8.2-1~ubuntu18.04.1_amd64.deb ...\r\n",
            "Unpacking libxkbcommon-dev:amd64 (0.8.2-1~ubuntu18.04.1) ...\r\n",
            "Selecting previously unselected package libmircommon-dev:amd64.\r\n",
            "Preparing to unpack .../63-libmircommon-dev_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmircommon-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libmircookie2:amd64.\r\n",
            "Preparing to unpack .../64-libmircookie2_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmircookie2:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libmircookie-dev:amd64.\r\n",
            "Preparing to unpack .../65-libmircookie-dev_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmircookie-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libmirclient-dev:amd64.\r\n",
            "Preparing to unpack .../66-libmirclient-dev_0.31.1-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libmirclient-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libopenal-dev:amd64.\r\n",
            "Preparing to unpack .../67-libopenal-dev_1%3a1.18.2-2_amd64.deb ...\r\n",
            "Unpacking libopenal-dev:amd64 (1:1.18.2-2) ...\r\n",
            "Selecting previously unselected package libpulse-mainloop-glib0:amd64.\r\n",
            "Preparing to unpack .../68-libpulse-mainloop-glib0_1%3a11.1-1ubuntu7.8_amd64.deb ...\r\n",
            "Unpacking libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.8) ...\r\n",
            "Selecting previously unselected package libpulse-dev:amd64.\r\n",
            "Preparing to unpack .../69-libpulse-dev_1%3a11.1-1ubuntu7.8_amd64.deb ...\r\n",
            "Unpacking libpulse-dev:amd64 (1:11.1-1ubuntu7.8) ...\r\n",
            "Selecting previously unselected package libsndio-dev:amd64.\r\n",
            "Preparing to unpack .../70-libsndio-dev_1.1.0-3_amd64.deb ...\r\n",
            "Unpacking libsndio-dev:amd64 (1.1.0-3) ...\r\n",
            "Selecting previously unselected package libudev-dev:amd64.\r\n",
            "Preparing to unpack .../71-libudev-dev_237-3ubuntu10.41_amd64.deb ...\r\n",
            "Unpacking libudev-dev:amd64 (237-3ubuntu10.41) ...\r\n",
            "Selecting previously unselected package libxv-dev:amd64.\r\n",
            "Preparing to unpack .../72-libxv-dev_2%3a1.0.11-1_amd64.deb ...\r\n",
            "Unpacking libxv-dev:amd64 (2:1.0.11-1) ...\r\n",
            "Selecting previously unselected package libsdl2-dev:amd64.\r\n",
            "Preparing to unpack .../73-libsdl2-dev_2.0.8+dfsg1-1ubuntu1.18.04.4_amd64.deb ...\r\n",
            "Unpacking libsdl2-dev:amd64 (2.0.8+dfsg1-1ubuntu1.18.04.4) ...\r\n",
            "Selecting previously unselected package libwildmidi-config.\r\n",
            "Preparing to unpack .../74-libwildmidi-config_0.4.2-1_all.deb ...\r\n",
            "Unpacking libwildmidi-config (0.4.2-1) ...\r\n",
            "Selecting previously unselected package libwildmidi2:amd64.\r\n",
            "Preparing to unpack .../75-libwildmidi2_0.4.2-1_amd64.deb ...\r\n",
            "Unpacking libwildmidi2:amd64 (0.4.2-1) ...\r\n",
            "Selecting previously unselected package libwildmidi-dev.\r\n",
            "Preparing to unpack .../76-libwildmidi-dev_0.4.2-1_amd64.deb ...\r\n",
            "Unpacking libwildmidi-dev (0.4.2-1) ...\r\n",
            "Selecting previously unselected package nasm.\r\n",
            "Preparing to unpack .../77-nasm_2.13.02-0.1_amd64.deb ...\r\n",
            "Unpacking nasm (2.13.02-0.1) ...\r\n",
            "Selecting previously unselected package timidity.\r\n",
            "Preparing to unpack .../78-timidity_2.13.2-41_amd64.deb ...\r\n",
            "Unpacking timidity (2.13.2-41) ...\r\n",
            "Selecting previously unselected package timidity-daemon.\r\n",
            "Preparing to unpack .../79-timidity-daemon_2.13.2-41_all.deb ...\r\n",
            "Unpacking timidity-daemon (2.13.2-41) ...\r\n",
            "Setting up libdbus-1-dev:amd64 (1.12.2-1ubuntu1.1) ...\r\n",
            "Setting up libxcursor-dev:amd64 (1:1.1.15-1) ...\r\n",
            "Setting up gir1.2-atk-1.0:amd64 (2.28.1-1) ...\r\n",
            "Setting up libxkbcommon-dev:amd64 (0.8.2-1~ubuntu18.04.1) ...\r\n",
            "Setting up libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.8) ...\r\n",
            "Setting up libpulse-dev:amd64 (1:11.1-1ubuntu7.8) ...\r\n",
            "Setting up libarchive-zip-perl (1.60-1ubuntu0.1) ...\r\n",
            "Setting up libmircore-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libtimedate-perl (2.3000-2) ...\r\n",
            "Setting up libcairo-script-interpreter2:amd64 (1.15.10-2ubuntu0.1) ...\r\n",
            "Setting up libsigsegv2:amd64 (2.12-1) ...\r\n",
            "Setting up libgme-dev:amd64 (0.6.2-1) ...\r\n",
            "Setting up gir1.2-freedesktop:amd64 (1.56.1-1) ...\r\n",
            "Setting up libsndio-dev:amd64 (1.1.0-3) ...\r\n",
            "Setting up libxcb-shm0-dev:amd64 (1.13-2~ubuntu18.04) ...\r\n",
            "Setting up libpangoxft-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\r\n",
            "Setting up libxml2-utils (2.9.4+dfsg1-6.1ubuntu1.3) ...\r\n",
            "Setting up libarchive-cpio-perl (0.10-1) ...\r\n",
            "Setting up gir1.2-gdkpixbuf-2.0:amd64 (2.36.11-2) ...\r\n",
            "Setting up libatk1.0-dev:amd64 (2.28.1-1) ...\r\n",
            "Setting up gettext-base (0.19.8.1-6ubuntu0.3) ...\r\n",
            "Setting up m4 (1.4.18-1) ...\r\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\r\n",
            "Setting up gir1.2-pango-1.0:amd64 (1.40.14-1ubuntu0.1) ...\r\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\r\n",
            "Setting up libopenal-dev:amd64 (1:1.18.2-2) ...\r\n",
            "Setting up libsys-hostname-long-perl (1.5-1) ...\r\n",
            "Setting up libwildmidi-config (0.4.2-1) ...\r\n",
            "Setting up libmircookie2:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libgdk-pixbuf2.0-dev (2.36.11-2) ...\r\n",
            "Setting up libmail-sendmail-perl (0.80-1) ...\r\n",
            "Setting up x11proto-xinerama-dev (2018.4-4) ...\r\n",
            "Setting up autotools-dev (20180224.1) ...\r\n",
            "Setting up libpixman-1-dev:amd64 (0.34.0-2) ...\r\n",
            "Setting up x11proto-randr-dev (2018.4-4) ...\r\n",
            "Setting up libxinerama-dev:amd64 (2:1.1.3-1) ...\r\n",
            "Setting up libxv-dev:amd64 (2:1.0.11-1) ...\r\n",
            "Setting up nasm (2.13.02-0.1) ...\r\n",
            "Setting up libcapnp-0.6.1:amd64 (0.6.1-1ubuntu1) ...\r\n",
            "Setting up libibus-1.0-5:amd64 (1.5.17-3ubuntu5.3) ...\r\n",
            "Setting up libmircore1:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up freepats (20060219-1) ...\r\n",
            "Setting up libprotobuf-lite10:amd64 (3.0.0-9.1ubuntu1) ...\r\n",
            "Setting up libudev-dev:amd64 (237-3ubuntu10.41) ...\r\n",
            "Setting up libfluidsynth1:amd64 (1.1.9-1) ...\r\n",
            "Setting up x11proto-composite-dev (1:2018.4-4) ...\r\n",
            "Setting up autopoint (0.19.8.1-6ubuntu0.3) ...\r\n",
            "Setting up libaudio2:amd64 (1.9.4-6) ...\r\n",
            "Setting up libfile-stripnondeterminism-perl (0.040-1.1~build1) ...\r\n",
            "Setting up gir1.2-gtk-2.0 (2.24.32-1ubuntu1) ...\r\n",
            "Setting up gir1.2-ibus-1.0:amd64 (1.5.17-3ubuntu5.3) ...\r\n",
            "Setting up libxrandr-dev:amd64 (2:1.5.1-1) ...\r\n",
            "Setting up libcairo2-dev:amd64 (1.15.10-2ubuntu0.1) ...\r\n",
            "Setting up gettext (0.19.8.1-6ubuntu0.3) ...\r\n",
            "Setting up libxcomposite-dev:amd64 (1:0.4.4-2) ...\r\n",
            "Setting up libmirprotobuf3:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libprotobuf-dev:amd64 (3.0.0-9.1ubuntu1) ...\r\n",
            "Setting up autoconf (2.69-11) ...\r\n",
            "Setting up libmircookie-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libwildmidi2:amd64 (0.4.2-1) ...\r\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\r\n",
            "Setting up intltool-debian (0.35.0+20060710.4) ...\r\n",
            "Setting up libibus-1.0-dev:amd64 (1.5.17-3ubuntu5.3) ...\r\n",
            "Setting up automake (1:1.15.1-3ubuntu2) ...\r\n",
            "update-alternatives: using /usr/bin/automake-1.15 to provide /usr/bin/automake (automake) in auto mode\r\n",
            "Setting up libmircommon7:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libpango1.0-dev (1.40.14-1ubuntu0.1) ...\r\n",
            "Setting up libfluidsynth-dev:amd64 (1.1.9-1) ...\r\n",
            "Setting up timidity (2.13.2-41) ...\r\n",
            "Setting up libtool (2.4.6-2) ...\r\n",
            "Setting up po-debconf (1.0.20) ...\r\n",
            "Setting up libwildmidi-dev (0.4.2-1) ...\r\n",
            "Setting up libgtk2.0-dev (2.24.32-1ubuntu1) ...\r\n",
            "Setting up libmirclient9:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libmircommon-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up timidity-daemon (2.13.2-41) ...\r\n",
            "Adding group timidity....done\r\n",
            "Adding system user timidity....done\r\n",
            "Adding user `timidity' to group `audio' ...\r\n",
            "Adding user timidity to group audio\r\n",
            "Done.\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of stop.\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\r\n",
            "Setting up libmirclient-dev:amd64 (0.31.1-0ubuntu1) ...\r\n",
            "Setting up libsdl2-dev:amd64 (2.0.8+dfsg1-1ubuntu1.18.04.4) ...\r\n",
            "Setting up debhelper (11.1.6ubuntu2) ...\r\n",
            "Setting up dh-autoreconf (17) ...\r\n",
            "Setting up dh-strip-nondeterminism (0.040-1.1~build1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n",
            "\r\n",
            "Processing triggers for systemd (237-3ubuntu10.41) ...\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libboost-all-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libtool-bin\n",
            "The following NEW packages will be installed:\n",
            "  liblua5.1-0-dev libtool-bin\n",
            "0 upgraded, 2 newly installed, 0 to remove and 44 not upgraded.\n",
            "Need to get 198 kB of archives.\n",
            "After this operation, 1,188 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 liblua5.1-0-dev amd64 5.1.5-8.1build2 [119 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtool-bin amd64 2.4.6-2 [79.5 kB]\n",
            "Fetched 198 kB in 1s (222 kB/s)\n",
            "Selecting previously unselected package liblua5.1-0-dev:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 147354 files and directories currently installed.)\r\n",
            "Preparing to unpack .../liblua5.1-0-dev_5.1.5-8.1build2_amd64.deb ...\r\n",
            "Unpacking liblua5.1-0-dev:amd64 (5.1.5-8.1build2) ...\r\n",
            "Selecting previously unselected package libtool-bin.\r\n",
            "Preparing to unpack .../libtool-bin_2.4.6-2_amd64.deb ...\r\n",
            "Unpacking libtool-bin (2.4.6-2) ...\r\n",
            "Setting up libtool-bin (2.4.6-2) ...\r\n",
            "Setting up liblua5.1-0-dev:amd64 (5.1.5-8.1build2) ...\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rkVyd_mcoM9",
        "colab_type": "code",
        "outputId": "d33d27c8-1a4a-4c06-beae-7f7245a2b374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!pip install vizdoom"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vizdoom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/6c/23565c09387173423883e7881fce53541ff89b5209ca0904c67e577dd6ac/vizdoom-1.1.7.tar.gz (4.9MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9MB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from vizdoom) (1.18.5)\n",
            "Building wheels for collected packages: vizdoom\n",
            "  Building wheel for vizdoom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vizdoom: filename=vizdoom-1.1.7-cp36-none-any.whl size=14285790 sha256=fb693ee2fd6f984411725a05e84ad6a3f8f90ea95e7352bed88a5038f72d9734\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/98/04/d96d2c8edb8d1c008d926716257b407e56fb3ee0c81e51d25e\n",
            "Successfully built vizdoom\n",
            "Installing collected packages: vizdoom\n",
            "Successfully installed vizdoom-1.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vUb4NSSb-Hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf      # Deep Learning library\n",
        "import tensorflow as tf2\n",
        "import numpy as np           # Handle matrices\n",
        "from vizdoom import *        # Doom Environment\n",
        "\n",
        "\n",
        "import random                # Handling random number generation\n",
        "import time                  # Handling time calculation\n",
        "from skimage import transform# Help us to preprocess the frames\n",
        "\n",
        "from collections import deque# Ordered collection with ends\n",
        "import matplotlib.pyplot as plt # Display graphs\n",
        "\n",
        "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByxZrTwnwxrs",
        "colab_type": "code",
        "outputId": "fde12117-aae4-49da-ebee-ee92abf1e997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "tf.disable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO2AsJRue94K",
        "colab_type": "text"
      },
      "source": [
        "## Creating the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpNrppu_fbFW",
        "colab_type": "code",
        "outputId": "3d8f46c4-aa1c-4eb1-ff33-2d376760c562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/Deep%20Q%20Learning/Doom/basic.cfg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-15 08:28:54--  https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/Deep%20Q%20Learning/Doom/basic.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 892 [text/plain]\n",
            "Saving to: ‘basic.cfg’\n",
            "\n",
            "\rbasic.cfg             0%[                    ]       0  --.-KB/s               \rbasic.cfg           100%[===================>]     892  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-15 08:28:54 (61.9 MB/s) - ‘basic.cfg’ saved [892/892]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuiWMExWfBT2",
        "colab_type": "code",
        "outputId": "cc3054ba-6e45-479f-ca24-5c7869f7b9a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/Deep%20Q%20Learning/Doom/basic.wad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-15 08:28:55--  https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/Deep%20Q%20Learning/Doom/basic.wad\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2704 (2.6K) [application/octet-stream]\n",
            "Saving to: ‘basic.wad’\n",
            "\n",
            "\rbasic.wad             0%[                    ]       0  --.-KB/s               \rbasic.wad           100%[===================>]   2.64K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-15 08:28:56 (62.1 MB/s) - ‘basic.wad’ saved [2704/2704]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGKQ-ubGfAwU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkGtiWQTcHNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_environment():\n",
        "    game = DoomGame()\n",
        "    \n",
        "    game.load_config(\"basic.cfg\")  # You may load different config for a different game scenario\n",
        "    \n",
        "    game.set_doom_scenario_path(\"basic.wad\") # You may load different wad for a different game scenario\n",
        "    game.set_window_visible(False)\n",
        "    \n",
        "    game.init()\n",
        "    \n",
        "    left = [1, 0, 0]\n",
        "    right = [0, 1, 0]\n",
        "    shoot = [0, 0, 1]\n",
        "    \n",
        "    possible_actions = [left, right, shoot]\n",
        "    \n",
        "    return game, possible_actions\n",
        "\n",
        "def test_environment():\n",
        "    game = DoomGame()\n",
        "    game.load_config(\"basic.cfg\")\n",
        "    game.set_doom_scenario_path(\"basic.wad\")\n",
        "    game.init()\n",
        "    \n",
        "    left = [1, 0, 0]\n",
        "    right = [0, 1, 0]\n",
        "    shoot = [0, 0, 1]\n",
        "    \n",
        "    actions = [left, right, shoot]\n",
        "    \n",
        "    episodes = 10\n",
        "    \n",
        "    for i in range(episodes):\n",
        "        game.new_episode()\n",
        "        \n",
        "        while not game.is_episode_finished():\n",
        "            state = game.get_state() \n",
        "            img = state.screen_buffer # Returns the frame from a game (RGB Frame)\n",
        "            misc = state.game_variables\n",
        "            action = random.choice(actions)\n",
        "            print(action)\n",
        "            reward = game.make_action(action)\n",
        "            print(\"reward: \",reward)\n",
        "            time.sleep(0.02)\n",
        "            \n",
        "        print(\"Result: \",game.get_total_reward())\n",
        "        time.sleep(2)\n",
        "    \n",
        "    game.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCOra0EQelgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "game, possible_actions = create_environment()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdOXyvEZuxf-",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Define the preprocessing functions ⚙️\n",
        "### preprocess_frame\n",
        "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
        "<br><br>\n",
        "Our steps:\n",
        "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
        "- Crop the screen (in our case we remove the roof because it contains no information)\n",
        "- We normalize pixel values\n",
        "- Finally we resize the preprocessed frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BysplZ3FuvAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def preprocess_frame(frame):\n",
        "    # Greyscale frame already done in our vizdoom config\n",
        "    # x = np.mean(frame,-1)\n",
        "    \n",
        "    # Crop the screen (remove the roof because it contains no information)\n",
        "    cropped_frame = frame[30:-10,30:-30]\n",
        "    \n",
        "    # Normalize Pixel Values\n",
        "    normalized_frame = cropped_frame/255.0\n",
        "    \n",
        "    # Resize\n",
        "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
        "    \n",
        "    return preprocessed_frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPAAdDKQvMj6",
        "colab_type": "text"
      },
      "source": [
        "### stack_frames\n",
        "\n",
        "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
        "\n",
        "- First we preprocess frame\n",
        "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
        "- Finally we **build the stacked state**\n",
        "\n",
        "This is how work stack:\n",
        "- For the first frame, we feed 4 frames\n",
        "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
        "- And so on\n",
        "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BrJ-QdavIqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stack_size = 4 # We stack 4 frames\n",
        "\n",
        "# Initialize deque with zero-images one array for each image\n",
        "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "    \n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "        \n",
        "        # Because we're in a new episode, copy the same frame 4x\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        \n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "        \n",
        "    else:\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "    \n",
        "    return stacked_state, stacked_frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hLfzn_avae7",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Set up our hyperparameters ⚗️\n",
        "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
        "\n",
        "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
        "- Then, you'll add the training hyperparameters when you implement the training algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N81jtU-vXp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### MODEL HYPERPARAMETERS\n",
        "state_size = [84,84,4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
        "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
        "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
        "\n",
        "### TRAINING HYPERPARAMETERS\n",
        "total_episodes = 500        # Total episodes for training\n",
        "max_steps = 100              # Max possible steps in an episode\n",
        "batch_size = 64             \n",
        "\n",
        "# Exploration parameters for epsilon greedy strategy\n",
        "explore_start = 1.0            # exploration probability at start\n",
        "explore_stop = 0.01            # minimum exploration probability \n",
        "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
        "\n",
        "# Q learning hyperparameters\n",
        "gamma = 0.95               # Discounting rate\n",
        "\n",
        "### MEMORY HYPERPARAMETERS\n",
        "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
        "memory_size = 1000000          # Number of experiences the Memory can keep\n",
        "\n",
        "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
        "training = True\n",
        "\n",
        "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
        "episode_render = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAp5ZJKYv9QR",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Create our Deep Q-learning Neural Network model 🧠\n",
        "<img src=\"assets/model.png\" alt=\"Model\" />\n",
        "This is our Deep Q-learning model:\n",
        "- We take a stack of 4 frames as input\n",
        "- It passes through 3 convnets\n",
        "- Then it is flatened\n",
        "- Finally it passes through 2 FC layers\n",
        "- It outputs a Q value for each actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-wI9tWhv6O9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNetwork:\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        with tf.variable_scope(name):\n",
        "            # We create the placeholders\n",
        "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
        "            # [None, 84, 84, 4]\n",
        "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
        "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
        "            \n",
        "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
        "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
        "            \n",
        "            \"\"\"\n",
        "            First convnet:\n",
        "            CNN\n",
        "            BatchNormalization\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            # Input is 84x84x4\n",
        "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
        "                                         filters = 32,\n",
        "                                         kernel_size = [8,8],\n",
        "                                         strides = [4,4],\n",
        "                                         padding = \"VALID\",\n",
        "                                          kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                         name = \"conv1\")\n",
        "            \n",
        "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
        "                                                   training = True,\n",
        "                                                   epsilon = 1e-5,\n",
        "                                                     name = 'batch_norm1')\n",
        "            \n",
        "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
        "            ## --> [20, 20, 32]\n",
        "            \n",
        "            \n",
        "            \"\"\"\n",
        "            Second convnet:\n",
        "            CNN\n",
        "            BatchNormalization\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
        "                                 filters = 64,\n",
        "                                 kernel_size = [4,4],\n",
        "                                 strides = [2,2],\n",
        "                                 padding = \"VALID\",\n",
        "                                kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                 name = \"conv2\")\n",
        "        \n",
        "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
        "                                                   training = True,\n",
        "                                                   epsilon = 1e-5,\n",
        "                                                     name = 'batch_norm2')\n",
        "\n",
        "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
        "            ## --> [9, 9, 64]\n",
        "            \n",
        "            \n",
        "            \"\"\"\n",
        "            Third convnet:\n",
        "            CNN\n",
        "            BatchNormalization\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
        "                                 filters = 128,\n",
        "                                 kernel_size = [4,4],\n",
        "                                 strides = [2,2],\n",
        "                                 padding = \"VALID\",\n",
        "                                kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                 name = \"conv3\")\n",
        "        \n",
        "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
        "                                                   training = True,\n",
        "                                                   epsilon = 1e-5,\n",
        "                                                     name = 'batch_norm3')\n",
        "\n",
        "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
        "            ## --> [3, 3, 128]\n",
        "            \n",
        "            \n",
        "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
        "            ## --> [1152]\n",
        "            \n",
        "            \n",
        "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
        "                                  units = 512,\n",
        "                                  activation = tf.nn.elu,\n",
        "                                       kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                name=\"fc1\")\n",
        "            \n",
        "            \n",
        "            self.output = tf.layers.dense(inputs = self.fc, \n",
        "                                           kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                          units = 3, \n",
        "                                        activation=None)\n",
        "\n",
        "  \n",
        "            # Q is our predicted Q value.\n",
        "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
        "            \n",
        "            \n",
        "            # The loss is the difference between our predicted Q_values and the Q_target\n",
        "            # Sum(Qtarget - Q)^2\n",
        "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
        "            \n",
        "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGrChwcjwCiU",
        "colab_type": "code",
        "outputId": "545ca2f0-7cc1-4a76-d266-3966c1945ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# Reset the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Instantiate the DQNetwork\n",
        "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-85dd4054ea82>:30: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-17-85dd4054ea82>:35: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "WARNING:tensorflow:From <ipython-input-17-85dd4054ea82>:87: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-17-85dd4054ea82>:95: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOUNF_SJHMBl",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Experience Replay 🔁\n",
        "Now that we create our Neural Network, **we need to implement the Experience Replay method.** <br><br>\n",
        "Here we'll create the Memory object that creates a deque.A deque (double ended queue) is a data type that **removes the oldest element each time that you add a new element.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuTn6a4AwE-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Memory():\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen = max_size)\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        buffer_size = len(self.buffer)\n",
        "        index = np.random.choice(np.arange(buffer_size),\n",
        "                                size = batch_size,\n",
        "                                replace = False)\n",
        "        \n",
        "        return [self.buffer[i] for i in index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJBQlWfbHQHq",
        "colab_type": "text"
      },
      "source": [
        "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience (state, action, reward, new_state)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW5OMyptHOMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate memory\n",
        "memory = Memory(max_size = memory_size)\n",
        "\n",
        "# Render the environment\n",
        "game.new_episode()\n",
        "\n",
        "for i in range(pretrain_length):\n",
        "    # If it's the first step\n",
        "    if i == 0:\n",
        "        # First we need a state\n",
        "        state = game.get_state().screen_buffer\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "    \n",
        "    # Random action\n",
        "    action = random.choice(possible_actions)\n",
        "    \n",
        "    # Get the rewards\n",
        "    reward = game.make_action(action)\n",
        "    \n",
        "    # Look if the episode is finished\n",
        "    done = game.is_episode_finished()\n",
        "    \n",
        "    # If we're dead\n",
        "    if done:\n",
        "        # We finished the episode\n",
        "        next_state = np.zeros(state.shape)\n",
        "        \n",
        "        # Add experience to memory\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "        \n",
        "        # Start a new episode\n",
        "        game.new_episode()\n",
        "        \n",
        "        # First we need a state\n",
        "        state = game.get_state().screen_buffer\n",
        "        \n",
        "        # Stack the frames\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "        \n",
        "    else:\n",
        "        # Get the next state\n",
        "        next_state = game.get_state().screen_buffer\n",
        "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "        \n",
        "        # Add experience to memory\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "        \n",
        "        # Our state is now the next_state\n",
        "        state = next_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTCMeB8ZHcbf",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Set up Tensorboard 📊\n",
        "\n",
        "To launch tensorboard : `tensorboard --logdir=/tensorboard/dqn/1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu63xXR3HSrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup TensorBoard Writer\n",
        "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
        "\n",
        "## Losses\n",
        "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
        "\n",
        "write_op = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycSSOQOQHb9Q",
        "colab_type": "text"
      },
      "source": [
        "## Step 8: Train our Agent 🏃‍♂️\n",
        "\n",
        "Our algorithm:\n",
        "<br>\n",
        "* Initialize the weights\n",
        "* Init the environment\n",
        "* Initialize the decay rate (that will use to reduce epsilon) \n",
        "<br><br>\n",
        "* **For** episode to max_episode **do** \n",
        "    * Make new episode\n",
        "    * Set step to 0\n",
        "    * Observe the first state $s_0$\n",
        "    <br><br>\n",
        "    * **While** step < max_steps **do**:\n",
        "        * Increase decay_rate\n",
        "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
        "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
        "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
        "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
        "        * Set $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
        "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
        "    * **endfor**\n",
        "    <br><br>\n",
        "* **endfor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbF24uReHZvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This function will do the part\n",
        "With ϵ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
        "\"\"\"\n",
        "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
        "    ## EPSILON GREEDY STRATEGY\n",
        "    # Choose action a from state s using epsilon greedy.\n",
        "    ## First we randomize a number\n",
        "    exp_exp_tradeoff = np.random.rand()\n",
        "\n",
        "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
        "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
        "    \n",
        "    if (explore_probability > exp_exp_tradeoff):\n",
        "        # Make a random action (exploration)\n",
        "        action = random.choice(possible_actions)\n",
        "        \n",
        "    else:\n",
        "        # Get action from Q-network (exploitation)\n",
        "        # Estimate the Qs values state\n",
        "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
        "        \n",
        "        # Take the biggest Q value (= the best action)\n",
        "        choice = np.argmax(Qs)\n",
        "        action = possible_actions[int(choice)]\n",
        "                \n",
        "    return action, explore_probability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbpLICoGHiih",
        "colab_type": "code",
        "outputId": "9453228f-1373-47b3-bc94-c6dc389f48c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Saver will help us to save our model\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "if training == True:\n",
        "    with tf.Session() as sess:\n",
        "        # Initialize the variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        # Initialize the decay rate (that will use to reduce epsilon) \n",
        "        decay_step = 0\n",
        "\n",
        "        # Init the game\n",
        "        game.init()\n",
        "\n",
        "        for episode in range(total_episodes):\n",
        "            # Set step to 0\n",
        "            step = 0\n",
        "            \n",
        "            # Initialize the rewards of the episode\n",
        "            episode_rewards = []\n",
        "            \n",
        "            # Make a new episode and observe the first state\n",
        "            game.new_episode()\n",
        "            state = game.get_state().screen_buffer\n",
        "            \n",
        "            # Remember that stack frame function also call our preprocess function.\n",
        "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "            while step < max_steps:\n",
        "                step += 1\n",
        "                \n",
        "                # Increase decay_step\n",
        "                decay_step +=1\n",
        "                \n",
        "                # Predict the action to take and take it\n",
        "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
        "\n",
        "                # Do the action\n",
        "                reward = game.make_action(action)\n",
        "\n",
        "                # Look if the episode is finished\n",
        "                done = game.is_episode_finished()\n",
        "                \n",
        "                # Add the reward to total reward\n",
        "                episode_rewards.append(reward)\n",
        "\n",
        "                # If the game is finished\n",
        "                if done:\n",
        "                    # the episode ends so no next state\n",
        "                    next_state = np.zeros((84,84), dtype=np.int)\n",
        "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "                    # Set step = max_steps to end the episode\n",
        "                    step = max_steps\n",
        "\n",
        "                    # Get the total reward of the episode\n",
        "                    total_reward = np.sum(episode_rewards)\n",
        "\n",
        "                    print('Episode: {}'.format(episode),\n",
        "                              'Total reward: {}'.format(total_reward),\n",
        "                              'Training loss: {:.4f}'.format(loss),\n",
        "                              'Explore P: {:.4f}'.format(explore_probability))\n",
        "\n",
        "                    memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "                else:\n",
        "                    # Get the next state\n",
        "                    next_state = game.get_state().screen_buffer\n",
        "                    \n",
        "                    # Stack the frame of the next_state\n",
        "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "                    \n",
        "\n",
        "                    # Add experience to memory\n",
        "                    memory.add((state, action, reward, next_state, done))\n",
        "                    \n",
        "                    # st+1 is now our current state\n",
        "                    state = next_state\n",
        "\n",
        "\n",
        "                ### LEARNING PART            \n",
        "                # Obtain random mini-batch from memory\n",
        "                batch = memory.sample(batch_size)\n",
        "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
        "                actions_mb = np.array([each[1] for each in batch])\n",
        "                rewards_mb = np.array([each[2] for each in batch]) \n",
        "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
        "                dones_mb = np.array([each[4] for each in batch])\n",
        "\n",
        "                target_Qs_batch = []\n",
        "\n",
        "                 # Get Q values for next_state \n",
        "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
        "                \n",
        "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
        "                for i in range(0, len(batch)):\n",
        "                    terminal = dones_mb[i]\n",
        "\n",
        "                    # If we are in a terminal state, only equals reward\n",
        "                    if terminal:\n",
        "                        target_Qs_batch.append(rewards_mb[i])\n",
        "                        \n",
        "                    else:\n",
        "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
        "                        target_Qs_batch.append(target)\n",
        "                        \n",
        "\n",
        "                targets_mb = np.array([each for each in target_Qs_batch])\n",
        "\n",
        "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
        "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
        "                                               DQNetwork.target_Q: targets_mb,\n",
        "                                               DQNetwork.actions_: actions_mb})\n",
        "\n",
        "                # Write TF Summaries\n",
        "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
        "                                                   DQNetwork.target_Q: targets_mb,\n",
        "                                                   DQNetwork.actions_: actions_mb})\n",
        "                writer.add_summary(summary, episode)\n",
        "                writer.flush()\n",
        "\n",
        "            # Save model every 5 episodes\n",
        "            if episode % 5 == 0:\n",
        "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
        "                print(\"Model Saved\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Saved\n",
            "Episode: 2 Total reward: 70.0 Training loss: 0.5035 Explore P: 0.9779\n",
            "Episode: 3 Total reward: 20.0 Training loss: 10.4035 Explore P: 0.9715\n",
            "Episode: 4 Total reward: 92.0 Training loss: 63.4153 Explore P: 0.9706\n",
            "Model Saved\n",
            "Episode: 8 Total reward: 21.0 Training loss: 4.8984 Explore P: 0.9362\n",
            "Model Saved\n",
            "Episode: 12 Total reward: 7.0 Training loss: 1.7350 Explore P: 0.9022\n",
            "Episode: 14 Total reward: 12.0 Training loss: 7.9887 Explore P: 0.8868\n",
            "Episode: 15 Total reward: 47.0 Training loss: 10.7655 Explore P: 0.8830\n",
            "Model Saved\n",
            "Model Saved\n",
            "Episode: 24 Total reward: 93.0 Training loss: 0.9225 Explore P: 0.8152\n",
            "Model Saved\n",
            "Episode: 26 Total reward: 70.0 Training loss: 2.3405 Explore P: 0.8051\n",
            "Episode: 28 Total reward: 19.0 Training loss: 2.6398 Explore P: 0.7920\n",
            "Episode: 30 Total reward: 67.0 Training loss: 4.0161 Explore P: 0.7819\n",
            "Model Saved\n",
            "Episode: 31 Total reward: 95.0 Training loss: 2.9084 Explore P: 0.7815\n",
            "Episode: 32 Total reward: 95.0 Training loss: 3.9164 Explore P: 0.7810\n",
            "Episode: 34 Total reward: 93.0 Training loss: 3.1337 Explore P: 0.7727\n",
            "Model Saved\n",
            "Episode: 37 Total reward: 91.0 Training loss: 8.7158 Explore P: 0.7569\n",
            "Episode: 39 Total reward: 19.0 Training loss: 2.4391 Explore P: 0.7445\n",
            "Episode: 40 Total reward: 28.0 Training loss: 9.2301 Explore P: 0.7403\n",
            "Model Saved\n",
            "Episode: 41 Total reward: 86.0 Training loss: 1.7018 Explore P: 0.7392\n",
            "Episode: 43 Total reward: 43.0 Training loss: 1.8426 Explore P: 0.7285\n",
            "Episode: 45 Total reward: 95.0 Training loss: 1.6707 Explore P: 0.7209\n",
            "Model Saved\n",
            "Episode: 46 Total reward: 38.0 Training loss: 3.0392 Explore P: 0.7171\n",
            "Episode: 47 Total reward: -17.0 Training loss: 3.5595 Explore P: 0.7102\n",
            "Episode: 50 Total reward: 17.0 Training loss: 5.5195 Explore P: 0.6916\n",
            "Model Saved\n",
            "Episode: 51 Total reward: 44.0 Training loss: 2.3898 Explore P: 0.6884\n",
            "Episode: 52 Total reward: 69.0 Training loss: 5.9376 Explore P: 0.6866\n",
            "Episode: 53 Total reward: 47.0 Training loss: 2.4699 Explore P: 0.6836\n",
            "Model Saved\n",
            "Episode: 57 Total reward: -11.0 Training loss: 2.6124 Explore P: 0.6578\n",
            "Episode: 59 Total reward: 47.0 Training loss: 11.1486 Explore P: 0.6485\n",
            "Episode: 60 Total reward: 51.0 Training loss: 3.5823 Explore P: 0.6459\n",
            "Model Saved\n",
            "Episode: 61 Total reward: 18.0 Training loss: 6.0910 Explore P: 0.6416\n",
            "Episode: 63 Total reward: 93.0 Training loss: 16.1192 Explore P: 0.6348\n",
            "Episode: 64 Total reward: 25.0 Training loss: 5.7921 Explore P: 0.6310\n",
            "Episode: 65 Total reward: 93.0 Training loss: 5.4967 Explore P: 0.6305\n",
            "Model Saved\n",
            "Episode: 67 Total reward: 65.0 Training loss: 10.2501 Explore P: 0.6225\n",
            "Episode: 68 Total reward: 17.0 Training loss: 4.5948 Explore P: 0.6183\n",
            "Episode: 69 Total reward: 94.0 Training loss: 8.1222 Explore P: 0.6178\n",
            "Episode: 70 Total reward: 59.0 Training loss: 4.7214 Explore P: 0.6156\n",
            "Model Saved\n",
            "Episode: 71 Total reward: 67.0 Training loss: 6.6940 Explore P: 0.6138\n",
            "Episode: 72 Total reward: 35.0 Training loss: 2.7151 Explore P: 0.6105\n",
            "Episode: 74 Total reward: 92.0 Training loss: 7.6434 Explore P: 0.6040\n",
            "Episode: 75 Total reward: 9.0 Training loss: 3.2731 Explore P: 0.5994\n",
            "Model Saved\n",
            "Episode: 76 Total reward: 91.0 Training loss: 3.1995 Explore P: 0.5988\n",
            "Episode: 77 Total reward: 12.0 Training loss: 5.8509 Explore P: 0.5945\n",
            "Episode: 78 Total reward: 95.0 Training loss: 2.5549 Explore P: 0.5941\n",
            "Episode: 79 Total reward: 30.0 Training loss: 3.9979 Explore P: 0.5909\n",
            "Episode: 80 Total reward: 57.0 Training loss: 3.2805 Explore P: 0.5889\n",
            "Model Saved\n",
            "Episode: 81 Total reward: 39.0 Training loss: 5.0052 Explore P: 0.5859\n",
            "Episode: 82 Total reward: 60.0 Training loss: 2.6993 Explore P: 0.5838\n",
            "Episode: 83 Total reward: 93.0 Training loss: 12.1319 Explore P: 0.5834\n",
            "Episode: 84 Total reward: 50.0 Training loss: 7.2871 Explore P: 0.5810\n",
            "Episode: 85 Total reward: 42.0 Training loss: 4.7964 Explore P: 0.5782\n",
            "Model Saved\n",
            "Episode: 86 Total reward: 38.0 Training loss: 5.1561 Explore P: 0.5752\n",
            "Episode: 87 Total reward: 3.0 Training loss: 6.7822 Explore P: 0.5708\n",
            "Episode: 88 Total reward: 59.0 Training loss: 5.1059 Explore P: 0.5688\n",
            "Episode: 89 Total reward: 19.0 Training loss: 16.3534 Explore P: 0.5650\n",
            "Episode: 90 Total reward: 95.0 Training loss: 7.5456 Explore P: 0.5647\n",
            "Model Saved\n",
            "Episode: 91 Total reward: 40.0 Training loss: 6.7301 Explore P: 0.5619\n",
            "Episode: 92 Total reward: 62.0 Training loss: 9.1205 Explore P: 0.5600\n",
            "Episode: 93 Total reward: 49.0 Training loss: 6.9857 Explore P: 0.5577\n",
            "Model Saved\n",
            "Episode: 97 Total reward: 56.0 Training loss: 2.9458 Explore P: 0.5396\n",
            "Episode: 98 Total reward: 66.0 Training loss: 4.2971 Explore P: 0.5381\n",
            "Episode: 99 Total reward: 28.0 Training loss: 6.6815 Explore P: 0.5350\n",
            "Episode: 100 Total reward: 47.0 Training loss: 2.3373 Explore P: 0.5327\n",
            "Model Saved\n",
            "Episode: 101 Total reward: 65.0 Training loss: 4.9267 Explore P: 0.5311\n",
            "Episode: 102 Total reward: 63.0 Training loss: 4.9884 Explore P: 0.5294\n",
            "Episode: 103 Total reward: 68.0 Training loss: 7.9165 Explore P: 0.5279\n",
            "Episode: 104 Total reward: 43.0 Training loss: 5.5657 Explore P: 0.5254\n",
            "Episode: 105 Total reward: 95.0 Training loss: 4.4144 Explore P: 0.5251\n",
            "Model Saved\n",
            "Episode: 106 Total reward: 50.0 Training loss: 5.4802 Explore P: 0.5228\n",
            "Episode: 107 Total reward: 16.0 Training loss: 5.2955 Explore P: 0.5192\n",
            "Episode: 108 Total reward: 94.0 Training loss: 7.4021 Explore P: 0.5188\n",
            "Episode: 109 Total reward: 90.0 Training loss: 5.4829 Explore P: 0.5183\n",
            "Episode: 110 Total reward: 75.0 Training loss: 3.3550 Explore P: 0.5172\n",
            "Model Saved\n",
            "Episode: 111 Total reward: 23.0 Training loss: 9.5220 Explore P: 0.5140\n",
            "Episode: 112 Total reward: 88.0 Training loss: 5.1502 Explore P: 0.5134\n",
            "Episode: 113 Total reward: 73.0 Training loss: 7.7938 Explore P: 0.5122\n",
            "Episode: 114 Total reward: 36.0 Training loss: 8.2458 Explore P: 0.5094\n",
            "Episode: 115 Total reward: 95.0 Training loss: 4.8387 Explore P: 0.5091\n",
            "Model Saved\n",
            "Episode: 116 Total reward: 36.0 Training loss: 3.1571 Explore P: 0.5064\n",
            "Episode: 117 Total reward: 7.0 Training loss: 6.3520 Explore P: 0.5025\n",
            "Episode: 118 Total reward: 30.0 Training loss: 2.4861 Explore P: 0.4998\n",
            "Episode: 119 Total reward: 95.0 Training loss: 6.2938 Explore P: 0.4995\n",
            "Episode: 120 Total reward: 92.0 Training loss: 6.9184 Explore P: 0.4990\n",
            "Model Saved\n",
            "Episode: 121 Total reward: 19.0 Training loss: 5.7241 Explore P: 0.4958\n",
            "Episode: 123 Total reward: 76.0 Training loss: 4.8971 Explore P: 0.4900\n",
            "Episode: 124 Total reward: 23.0 Training loss: 7.8212 Explore P: 0.4869\n",
            "Episode: 125 Total reward: 95.0 Training loss: 5.6066 Explore P: 0.4867\n",
            "Model Saved\n",
            "Episode: 126 Total reward: 62.0 Training loss: 5.2105 Explore P: 0.4850\n",
            "Episode: 127 Total reward: 75.0 Training loss: 5.4089 Explore P: 0.4840\n",
            "Episode: 128 Total reward: 95.0 Training loss: 5.7432 Explore P: 0.4838\n",
            "Episode: 129 Total reward: 37.0 Training loss: 3.0752 Explore P: 0.4812\n",
            "Episode: 130 Total reward: 22.0 Training loss: 5.5943 Explore P: 0.4782\n",
            "Model Saved\n",
            "Episode: 133 Total reward: 54.0 Training loss: 6.4349 Explore P: 0.4672\n",
            "Episode: 134 Total reward: -4.0 Training loss: 7.8570 Explore P: 0.4634\n",
            "Episode: 135 Total reward: 69.0 Training loss: 3.6538 Explore P: 0.4621\n",
            "Model Saved\n",
            "Episode: 136 Total reward: 74.0 Training loss: 9.1433 Explore P: 0.4612\n",
            "Episode: 137 Total reward: 44.0 Training loss: 4.6739 Explore P: 0.4590\n",
            "Episode: 139 Total reward: 43.0 Training loss: 2.8896 Explore P: 0.4524\n",
            "Episode: 140 Total reward: 32.0 Training loss: 5.3577 Explore P: 0.4501\n",
            "Model Saved\n",
            "Episode: 142 Total reward: 65.0 Training loss: 17.2107 Explore P: 0.4443\n",
            "Episode: 143 Total reward: 95.0 Training loss: 8.5395 Explore P: 0.4441\n",
            "Episode: 144 Total reward: 66.0 Training loss: 10.6040 Explore P: 0.4428\n",
            "Episode: 145 Total reward: 14.0 Training loss: 9.0547 Explore P: 0.4399\n",
            "Model Saved\n",
            "Episode: 146 Total reward: 46.0 Training loss: 7.5919 Explore P: 0.4379\n",
            "Episode: 147 Total reward: 95.0 Training loss: 3.7917 Explore P: 0.4377\n",
            "Episode: 148 Total reward: 25.0 Training loss: 5.8204 Explore P: 0.4351\n",
            "Episode: 149 Total reward: 41.0 Training loss: 8.8039 Explore P: 0.4330\n",
            "Episode: 150 Total reward: 46.0 Training loss: 11.5575 Explore P: 0.4311\n",
            "Model Saved\n",
            "Episode: 151 Total reward: 48.0 Training loss: 14.8456 Explore P: 0.4293\n",
            "Episode: 152 Total reward: 14.0 Training loss: 6.2608 Explore P: 0.4263\n",
            "Episode: 154 Total reward: 65.0 Training loss: 6.2348 Explore P: 0.4208\n",
            "Episode: 155 Total reward: 68.0 Training loss: 7.5978 Explore P: 0.4197\n",
            "Model Saved\n",
            "Episode: 156 Total reward: 15.0 Training loss: 10.9530 Explore P: 0.4168\n",
            "Episode: 157 Total reward: 62.0 Training loss: 8.0884 Explore P: 0.4154\n",
            "Episode: 158 Total reward: 71.0 Training loss: 3.8176 Explore P: 0.4144\n",
            "Episode: 159 Total reward: 76.0 Training loss: 6.0440 Explore P: 0.4136\n",
            "Episode: 160 Total reward: 36.0 Training loss: 4.2949 Explore P: 0.4116\n",
            "Model Saved\n",
            "Episode: 161 Total reward: 37.0 Training loss: 5.9715 Explore P: 0.4094\n",
            "Episode: 163 Total reward: 55.0 Training loss: 15.6242 Explore P: 0.4040\n",
            "Episode: 164 Total reward: 33.0 Training loss: 7.7071 Explore P: 0.4019\n",
            "Model Saved\n",
            "Episode: 166 Total reward: 47.0 Training loss: 3.8108 Explore P: 0.3963\n",
            "Episode: 167 Total reward: 23.0 Training loss: 12.3211 Explore P: 0.3939\n",
            "Episode: 168 Total reward: 66.0 Training loss: 4.3354 Explore P: 0.3928\n",
            "Episode: 169 Total reward: 95.0 Training loss: 4.3440 Explore P: 0.3925\n",
            "Episode: 170 Total reward: 92.0 Training loss: 5.3173 Explore P: 0.3922\n",
            "Model Saved\n",
            "Episode: 171 Total reward: 19.0 Training loss: 9.2921 Explore P: 0.3896\n",
            "Episode: 172 Total reward: 95.0 Training loss: 7.9566 Explore P: 0.3894\n",
            "Episode: 173 Total reward: 68.0 Training loss: 8.2940 Explore P: 0.3883\n",
            "Episode: 175 Total reward: 38.0 Training loss: 5.3187 Explore P: 0.3826\n",
            "Model Saved\n",
            "Episode: 176 Total reward: 95.0 Training loss: 3.9938 Explore P: 0.3824\n",
            "Episode: 177 Total reward: 73.0 Training loss: 5.9635 Explore P: 0.3815\n",
            "Episode: 178 Total reward: 47.0 Training loss: 5.7777 Explore P: 0.3799\n",
            "Episode: 179 Total reward: 40.0 Training loss: 10.8537 Explore P: 0.3780\n",
            "Model Saved\n",
            "Episode: 181 Total reward: 68.0 Training loss: 15.5529 Explore P: 0.3733\n",
            "Episode: 182 Total reward: 57.0 Training loss: 8.2838 Explore P: 0.3719\n",
            "Episode: 183 Total reward: 59.0 Training loss: 12.8299 Explore P: 0.3706\n",
            "Episode: 184 Total reward: 25.0 Training loss: 4.0322 Explore P: 0.3684\n",
            "Episode: 185 Total reward: 57.0 Training loss: 10.4987 Explore P: 0.3670\n",
            "Model Saved\n",
            "Episode: 186 Total reward: 40.0 Training loss: 7.0542 Explore P: 0.3652\n",
            "Episode: 188 Total reward: 48.0 Training loss: 7.5828 Explore P: 0.3601\n",
            "Episode: 189 Total reward: 84.0 Training loss: 10.3407 Explore P: 0.3595\n",
            "Episode: 190 Total reward: 95.0 Training loss: 5.2922 Explore P: 0.3593\n",
            "Model Saved\n",
            "Episode: 191 Total reward: 37.0 Training loss: 5.5311 Explore P: 0.3574\n",
            "Episode: 192 Total reward: 46.0 Training loss: 6.4940 Explore P: 0.3559\n",
            "Episode: 193 Total reward: 39.0 Training loss: 4.1557 Explore P: 0.3541\n",
            "Episode: 194 Total reward: 95.0 Training loss: 3.7249 Explore P: 0.3539\n",
            "Episode: 195 Total reward: 46.0 Training loss: 6.5949 Explore P: 0.3523\n",
            "Model Saved\n",
            "Episode: 196 Total reward: 63.0 Training loss: 6.6508 Explore P: 0.3512\n",
            "Episode: 197 Total reward: 67.0 Training loss: 17.1952 Explore P: 0.3502\n",
            "Episode: 198 Total reward: 61.0 Training loss: 4.6014 Explore P: 0.3490\n",
            "Episode: 199 Total reward: 31.0 Training loss: 6.3322 Explore P: 0.3470\n",
            "Model Saved\n",
            "Episode: 201 Total reward: 22.0 Training loss: 6.1980 Explore P: 0.3415\n",
            "Episode: 202 Total reward: 55.0 Training loss: 5.9930 Explore P: 0.3402\n",
            "Episode: 203 Total reward: 44.0 Training loss: 5.4623 Explore P: 0.3386\n",
            "Episode: 204 Total reward: 43.0 Training loss: 5.9736 Explore P: 0.3370\n",
            "Episode: 205 Total reward: 28.0 Training loss: 7.3840 Explore P: 0.3352\n",
            "Model Saved\n",
            "Episode: 206 Total reward: 92.0 Training loss: 6.4951 Explore P: 0.3349\n",
            "Episode: 207 Total reward: 65.0 Training loss: 4.0524 Explore P: 0.3339\n",
            "Episode: 208 Total reward: 59.0 Training loss: 4.9754 Explore P: 0.3327\n",
            "Episode: 209 Total reward: 31.0 Training loss: 16.6370 Explore P: 0.3309\n",
            "Episode: 210 Total reward: 44.0 Training loss: 10.0342 Explore P: 0.3294\n",
            "Model Saved\n",
            "Episode: 211 Total reward: 93.0 Training loss: 5.6887 Explore P: 0.3291\n",
            "Episode: 212 Total reward: 95.0 Training loss: 3.1216 Explore P: 0.3289\n",
            "Episode: 213 Total reward: 74.0 Training loss: 3.6284 Explore P: 0.3282\n",
            "Episode: 214 Total reward: 67.0 Training loss: 20.5979 Explore P: 0.3273\n",
            "Episode: 215 Total reward: 94.0 Training loss: 8.2952 Explore P: 0.3271\n",
            "Model Saved\n",
            "Episode: 216 Total reward: 70.0 Training loss: 7.2040 Explore P: 0.3263\n",
            "Episode: 217 Total reward: 59.0 Training loss: 5.7564 Explore P: 0.3251\n",
            "Episode: 218 Total reward: 76.0 Training loss: 4.6799 Explore P: 0.3245\n",
            "Episode: 219 Total reward: 47.0 Training loss: 7.2072 Explore P: 0.3231\n",
            "Episode: 220 Total reward: 61.0 Training loss: 6.3136 Explore P: 0.3220\n",
            "Model Saved\n",
            "Episode: 221 Total reward: 55.0 Training loss: 7.5312 Explore P: 0.3209\n",
            "Episode: 222 Total reward: 95.0 Training loss: 5.4700 Explore P: 0.3207\n",
            "Episode: 223 Total reward: 14.0 Training loss: 4.4436 Explore P: 0.3185\n",
            "Episode: 225 Total reward: 18.0 Training loss: 9.7404 Explore P: 0.3133\n",
            "Model Saved\n",
            "Episode: 226 Total reward: 72.0 Training loss: 5.2181 Explore P: 0.3126\n",
            "Episode: 227 Total reward: 48.0 Training loss: 10.9016 Explore P: 0.3113\n",
            "Episode: 228 Total reward: 58.0 Training loss: 5.7609 Explore P: 0.3102\n",
            "Episode: 229 Total reward: 74.0 Training loss: 8.5895 Explore P: 0.3095\n",
            "Episode: 230 Total reward: 64.0 Training loss: 10.8865 Explore P: 0.3085\n",
            "Model Saved\n",
            "Episode: 231 Total reward: 21.0 Training loss: 13.6753 Explore P: 0.3066\n",
            "Episode: 233 Total reward: 72.0 Training loss: 4.3297 Explore P: 0.3030\n",
            "Episode: 234 Total reward: 95.0 Training loss: 5.9916 Explore P: 0.3028\n",
            "Episode: 235 Total reward: 93.0 Training loss: 12.7944 Explore P: 0.3025\n",
            "Model Saved\n",
            "Episode: 237 Total reward: 36.0 Training loss: 7.7672 Explore P: 0.2980\n",
            "Episode: 238 Total reward: 62.0 Training loss: 6.9821 Explore P: 0.2971\n",
            "Episode: 239 Total reward: 94.0 Training loss: 11.4954 Explore P: 0.2969\n",
            "Model Saved\n",
            "Episode: 241 Total reward: 86.0 Training loss: 3.5992 Explore P: 0.2936\n",
            "Episode: 242 Total reward: 39.0 Training loss: 4.9944 Explore P: 0.2921\n",
            "Episode: 243 Total reward: 74.0 Training loss: 6.0169 Explore P: 0.2915\n",
            "Episode: 244 Total reward: 49.0 Training loss: 5.8972 Explore P: 0.2903\n",
            "Episode: 245 Total reward: 71.0 Training loss: 4.0545 Explore P: 0.2896\n",
            "Model Saved\n",
            "Episode: 246 Total reward: 70.0 Training loss: 6.4918 Explore P: 0.2889\n",
            "Episode: 247 Total reward: 95.0 Training loss: 3.0419 Explore P: 0.2887\n",
            "Episode: 249 Total reward: 59.0 Training loss: 4.6189 Explore P: 0.2849\n",
            "Episode: 250 Total reward: 46.0 Training loss: 5.9497 Explore P: 0.2837\n",
            "Model Saved\n",
            "Episode: 251 Total reward: 92.0 Training loss: 4.8674 Explore P: 0.2834\n",
            "Episode: 252 Total reward: 64.0 Training loss: 9.0897 Explore P: 0.2826\n",
            "Episode: 253 Total reward: 10.0 Training loss: 4.3160 Explore P: 0.2806\n",
            "Episode: 254 Total reward: 76.0 Training loss: 6.5044 Explore P: 0.2801\n",
            "Episode: 255 Total reward: 67.0 Training loss: 5.4722 Explore P: 0.2793\n",
            "Model Saved\n",
            "Episode: 256 Total reward: 27.0 Training loss: 8.9877 Explore P: 0.2777\n",
            "Episode: 257 Total reward: 37.0 Training loss: 4.5214 Explore P: 0.2763\n",
            "Episode: 258 Total reward: 13.0 Training loss: 4.9234 Explore P: 0.2744\n",
            "Episode: 259 Total reward: 57.0 Training loss: 13.2095 Explore P: 0.2733\n",
            "Model Saved\n",
            "Episode: 261 Total reward: 55.0 Training loss: 5.3494 Explore P: 0.2698\n",
            "Episode: 262 Total reward: 48.0 Training loss: 4.3339 Explore P: 0.2687\n",
            "Episode: 263 Total reward: 95.0 Training loss: 3.9123 Explore P: 0.2685\n",
            "Episode: 264 Total reward: 75.0 Training loss: 6.9655 Explore P: 0.2680\n",
            "Model Saved\n",
            "Episode: 266 Total reward: 45.0 Training loss: 2.8090 Explore P: 0.2642\n",
            "Episode: 267 Total reward: 89.0 Training loss: 3.8770 Explore P: 0.2639\n",
            "Episode: 268 Total reward: 70.0 Training loss: 6.3545 Explore P: 0.2633\n",
            "Model Saved\n",
            "Episode: 271 Total reward: 69.0 Training loss: 8.3332 Explore P: 0.2576\n",
            "Episode: 272 Total reward: 62.0 Training loss: 4.8718 Explore P: 0.2567\n",
            "Episode: 273 Total reward: 72.0 Training loss: 4.8416 Explore P: 0.2561\n",
            "Episode: 274 Total reward: 95.0 Training loss: 3.1824 Explore P: 0.2560\n",
            "Episode: 275 Total reward: 93.0 Training loss: 3.0817 Explore P: 0.2558\n",
            "Model Saved\n",
            "Episode: 276 Total reward: 78.0 Training loss: 5.8489 Explore P: 0.2552\n",
            "Episode: 278 Total reward: 45.0 Training loss: 5.5798 Explore P: 0.2517\n",
            "Episode: 279 Total reward: 76.0 Training loss: 4.9050 Explore P: 0.2512\n",
            "Episode: 280 Total reward: 62.0 Training loss: 4.0626 Explore P: 0.2504\n",
            "Model Saved\n",
            "Episode: 281 Total reward: 38.0 Training loss: 26.1180 Explore P: 0.2491\n",
            "Episode: 282 Total reward: 67.0 Training loss: 4.5418 Explore P: 0.2484\n",
            "Episode: 283 Total reward: 94.0 Training loss: 6.9725 Explore P: 0.2482\n",
            "Episode: 284 Total reward: 25.0 Training loss: 5.7948 Explore P: 0.2468\n",
            "Episode: 285 Total reward: 70.0 Training loss: 3.9640 Explore P: 0.2462\n",
            "Model Saved\n",
            "Episode: 286 Total reward: 95.0 Training loss: 6.5461 Explore P: 0.2460\n",
            "Episode: 287 Total reward: 39.0 Training loss: 9.6893 Explore P: 0.2448\n",
            "Episode: 288 Total reward: 39.0 Training loss: 5.2361 Explore P: 0.2436\n",
            "Episode: 289 Total reward: 66.0 Training loss: 6.2015 Explore P: 0.2429\n",
            "Episode: 290 Total reward: 72.0 Training loss: 5.3090 Explore P: 0.2423\n",
            "Model Saved\n",
            "Episode: 291 Total reward: 73.0 Training loss: 5.0218 Explore P: 0.2418\n",
            "Episode: 293 Total reward: 77.0 Training loss: 4.8756 Explore P: 0.2390\n",
            "Episode: 294 Total reward: 68.0 Training loss: 4.9312 Explore P: 0.2383\n",
            "Episode: 295 Total reward: 95.0 Training loss: 6.3049 Explore P: 0.2382\n",
            "Model Saved\n",
            "Episode: 296 Total reward: 59.0 Training loss: 9.8111 Explore P: 0.2373\n",
            "Episode: 297 Total reward: 64.0 Training loss: 5.7230 Explore P: 0.2366\n",
            "Episode: 298 Total reward: 55.0 Training loss: 4.2637 Explore P: 0.2357\n",
            "Episode: 299 Total reward: 43.0 Training loss: 2.9477 Explore P: 0.2346\n",
            "Model Saved\n",
            "Episode: 301 Total reward: 32.0 Training loss: 6.6637 Explore P: 0.2312\n",
            "Episode: 302 Total reward: 63.0 Training loss: 4.8029 Explore P: 0.2304\n",
            "Episode: 303 Total reward: 21.0 Training loss: 5.2240 Explore P: 0.2290\n",
            "Episode: 304 Total reward: 33.0 Training loss: 4.6633 Explore P: 0.2278\n",
            "Episode: 305 Total reward: 66.0 Training loss: 9.9316 Explore P: 0.2272\n",
            "Model Saved\n",
            "Episode: 306 Total reward: 57.0 Training loss: 8.2609 Explore P: 0.2265\n",
            "Episode: 307 Total reward: 59.0 Training loss: 9.3244 Explore P: 0.2257\n",
            "Episode: 308 Total reward: 95.0 Training loss: 5.2386 Explore P: 0.2255\n",
            "Episode: 309 Total reward: 52.0 Training loss: 2.8314 Explore P: 0.2246\n",
            "Episode: 310 Total reward: 58.0 Training loss: 5.1893 Explore P: 0.2238\n",
            "Model Saved\n",
            "Episode: 311 Total reward: 92.0 Training loss: 6.0954 Explore P: 0.2236\n",
            "Episode: 313 Total reward: 70.0 Training loss: 6.3603 Explore P: 0.2209\n",
            "Episode: 314 Total reward: 77.0 Training loss: 6.7354 Explore P: 0.2204\n",
            "Episode: 315 Total reward: 95.0 Training loss: 4.8291 Explore P: 0.2203\n",
            "Model Saved\n",
            "Episode: 316 Total reward: 64.0 Training loss: 4.7640 Explore P: 0.2196\n",
            "Episode: 317 Total reward: 42.0 Training loss: 5.4778 Explore P: 0.2186\n",
            "Episode: 318 Total reward: 58.0 Training loss: 7.3742 Explore P: 0.2178\n",
            "Episode: 319 Total reward: 84.0 Training loss: 4.1625 Explore P: 0.2174\n",
            "Model Saved\n",
            "Episode: 321 Total reward: 71.0 Training loss: 9.0910 Explore P: 0.2149\n",
            "Episode: 322 Total reward: 47.0 Training loss: 6.6639 Explore P: 0.2140\n",
            "Episode: 323 Total reward: 64.0 Training loss: 27.7684 Explore P: 0.2133\n",
            "Episode: 324 Total reward: 94.0 Training loss: 7.1628 Explore P: 0.2132\n",
            "Episode: 325 Total reward: 68.0 Training loss: 6.1227 Explore P: 0.2126\n",
            "Model Saved\n",
            "Episode: 326 Total reward: 49.0 Training loss: 12.3792 Explore P: 0.2117\n",
            "Episode: 327 Total reward: 40.0 Training loss: 4.1529 Explore P: 0.2107\n",
            "Episode: 328 Total reward: 93.0 Training loss: 5.2038 Explore P: 0.2106\n",
            "Episode: 329 Total reward: 70.0 Training loss: 16.1478 Explore P: 0.2100\n",
            "Episode: 330 Total reward: 92.0 Training loss: 6.6000 Explore P: 0.2099\n",
            "Model Saved\n",
            "Episode: 331 Total reward: 5.0 Training loss: 2.7622 Explore P: 0.2083\n",
            "Episode: 332 Total reward: 76.0 Training loss: 5.2587 Explore P: 0.2079\n",
            "Episode: 333 Total reward: 95.0 Training loss: 6.0859 Explore P: 0.2078\n",
            "Episode: 334 Total reward: 65.0 Training loss: 5.5190 Explore P: 0.2072\n",
            "Episode: 335 Total reward: 94.0 Training loss: 6.7343 Explore P: 0.2071\n",
            "Model Saved\n",
            "Episode: 337 Total reward: 95.0 Training loss: 2.8577 Explore P: 0.2050\n",
            "Episode: 339 Total reward: 53.0 Training loss: 6.8160 Explore P: 0.2023\n",
            "Episode: 340 Total reward: 52.0 Training loss: 5.1447 Explore P: 0.2016\n",
            "Model Saved\n",
            "Episode: 341 Total reward: 61.0 Training loss: 10.6732 Explore P: 0.2009\n",
            "Episode: 342 Total reward: 61.0 Training loss: 6.1171 Explore P: 0.2002\n",
            "Episode: 343 Total reward: 63.0 Training loss: 4.5063 Explore P: 0.1996\n",
            "Episode: 344 Total reward: 95.0 Training loss: 8.8793 Explore P: 0.1995\n",
            "Episode: 345 Total reward: 71.0 Training loss: 4.3712 Explore P: 0.1990\n",
            "Model Saved\n",
            "Episode: 346 Total reward: 54.0 Training loss: 25.4107 Explore P: 0.1983\n",
            "Episode: 348 Total reward: 92.0 Training loss: 6.7394 Explore P: 0.1963\n",
            "Episode: 349 Total reward: 5.0 Training loss: 6.0888 Explore P: 0.1949\n",
            "Episode: 350 Total reward: 95.0 Training loss: 3.6992 Explore P: 0.1948\n",
            "Model Saved\n",
            "Episode: 351 Total reward: 61.0 Training loss: 7.9874 Explore P: 0.1941\n",
            "Episode: 352 Total reward: 83.0 Training loss: 3.0901 Explore P: 0.1938\n",
            "Episode: 353 Total reward: 72.0 Training loss: 8.4159 Explore P: 0.1933\n",
            "Episode: 355 Total reward: 68.0 Training loss: 4.7739 Explore P: 0.1909\n",
            "Model Saved\n",
            "Episode: 356 Total reward: 66.0 Training loss: 6.1465 Explore P: 0.1904\n",
            "Episode: 357 Total reward: 90.0 Training loss: 3.8173 Explore P: 0.1902\n",
            "Episode: 359 Total reward: 46.0 Training loss: 7.0613 Explore P: 0.1875\n",
            "Episode: 360 Total reward: 47.0 Training loss: 12.8906 Explore P: 0.1867\n",
            "Model Saved\n",
            "Episode: 361 Total reward: 92.0 Training loss: 6.6307 Explore P: 0.1866\n",
            "Episode: 362 Total reward: 73.0 Training loss: 5.3121 Explore P: 0.1862\n",
            "Episode: 363 Total reward: 61.0 Training loss: 6.8706 Explore P: 0.1855\n",
            "Episode: 364 Total reward: 94.0 Training loss: 14.6603 Explore P: 0.1854\n",
            "Episode: 365 Total reward: 48.0 Training loss: 3.9360 Explore P: 0.1847\n",
            "Model Saved\n",
            "Episode: 366 Total reward: 47.0 Training loss: 9.9186 Explore P: 0.1838\n",
            "Episode: 367 Total reward: 71.0 Training loss: 9.2290 Explore P: 0.1834\n",
            "Episode: 368 Total reward: 56.0 Training loss: 6.0110 Explore P: 0.1827\n",
            "Episode: 369 Total reward: 64.0 Training loss: 6.5554 Explore P: 0.1821\n",
            "Model Saved\n",
            "Episode: 371 Total reward: 53.0 Training loss: 6.6257 Explore P: 0.1798\n",
            "Episode: 372 Total reward: 62.0 Training loss: 5.4973 Explore P: 0.1792\n",
            "Episode: 373 Total reward: 95.0 Training loss: 6.7138 Explore P: 0.1791\n",
            "Episode: 374 Total reward: 30.0 Training loss: 2.9430 Explore P: 0.1782\n",
            "Episode: 375 Total reward: 63.0 Training loss: 6.0616 Explore P: 0.1776\n",
            "Model Saved\n",
            "Episode: 376 Total reward: 95.0 Training loss: 13.9051 Explore P: 0.1775\n",
            "Episode: 377 Total reward: 24.0 Training loss: 4.1927 Explore P: 0.1765\n",
            "Episode: 378 Total reward: 73.0 Training loss: 2.5350 Explore P: 0.1761\n",
            "Episode: 379 Total reward: 51.0 Training loss: 3.3005 Explore P: 0.1754\n",
            "Episode: 380 Total reward: 61.0 Training loss: 6.9314 Explore P: 0.1748\n",
            "Model Saved\n",
            "Episode: 381 Total reward: 95.0 Training loss: 4.7343 Explore P: 0.1747\n",
            "Episode: 382 Total reward: 60.0 Training loss: 4.5878 Explore P: 0.1742\n",
            "Episode: 383 Total reward: 19.0 Training loss: 4.6910 Explore P: 0.1731\n",
            "Episode: 384 Total reward: 70.0 Training loss: 4.8596 Explore P: 0.1726\n",
            "Episode: 385 Total reward: 89.0 Training loss: 5.4751 Explore P: 0.1724\n",
            "Model Saved\n",
            "Episode: 386 Total reward: 38.0 Training loss: 5.5776 Explore P: 0.1716\n",
            "Episode: 387 Total reward: 60.0 Training loss: 5.3761 Explore P: 0.1710\n",
            "Episode: 388 Total reward: 86.0 Training loss: 4.2721 Explore P: 0.1708\n",
            "Episode: 389 Total reward: 56.0 Training loss: 3.1579 Explore P: 0.1702\n",
            "Model Saved\n",
            "Episode: 391 Total reward: 95.0 Training loss: 8.9131 Explore P: 0.1685\n",
            "Episode: 392 Total reward: 95.0 Training loss: 7.5987 Explore P: 0.1684\n",
            "Episode: 393 Total reward: 61.0 Training loss: 9.0768 Explore P: 0.1679\n",
            "Episode: 394 Total reward: 94.0 Training loss: 3.1425 Explore P: 0.1677\n",
            "Episode: 395 Total reward: 81.0 Training loss: 3.8515 Explore P: 0.1674\n",
            "Model Saved\n",
            "Episode: 397 Total reward: 41.0 Training loss: 5.5387 Explore P: 0.1651\n",
            "Episode: 399 Total reward: 62.0 Training loss: 2.8361 Explore P: 0.1630\n",
            "Episode: 400 Total reward: 63.0 Training loss: 5.5965 Explore P: 0.1625\n",
            "Model Saved\n",
            "Episode: 401 Total reward: 62.0 Training loss: 8.3885 Explore P: 0.1620\n",
            "Episode: 402 Total reward: 65.0 Training loss: 13.7931 Explore P: 0.1615\n",
            "Episode: 403 Total reward: 67.0 Training loss: 4.6752 Explore P: 0.1611\n",
            "Episode: 404 Total reward: 63.0 Training loss: 5.3641 Explore P: 0.1606\n",
            "Episode: 405 Total reward: 35.0 Training loss: 6.3184 Explore P: 0.1598\n",
            "Model Saved\n",
            "Episode: 406 Total reward: 87.0 Training loss: 5.2421 Explore P: 0.1595\n",
            "Episode: 407 Total reward: 25.0 Training loss: 5.2527 Explore P: 0.1586\n",
            "Episode: 408 Total reward: 44.0 Training loss: 4.9472 Explore P: 0.1579\n",
            "Episode: 409 Total reward: 89.0 Training loss: 7.2950 Explore P: 0.1578\n",
            "Episode: 410 Total reward: 56.0 Training loss: 4.5651 Explore P: 0.1572\n",
            "Model Saved\n",
            "Episode: 411 Total reward: 95.0 Training loss: 5.9466 Explore P: 0.1572\n",
            "Episode: 412 Total reward: 92.0 Training loss: 6.2702 Explore P: 0.1570\n",
            "Episode: 413 Total reward: 61.0 Training loss: 4.3736 Explore P: 0.1565\n",
            "Episode: 414 Total reward: 70.0 Training loss: 6.9028 Explore P: 0.1561\n",
            "Episode: 415 Total reward: 46.0 Training loss: 12.7224 Explore P: 0.1555\n",
            "Model Saved\n",
            "Episode: 416 Total reward: 76.0 Training loss: 5.3373 Explore P: 0.1551\n",
            "Episode: 417 Total reward: 65.0 Training loss: 7.0645 Explore P: 0.1547\n",
            "Episode: 419 Total reward: 43.0 Training loss: 4.2784 Explore P: 0.1525\n",
            "Episode: 420 Total reward: 62.0 Training loss: 13.0246 Explore P: 0.1521\n",
            "Model Saved\n",
            "Episode: 421 Total reward: 94.0 Training loss: 4.8953 Explore P: 0.1520\n",
            "Episode: 422 Total reward: 86.0 Training loss: 7.1265 Explore P: 0.1517\n",
            "Episode: 423 Total reward: 66.0 Training loss: 5.2725 Explore P: 0.1513\n",
            "Episode: 424 Total reward: 57.0 Training loss: 7.1568 Explore P: 0.1508\n",
            "Episode: 425 Total reward: 92.0 Training loss: 5.4374 Explore P: 0.1506\n",
            "Model Saved\n",
            "Episode: 427 Total reward: 64.0 Training loss: 5.4531 Explore P: 0.1488\n",
            "Episode: 428 Total reward: 86.0 Training loss: 6.0109 Explore P: 0.1486\n",
            "Episode: 429 Total reward: 53.0 Training loss: 8.8657 Explore P: 0.1480\n",
            "Episode: 430 Total reward: 74.0 Training loss: 15.4302 Explore P: 0.1477\n",
            "Model Saved\n",
            "Episode: 431 Total reward: 95.0 Training loss: 4.9535 Explore P: 0.1476\n",
            "Episode: 432 Total reward: 43.0 Training loss: 9.3260 Explore P: 0.1469\n",
            "Episode: 433 Total reward: 42.0 Training loss: 5.7591 Explore P: 0.1462\n",
            "Episode: 434 Total reward: 82.0 Training loss: 6.7577 Explore P: 0.1460\n",
            "Model Saved\n",
            "Episode: 436 Total reward: 56.0 Training loss: 4.3460 Explore P: 0.1441\n",
            "Episode: 438 Total reward: 67.0 Training loss: 5.4397 Explore P: 0.1423\n",
            "Episode: 439 Total reward: 7.0 Training loss: 7.3606 Explore P: 0.1414\n",
            "Episode: 440 Total reward: 64.0 Training loss: 4.8240 Explore P: 0.1409\n",
            "Model Saved\n",
            "Episode: 441 Total reward: 66.0 Training loss: 3.6114 Explore P: 0.1406\n",
            "Episode: 442 Total reward: 95.0 Training loss: 5.1015 Explore P: 0.1405\n",
            "Episode: 443 Total reward: 73.0 Training loss: 4.9904 Explore P: 0.1402\n",
            "Episode: 444 Total reward: 4.0 Training loss: 9.5976 Explore P: 0.1392\n",
            "Episode: 445 Total reward: 82.0 Training loss: 5.2067 Explore P: 0.1389\n",
            "Model Saved\n",
            "Episode: 446 Total reward: 95.0 Training loss: 4.8709 Explore P: 0.1389\n",
            "Episode: 447 Total reward: 95.0 Training loss: 13.9204 Explore P: 0.1388\n",
            "Episode: 448 Total reward: 44.0 Training loss: 7.1936 Explore P: 0.1382\n",
            "Episode: 449 Total reward: 65.0 Training loss: 7.4903 Explore P: 0.1378\n",
            "Episode: 450 Total reward: 87.0 Training loss: 7.0820 Explore P: 0.1376\n",
            "Model Saved\n",
            "Episode: 451 Total reward: 71.0 Training loss: 8.4954 Explore P: 0.1373\n",
            "Episode: 453 Total reward: 59.0 Training loss: 8.5438 Explore P: 0.1356\n",
            "Episode: 454 Total reward: 37.0 Training loss: 7.6836 Explore P: 0.1349\n",
            "Episode: 455 Total reward: 95.0 Training loss: 13.7953 Explore P: 0.1348\n",
            "Model Saved\n",
            "Episode: 456 Total reward: 36.0 Training loss: 4.4358 Explore P: 0.1341\n",
            "Episode: 457 Total reward: 48.0 Training loss: 7.8229 Explore P: 0.1336\n",
            "Episode: 458 Total reward: 47.0 Training loss: 3.9088 Explore P: 0.1330\n",
            "Episode: 459 Total reward: 68.0 Training loss: 3.7729 Explore P: 0.1327\n",
            "Episode: 460 Total reward: 95.0 Training loss: 11.0872 Explore P: 0.1326\n",
            "Model Saved\n",
            "Episode: 461 Total reward: 85.0 Training loss: 5.7069 Explore P: 0.1324\n",
            "Episode: 462 Total reward: 93.0 Training loss: 9.2648 Explore P: 0.1323\n",
            "Episode: 463 Total reward: 61.0 Training loss: 4.2256 Explore P: 0.1319\n",
            "Episode: 464 Total reward: 95.0 Training loss: 5.1941 Explore P: 0.1318\n",
            "Episode: 465 Total reward: 52.0 Training loss: 9.0439 Explore P: 0.1313\n",
            "Model Saved\n",
            "Episode: 467 Total reward: 67.0 Training loss: 7.6746 Explore P: 0.1297\n",
            "Episode: 468 Total reward: 66.0 Training loss: 3.0945 Explore P: 0.1294\n",
            "Episode: 469 Total reward: 64.0 Training loss: 18.0731 Explore P: 0.1290\n",
            "Episode: 470 Total reward: 63.0 Training loss: 8.1154 Explore P: 0.1286\n",
            "Model Saved\n",
            "Episode: 474 Total reward: 61.0 Training loss: 5.7134 Explore P: 0.1247\n",
            "Episode: 475 Total reward: 92.0 Training loss: 5.2311 Explore P: 0.1246\n",
            "Model Saved\n",
            "Episode: 476 Total reward: 47.0 Training loss: 3.1281 Explore P: 0.1241\n",
            "Episode: 477 Total reward: 62.0 Training loss: 5.9969 Explore P: 0.1237\n",
            "Episode: 478 Total reward: 87.0 Training loss: 8.5741 Explore P: 0.1235\n",
            "Episode: 479 Total reward: 69.0 Training loss: 2.8880 Explore P: 0.1232\n",
            "Episode: 480 Total reward: 48.0 Training loss: 7.7662 Explore P: 0.1228\n",
            "Model Saved\n",
            "Episode: 481 Total reward: 83.0 Training loss: 4.9348 Explore P: 0.1226\n",
            "Episode: 482 Total reward: 63.0 Training loss: 4.2942 Explore P: 0.1222\n",
            "Episode: 483 Total reward: 48.0 Training loss: 4.4550 Explore P: 0.1217\n",
            "Episode: 484 Total reward: 62.0 Training loss: 4.2668 Explore P: 0.1213\n",
            "Episode: 485 Total reward: 95.0 Training loss: 6.9545 Explore P: 0.1213\n",
            "Model Saved\n",
            "Episode: 487 Total reward: 73.0 Training loss: 3.4059 Explore P: 0.1199\n",
            "Episode: 488 Total reward: 49.0 Training loss: 14.1317 Explore P: 0.1194\n",
            "Episode: 489 Total reward: 35.0 Training loss: 4.0740 Explore P: 0.1188\n",
            "Episode: 490 Total reward: 66.0 Training loss: 4.6899 Explore P: 0.1185\n",
            "Model Saved\n",
            "Episode: 491 Total reward: 22.0 Training loss: 51.7304 Explore P: 0.1178\n",
            "Episode: 492 Total reward: 29.0 Training loss: 7.7024 Explore P: 0.1172\n",
            "Episode: 493 Total reward: 75.0 Training loss: 6.7001 Explore P: 0.1169\n",
            "Episode: 494 Total reward: 74.0 Training loss: 6.0073 Explore P: 0.1167\n",
            "Episode: 495 Total reward: 76.0 Training loss: 6.1042 Explore P: 0.1164\n",
            "Model Saved\n",
            "Episode: 496 Total reward: 56.0 Training loss: 4.1058 Explore P: 0.1160\n",
            "Episode: 497 Total reward: 63.0 Training loss: 5.6839 Explore P: 0.1156\n",
            "Episode: 498 Total reward: 77.0 Training loss: 7.1008 Explore P: 0.1154\n",
            "Episode: 499 Total reward: 95.0 Training loss: 9.4375 Explore P: 0.1153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivnHV58QKM6N",
        "colab_type": "text"
      },
      "source": [
        "## Step 9: Watch our Agent play 👀\n",
        "Now that we trained our agent, we can test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhojSHFmHlh4",
        "colab_type": "code",
        "outputId": "307b10f5-ee49-45df-ac5e-3bfd897a1e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    \n",
        "    game, possible_actions = create_environment()\n",
        "    \n",
        "    totalScore = 0\n",
        "    \n",
        "    # Load the model\n",
        "    saver.restore(sess, \"./models/model.ckpt\")\n",
        "    game.init()\n",
        "    for i in range(1):\n",
        "        \n",
        "        done = False\n",
        "        \n",
        "        game.new_episode()\n",
        "        \n",
        "        state = game.get_state().screen_buffer\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "            \n",
        "        while not game.is_episode_finished():\n",
        "            # Take the biggest Q value (= the best action)\n",
        "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
        "            \n",
        "            # Take the biggest Q value (= the best action)\n",
        "            choice = np.argmax(Qs)\n",
        "            action = possible_actions[int(choice)]\n",
        "            \n",
        "            game.make_action(action)\n",
        "            done = game.is_episode_finished()\n",
        "            score = game.get_total_reward()\n",
        "            \n",
        "            if done:\n",
        "                break  \n",
        "                \n",
        "            else:\n",
        "                next_state = game.get_state().screen_buffer\n",
        "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "                state = next_state\n",
        "                \n",
        "        score = game.get_total_reward()\n",
        "        print(\"Score: \", score)\n",
        "    game.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
            "Score:  48.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwuFalX_KPdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}