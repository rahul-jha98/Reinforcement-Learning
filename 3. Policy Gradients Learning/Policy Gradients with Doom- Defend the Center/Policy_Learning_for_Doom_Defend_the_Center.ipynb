{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy Learning for Doom Defend the Center.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N9WtXoMY2lM",
        "colab_type": "text"
      },
      "source": [
        "## Step 0: Preparing Colab for rendering the environment and installing Vizdoom üèóÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCtp77wO6kLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "# Install deps from \n",
        "# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n",
        "\n",
        "apt-get update\n",
        "\n",
        "apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n",
        "nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n",
        "libopenal-dev timidity libwildmidi-dev unzip\n",
        "\n",
        "# Boost libraries\n",
        "apt-get install libboost-all-dev\n",
        "\n",
        "# Lua binding dependencies\n",
        "apt-get install liblua5.1-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeSLv53PqoVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install vizdoom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OorCoFRhuSF",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Importing the libraries üìö"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msJkTSBJhbuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf      # Deep Learning library\n",
        "import tensorflow as tf2\n",
        "import numpy as np           # Handle matrices\n",
        "from vizdoom import *        # Doom Environment\n",
        "\n",
        "\n",
        "import random                # Handling random number generation\n",
        "import time                  # Handling time calculation\n",
        "\n",
        "from skimage.color import rgb2gray  # Help us to gray our frames\n",
        "from skimage import transform       # Help us to preprocess the frames\n",
        "\n",
        "from collections import deque# Ordered collection with ends\n",
        "import matplotlib.pyplot as plt # Display graphs\n",
        "\n",
        "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR5d0U8Nh4v2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Disable eager execution to make it compatible with tf session module\n",
        "tf.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWFq-0vSh7rw",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Creating the environment üéÆ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Yuv15Kh6wz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "8b026331-a634-4d64-b90a-e136f86dbe36"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rahul-jha98/Reinforcement-Learning/master/3.%20Policy%20Gradients%20Learning/Policy%20Gradients%20with%20Doom-%20Defend%20the%20Center/defend_the_center.wad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-20 12:41:52--  https://raw.githubusercontent.com/rahul-jha98/Reinforcement-Learning/master/3.%20Policy%20Gradients%20Learning/Policy%20Gradients%20with%20Doom-%20Defend%20the%20Center/defend_the_center.wad\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6478 (6.3K) [application/octet-stream]\n",
            "Saving to: ‚Äòdefend_the_center.wad‚Äô\n",
            "\n",
            "\rdefend_the_center.w   0%[                    ]       0  --.-KB/s               \rdefend_the_center.w 100%[===================>]   6.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-20 12:41:53 (65.0 MB/s) - ‚Äòdefend_the_center.wad‚Äô saved [6478/6478]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NMsDLJ6rGhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "76301ce9-64f0-419e-ba89-c8c28f32d784"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rahul-jha98/Reinforcement-Learning/master/3.%20Policy%20Gradients%20Learning/Policy%20Gradients%20with%20Doom-%20Defend%20the%20Center/defend_the_center.cfg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-20 12:42:07--  https://raw.githubusercontent.com/rahul-jha98/Reinforcement-Learning/master/3.%20Policy%20Gradients%20Learning/Policy%20Gradients%20with%20Doom-%20Defend%20the%20Center/defend_the_center.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 899 [text/plain]\n",
            "Saving to: ‚Äòdefend_the_center.cfg‚Äô\n",
            "\n",
            "\rdefend_the_center.c   0%[                    ]       0  --.-KB/s               \rdefend_the_center.c 100%[===================>]     899  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-20 12:42:07 (48.8 MB/s) - ‚Äòdefend_the_center.cfg‚Äô saved [899/899]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhhmf4XRrO8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Here we create our environment\n",
        "\"\"\"\n",
        "def create_environment():\n",
        "    game = DoomGame()\n",
        "    \n",
        "    # Load the correct configuration\n",
        "    game.load_config(\"defend_the_center.cfg\")\n",
        "    # Load the correct scenario (in our case defend_the_center scenario)\n",
        "    game.set_doom_scenario_path(\"defend_the_center.wad\")\n",
        "    \n",
        "    game.set_window_visible(False)\n",
        "    game.init()\n",
        "    \n",
        "    # Here our possible actions\n",
        "    # [[1,0,0],[0,1,0],[0,0,1]]\n",
        "    possible_actions  = np.identity(3,dtype=int).tolist()\n",
        "    \n",
        "    return game, possible_actions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAPyDezuf_Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "game, possible_actions = create_environment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4B7EjaSgLZj",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Define the preprocessing functions ‚öôÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP6KWFxcgM2L",
        "colab_type": "text"
      },
      "source": [
        "### preprocess_frame üñºÔ∏è\n",
        "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
        "<br><br>\n",
        "Our steps:\n",
        "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
        "- Crop the screen (in our case we remove the roof because it contains no information)\n",
        "- We normalize pixel values\n",
        "- Finally we resize the preprocessed frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPm7L4lXgAmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    preprocess_frame:\n",
        "    Take a frame.\n",
        "    Resize it.\n",
        "        __________________\n",
        "        |                 |\n",
        "        |                 |\n",
        "        |                 |\n",
        "        |                 |\n",
        "        |_________________|\n",
        "        \n",
        "        to\n",
        "        _____________\n",
        "        |            |\n",
        "        |            |\n",
        "        |            |\n",
        "        |____________|\n",
        "    Normalize it.\n",
        "    \n",
        "    return preprocessed_frame\n",
        "    \n",
        "    \"\"\"\n",
        "def preprocess_frame(frame):\n",
        "    frame_in_gray = rgb2gray(frame) \n",
        "    \n",
        "    # Crop the screen (remove the roof because it contains no information)\n",
        "    # [Up: Down, Left: right]\n",
        "    ## If the frame is not due to game end then we also remove\n",
        "    ## the bottom part which contatins the score, health info.\n",
        "    if frame_in_gray.shape[0] == 240:\n",
        "      cropped_frame = frame_in_gray[40:-32,:]\n",
        "    else:\n",
        "      cropped_frame = frame_in_gray[40:, :]\n",
        "    \n",
        "    # Normalize Pixel Values\n",
        "    normalized_frame = cropped_frame/255.0\n",
        "    \n",
        "    # Resize\n",
        "    preprocessed_frame = transform.resize(normalized_frame, [100,160])\n",
        "    \n",
        "    return preprocessed_frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H90llygigkFK",
        "colab_type": "text"
      },
      "source": [
        "### stack_frames\n",
        "\n",
        "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
        "\n",
        "- First we preprocess frame\n",
        "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
        "- Finally we **build the stacked state**\n",
        "\n",
        "This is how work stack:\n",
        "- For the first frame, we feed 4 frames\n",
        "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
        "- And so on\n",
        "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1Wmvut1glHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stack_size = 4 # We stack 4 frames\n",
        "\n",
        "# Initialize deque with zero-images one array for each image\n",
        "stacked_frames  =  deque([np.zeros((100,160), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "    \n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros((100,160), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "        \n",
        "        # Because we're in a new episode, copy the same frame 4x\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        \n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "\n",
        "    else:\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "    \n",
        "    return stacked_state, stacked_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVvQCrAthKBN",
        "colab_type": "text"
      },
      "source": [
        "### discount_and_normalize_rewards üí∞\n",
        "This function is important, because we are in a Monte Carlo situation. <br>\n",
        "\n",
        "We need to **discount the rewards at the end of the episode**. This function takes, the reward discount it, and **then normalize them** (to avoid a big variability in rewards)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9s_5VzOhKco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discount_and_normalize_rewards(episode_rewards):\n",
        "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
        "    cumulative = 0.0\n",
        "    for i in reversed(range(len(episode_rewards))):\n",
        "        cumulative = cumulative * gamma + episode_rewards[i]\n",
        "        discounted_episode_rewards[i] = cumulative\n",
        "    \n",
        "    mean = np.mean(discounted_episode_rewards)\n",
        "    std = np.std(discounted_episode_rewards)\n",
        "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
        "\n",
        "    return discounted_episode_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK5JjsScp7z3",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
        "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
        "\n",
        "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
        "- Then, you'll add the training hyperparameters when you implement the training algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_knMp2Yp5Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ENVIRONMENT HYPERPARAMETERS\n",
        "state_size = [100,160,4] # Our input is a stack of 4 frames hence 100x160x4 (Width, height, channels) \n",
        "action_size = game.get_available_buttons_size() # 3 possible actions: turn left, turn right, shoot\n",
        "stack_size = 4 # Defines how many frames are stacked together\n",
        "\n",
        "## TRAINING HYPERPARAMETERS\n",
        "learning_rate = 0.0001 \n",
        "num_epochs = 750  # Total epochs for training \n",
        "\n",
        "batch_size = 5000 # Each 1 is a timestep (NOT AN EPISODE) # YOU CAN CHANGE TO 5000 if you have GPU\n",
        "gamma = 0.99 # Discounting rate\n",
        "\n",
        "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
        "training = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioztvIsJqKmO",
        "colab_type": "text"
      },
      "source": [
        "Quick note: Policy gradient methods like reinforce **are on-policy method which can not be updated from experience replay.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7odVhcoYqR9c",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Create our Policy Gradient Neural Network model üß†\n",
        "\n",
        "This is our Policy Gradient model:\n",
        "- We take a stack of 4 frames as input\n",
        "- It passes through 3 convnets\n",
        "- Then it is flatened\n",
        "- Finally it passes through 2 FC layers\n",
        "- It outputs a probability distribution over actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P35PDOjqE1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratePGNetwork:\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='PGNetwork'):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        with tf.variable_scope(name):\n",
        "            with tf.name_scope(\"inputs\"):\n",
        "                # We create the placeholders\n",
        "                # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
        "                # [None, 84, 84, 4]\n",
        "                self.inputs_= tf.placeholder(tf.float32, [None, *state_size], name=\"inputs_\")\n",
        "                self.actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
        "                self.discounted_episode_rewards_ = tf.placeholder(tf.float32, [None, ], name=\"discounted_episode_rewards_\")\n",
        "            \n",
        "                \n",
        "                # Add this placeholder for having this variable in tensorboard\n",
        "                self.mean_reward_ = tf.placeholder(tf.float32, name=\"mean_reward\")\n",
        "                \n",
        "            with tf.name_scope(\"conv1\"):\n",
        "                \"\"\"\n",
        "                First convnet:\n",
        "                CNN\n",
        "                BatchNormalization\n",
        "                ELU\n",
        "                \"\"\"\n",
        "                # Input is 84x84x4\n",
        "                self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
        "                                             filters = 32,\n",
        "                                             kernel_size = [8,8],\n",
        "                                             strides = [4,4],\n",
        "                                             padding = \"VALID\",\n",
        "                                             kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                             name = \"conv1\")\n",
        "\n",
        "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
        "                                                       training = True,\n",
        "                                                       epsilon = 1e-5,\n",
        "                                                         name = 'batch_norm1')\n",
        "\n",
        "                self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
        "                ## --> [20, 20, 32]\n",
        "            \n",
        "            with tf.name_scope(\"conv2\"):\n",
        "                \"\"\"\n",
        "                Second convnet:\n",
        "                CNN\n",
        "                BatchNormalization\n",
        "                ELU\n",
        "                \"\"\"\n",
        "                self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
        "                                     filters = 64,\n",
        "                                     kernel_size = [4,4],\n",
        "                                     strides = [2,2],\n",
        "                                     padding = \"VALID\",\n",
        "                                     kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                     name = \"conv2\")\n",
        "\n",
        "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
        "                                                       training = True,\n",
        "                                                       epsilon = 1e-5,\n",
        "                                                         name = 'batch_norm2')\n",
        "\n",
        "                self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
        "                ## --> [9, 9, 64]\n",
        "            \n",
        "            with tf.name_scope(\"conv3\"):\n",
        "                \"\"\"\n",
        "                Third convnet:\n",
        "                CNN\n",
        "                BatchNormalization\n",
        "                ELU\n",
        "                \"\"\"\n",
        "                self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
        "                                     filters = 128,\n",
        "                                     kernel_size = [4,4],\n",
        "                                     strides = [2,2],\n",
        "                                     padding = \"VALID\",\n",
        "                                     kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                     name = \"conv3\")\n",
        "\n",
        "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
        "                                                       training = True,\n",
        "                                                       epsilon = 1e-5,\n",
        "                                                         name = 'batch_norm3')\n",
        "\n",
        "                self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
        "                ## --> [3, 3, 128]\n",
        "            \n",
        "            with tf.name_scope(\"flatten\"):\n",
        "                self.flatten = tf.layers.flatten(self.conv3_out)\n",
        "                ## --> [1152]\n",
        "            \n",
        "            with tf.name_scope(\"fc1\"):\n",
        "                self.fc = tf.layers.dense(inputs = self.flatten,\n",
        "                                      units = 512,\n",
        "                                      activation = tf.nn.elu,\n",
        "                                       kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                    name=\"fc1\")\n",
        "            \n",
        "            with tf.name_scope(\"logits\"):\n",
        "                self.logits = tf.layers.dense(inputs = self.fc, \n",
        "                                               kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                              units = 3, \n",
        "                                            activation=None)\n",
        "            \n",
        "            with tf.name_scope(\"softmax\"):\n",
        "                self.action_distribution = tf.nn.softmax(self.logits)\n",
        "                \n",
        "\n",
        "            with tf.name_scope(\"loss\"):\n",
        "                # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
        "                # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
        "                # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
        "                self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logits, labels = self.actions)\n",
        "                self.loss = tf.reduce_mean(self.neg_log_prob * self.discounted_episode_rewards_) \n",
        "        \n",
        "    \n",
        "            with tf.name_scope(\"train\"):\n",
        "                self.train_opt = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt8q_yWbqco8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "272c7a8f-418d-42bd-950c-2be6f53fed09"
      },
      "source": [
        "# Reset the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Instantiate the PGNetwork\n",
        "PGNetwork = GeneratePGNetwork(state_size, action_size, learning_rate)\n",
        "\n",
        "# Initialize Session\n",
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-13-40b1b693244c>:34: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From <ipython-input-13-40b1b693244c>:39: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "WARNING:tensorflow:From <ipython-input-13-40b1b693244c>:91: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-13-40b1b693244c>:99: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1zYu-SKh0dg",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Set up Tensorboard üìä\n",
        "\n",
        "To launch tensorboard : `tensorboard --logdir=/tensorboard/pg/1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUo4kbDkhbBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup TensorBoard Writer\n",
        "writer = tf.summary.FileWriter(\"/tensorboard/policy_gradients/1\")\n",
        "\n",
        "## Losses\n",
        "tf.summary.scalar(\"Loss\", PGNetwork.loss)\n",
        "\n",
        "## Reward mean\n",
        "tf.summary.scalar(\"Reward_mean\", PGNetwork.mean_reward_ )\n",
        "\n",
        "write_op = tf.summary.merge_all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLmuvz9bq-KM",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Train our Agent üèÉ‚Äç‚ôÇÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jan-SMufrEm-",
        "colab_type": "text"
      },
      "source": [
        "Here we'll create batches.<br>\n",
        "These batches contains episodes **(their number depends on how many rewards we collect**: for instance if we have episodes with only 10 rewards we can put batch_size/10 episodes\n",
        "<br>\n",
        "    Algo: \n",
        "\n",
        "      Make a batch\n",
        "        For each step:\n",
        "            Choose action a\n",
        "            Perform action a\n",
        "            Store s, a, r\n",
        "            **If** done:\n",
        "                Calculate sum reward\n",
        "                Calculate gamma Gt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWqXjO7Xq7u_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_batch(batch_size, stacked_frames):\n",
        "    # Initialize lists: states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards\n",
        "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
        "    \n",
        "    # Reward of batch is also a trick to keep track of how many timestep we made.\n",
        "    # We use to to verify at the end of each episode if > batch_size or not.\n",
        "    \n",
        "    # Keep track of how many episodes in our batch (useful when we'll need to calculate the average reward per episode)\n",
        "    episode_num  = 1\n",
        "    \n",
        "    # Launch a new episode\n",
        "    game.new_episode()\n",
        "        \n",
        "    # Get a new state\n",
        "    state = game.get_state().screen_buffer\n",
        "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "    while True:\n",
        "        # Run State Through Policy & Calculate Action\n",
        "        action_probability_distribution = sess.run(PGNetwork.action_distribution, \n",
        "                                                   feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
        "        \n",
        "        # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\n",
        "        # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\n",
        "        #30% chance that we take action a2)\n",
        "        action = np.random.choice(range(action_probability_distribution.shape[1]), \n",
        "                                  p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
        "        action = possible_actions[action]\n",
        "\n",
        "        # Perform action\n",
        "        reward = game.make_action(action)\n",
        "        done = game.is_episode_finished()\n",
        "\n",
        "        # Store results\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards_of_episode.append(reward)\n",
        "        \n",
        "        if done:\n",
        "            # The episode ends so no next state\n",
        "            next_state = np.zeros((100, 160), dtype=np.int)\n",
        "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "            \n",
        "            # Append the rewards_of_batch to reward_of_episode\n",
        "            rewards_of_batch.append(rewards_of_episode)\n",
        "            \n",
        "            # Calculate gamma Gt\n",
        "            discounted_rewards.append(discount_and_normalize_rewards(rewards_of_episode))\n",
        "           \n",
        "            # If the number of rewards_of_batch > batch_size stop the minibatch creation\n",
        "            # (Because we have sufficient number of episode mb)\n",
        "            # Remember that we put this condition here, because we want entire episode (Monte Carlo)\n",
        "            # so we can't check that condition for each step but only if an episode is finished\n",
        "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
        "                break\n",
        "                \n",
        "            # Reset the transition stores\n",
        "            rewards_of_episode = []\n",
        "            \n",
        "            # Add episode\n",
        "            episode_num += 1\n",
        "            \n",
        "            # Start a new episode\n",
        "            game.new_episode()\n",
        "\n",
        "            # First we need a state\n",
        "            state = game.get_state().screen_buffer\n",
        "\n",
        "            # Stack the frames\n",
        "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "         \n",
        "        else:\n",
        "            # If not done, the next_state become the current state\n",
        "            next_state = game.get_state().screen_buffer\n",
        "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "            state = next_state\n",
        "                         \n",
        "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAxMLGs0rlHa",
        "colab_type": "text"
      },
      "source": [
        "    Create the Neural Network\n",
        "    Initialize the weights\n",
        "    Init the environment\n",
        "    maxReward = 0 # Keep track of maximum reward\n",
        "    **For** epochs in range(num_epochs):\n",
        "        Get batches\n",
        "        Optimize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4RwpeqYrjCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keep track of all rewards total for each batch\n",
        "allRewards = []\n",
        "\n",
        "total_rewards = 0\n",
        "maximumRewardRecorded = 0\n",
        "mean_reward_total = []\n",
        "epoch = 1\n",
        "average_reward = []\n",
        "\n",
        "# Saver\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "if training:\n",
        "    # Load the model\n",
        "    #saver.restore(sess, \"./models/model.ckpt\")\n",
        "\n",
        "    while epoch < num_epochs + 1:\n",
        "        # Gather training data\n",
        "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(batch_size, stacked_frames)\n",
        "\n",
        "        ### These part is used for analytics\n",
        "        # Calculate the total reward ot the batch\n",
        "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
        "        allRewards.append(total_reward_of_that_batch)\n",
        "\n",
        "        # Calculate the mean reward of the batch\n",
        "        # Total rewards of batch / nb episodes in that batch\n",
        "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
        "        mean_reward_total.append(mean_reward_of_that_batch)\n",
        "\n",
        "        # Calculate the average reward of all training\n",
        "        # mean_reward_of_that_batch / epoch\n",
        "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
        "\n",
        "        # Calculate maximum reward recorded \n",
        "        maximumRewardRecorded = np.amax(allRewards)\n",
        "\n",
        "        print(\"==========================================\")\n",
        "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
        "        print(\"-----------\")\n",
        "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
        "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
        "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
        "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
        "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
        "\n",
        "        # Feedforward, gradient and backpropagation\n",
        "        loss_, _ = sess.run([PGNetwork.loss, PGNetwork.train_opt], feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 100,160,4)),\n",
        "                                                            PGNetwork.actions: actions_mb,\n",
        "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb \n",
        "                                                                    })\n",
        "\n",
        "        print(\"Training Loss: {}\".format(loss_))\n",
        "\n",
        "        # Write TF Summaries\n",
        "        summary = sess.run(write_op, feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 100,160,4)),\n",
        "                                                            PGNetwork.actions: actions_mb,\n",
        "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb,\n",
        "                                                                    PGNetwork.mean_reward_: mean_reward_of_that_batch\n",
        "                                                                    })\n",
        "\n",
        "        #summary = sess.run(write_op, feed_dict={x: s_.reshape(len(s_),84,84,1), y:a_, d_r: d_r_, r: r_, n: n_})\n",
        "        writer.add_summary(summary, epoch)\n",
        "        writer.flush()\n",
        "\n",
        "        # Save Model\n",
        "        if epoch % 10 == 0:\n",
        "            saver.save(sess, \"./models/model.ckpt\")\n",
        "            print(\"Model saved\")\n",
        "        epoch += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHeESuv2rxKB",
        "colab_type": "text"
      },
      "source": [
        "## Step 8: Watch our Agent play üëÄ\n",
        "Now that we trained our agent, we can test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ89GdIpVR2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saver\n",
        "saver = tf.train.Saver()\n",
        "allframes = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  test_game, possible_actions = create_environment()\n",
        "\n",
        "  saver.restore(sess, \"./models/model.ckpt\")\n",
        "  # init = tf.global_variables_initializer()\n",
        "  sess.run(init)\n",
        "  test_game.init()\n",
        "  test_game.new_episode()\n",
        "  total_rewards = 0\n",
        "\n",
        "  state = test_game.get_state().screen_buffer\n",
        "  state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "\n",
        "  while not test_game.is_episode_finished():\n",
        "      allframes.append(test_game.get_state().screen_buffer)\n",
        "\n",
        "      # Run State Through Policy & Calculate Action\n",
        "      action_probability_distribution = sess.run(PGNetwork.action_distribution, \n",
        "                                                  feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
        "\n",
        "      # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\n",
        "      # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\n",
        "      #30% chance that we take action a2)\n",
        "      action = np.random.choice(range(action_probability_distribution.shape[1]), \n",
        "                                p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
        "      action = possible_actions[action]\n",
        "\n",
        "      # Perform action\n",
        "      reward = test_game.make_action(action)\n",
        "      done = test_game.is_episode_finished()\n",
        "      total_rewards += reward\n",
        "\n",
        "      if done:\n",
        "          print(\"Total Reward: \", total_rewards)\n",
        "          break\n",
        "      else:\n",
        "          # If not done, the next_state become the current state\n",
        "          next_state = test_game.get_state().screen_buffer\n",
        "          next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "          state = next_state\n",
        "\n",
        "  test_game.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IleHislfBVz",
        "colab_type": "text"
      },
      "source": [
        "### Saving frame data to images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYwpYSFWi0CQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import glob\n",
        "!rm -rf frames\n",
        "!mkdir -p frames\n",
        "for idx, frame in enumerate(allframes):\n",
        "  cv2.imwrite('frames/{:05}.jpg'.format(idx), cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPOCQaj9o7fw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ad6ee01-1db7-4288-c99f-bae0f6e2bde2"
      },
      "source": [
        "from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "from PIL import Image\n",
        "\n",
        "fourcc = VideoWriter_fourcc(*'MJPG')\n",
        "outputfn = \"defendthecenter.avi\"\n",
        "fps = 24 # reset manually\n",
        "\n",
        "filenames = glob.glob('frames/*.jpg')\n",
        "filenames = sorted(filenames)\n",
        "\n",
        "im = Image.open(filenames[0])\n",
        "vw = VideoWriter(outputfn, fourcc, fps, im.size)\n",
        "\n",
        "\n",
        "for im in filenames:\n",
        "  im = cv2.imread(im)\n",
        "  vw.write(im.astype(np.uint8))\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "vw.release()\n",
        "print(vw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<VideoWriter 0x7f9f17e91c30>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHN3VZDFSSrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Downlaod file in Colab\n",
        "from google.colab import files\n",
        "\n",
        "files.download('defendthecenter.avi')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}