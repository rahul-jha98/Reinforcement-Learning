{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy Learning for Doom Health Gathering.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N9WtXoMY2lM",
        "colab_type": "text"
      },
      "source": [
        "## Step 0: Preparing Colab for rendering the environment and installing GYM üèóÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCtp77wO6kLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "# Install deps from \n",
        "# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n",
        "\n",
        "apt-get update\n",
        "\n",
        "apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n",
        "nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n",
        "libopenal-dev timidity libwildmidi-dev unzip\n",
        "\n",
        "# Boost libraries\n",
        "apt-get install libboost-all-dev\n",
        "\n",
        "# Lua binding dependencies\n",
        "apt-get install liblua5.1-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeSLv53PqoVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install vizdoom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OorCoFRhuSF",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Importing the libraries üìö"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msJkTSBJhbuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf      # Deep Learning library\n",
        "import tensorflow as tf2\n",
        "import numpy as np           # Handle matrices\n",
        "from vizdoom import *        # Doom Environment\n",
        "\n",
        "\n",
        "import random                # Handling random number generation\n",
        "import time                  # Handling time calculation\n",
        "\n",
        "from skimage.color import rgb2gray  # Help us to gray our frames\n",
        "from skimage import transform       # Help us to preprocess the frames\n",
        "\n",
        "from collections import deque# Ordered collection with ends\n",
        "import matplotlib.pyplot as plt # Display graphs\n",
        "\n",
        "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR5d0U8Nh4v2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Disable eager execution to make it compatible with tf session module\n",
        "tf.disable_eager_execution()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWFq-0vSh7rw",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Creating the environment üéÆ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Yuv15Kh6wz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "208add65-9c07-4802-9fa3-ff94fddf29ca"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rahul-jha98/Reinforcement-Learning/master/3.%20Policy%20Gradients%20Learning/Policy%20Gradients%20with%20Doom-Health%20Gathering/health_gathering.wad"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-19 02:06:49--  https://raw.githubusercontent.com/rahul-jha98/Reinforcement-Learning-Samples/master/3.%20Policy%20Gradients%20Learning/Policy%20Gradients%20with%20Doom-Health%20Gathering/health_gathering.wad\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2837 (2.8K) [application/octet-stream]\n",
            "Saving to: ‚Äòhealth_gathering.wad‚Äô\n",
            "\n",
            "\rhealth_gathering.wa   0%[                    ]       0  --.-KB/s               \rhealth_gathering.wa 100%[===================>]   2.77K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-19 02:06:49 (53.4 MB/s) - ‚Äòhealth_gathering.wad‚Äô saved [2837/2837]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NMsDLJ6rGhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "7f58234c-edd2-43f9-ff8f-e9bbd328ab00"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rahul-jha98/Reinforcement-Learning/master/3.%20Policy%20Gradients%20Learning/Policy%20Gradients%20with%20Doom-Health%20Gathering/health_gathering.cfg"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-19 02:06:53--  https://raw.githubusercontent.com/rahul-jha98/Reinforcement-Learning-Samples/master/3.%20Policy%20Gradients%20Learning/Policy%20Gradients%20with%20Doom-Health%20Gathering/health_gathering.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 849 [text/plain]\n",
            "Saving to: ‚Äòhealth_gathering.cfg‚Äô\n",
            "\n",
            "\rhealth_gathering.cf   0%[                    ]       0  --.-KB/s               \rhealth_gathering.cf 100%[===================>]     849  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-19 02:06:53 (62.4 MB/s) - ‚Äòhealth_gathering.cfg‚Äô saved [849/849]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhhmf4XRrO8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Here we create our environment\n",
        "\"\"\"\n",
        "def create_environment():\n",
        "    game = DoomGame()\n",
        "    \n",
        "    # Load the correct configuration\n",
        "    game.load_config(\"health_gathering.cfg\")\n",
        "    \n",
        "    # Load the correct scenario (in our case defend_the_center scenario)\n",
        "    game.set_doom_scenario_path(\"health_gathering.wad\")\n",
        "    game.set_window_visible(False)\n",
        "    game.init()\n",
        "    \n",
        "    # Here our possible actions\n",
        "    # [[1,0,0],[0,1,0],[0,0,1]]\n",
        "    possible_actions  = np.identity(3,dtype=int).tolist()\n",
        "    \n",
        "    return game, possible_actions"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAPyDezuf_Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "game, possible_actions = create_environment()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4B7EjaSgLZj",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Define the preprocessing functions ‚öôÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP6KWFxcgM2L",
        "colab_type": "text"
      },
      "source": [
        "### preprocess_frame üñºÔ∏è\n",
        "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
        "<br><br>\n",
        "Our steps:\n",
        "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
        "- Crop the screen (in our case we remove the roof because it contains no information)\n",
        "- We normalize pixel values\n",
        "- Finally we resize the preprocessed frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPm7L4lXgAmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    preprocess_frame:\n",
        "    Take a frame.\n",
        "    Resize it.\n",
        "        __________________\n",
        "        |                 |\n",
        "        |                 |\n",
        "        |                 |\n",
        "        |                 |\n",
        "        |_________________|\n",
        "        \n",
        "        to\n",
        "        _____________\n",
        "        |            |\n",
        "        |            |\n",
        "        |            |\n",
        "        |____________|\n",
        "    Normalize it.\n",
        "    \n",
        "    return preprocessed_frame\n",
        "    \n",
        "    \"\"\"\n",
        "def preprocess_frame(frame):\n",
        "    frame_in_gray = rgb2gray(frame) \n",
        "    \n",
        "    # Crop the screen (remove the roof because it contains no information)\n",
        "    # [Up: Down, Left: right]\n",
        "\n",
        "    ## If the frame is not due to game end then we also remove\n",
        "    ## the bottom part which contatins the score, health info.\n",
        "    if frame_in_gray.shape[0] == 240:\n",
        "      cropped_frame = frame_in_gray[80:-32,:]\n",
        "    else:\n",
        "      cropped_frame = frame_in_gray[80:, :]\n",
        "\n",
        "\n",
        "    # Normalize Pixel Values\n",
        "    normalized_frame = cropped_frame/255.0\n",
        "    \n",
        "    # Resize\n",
        "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
        "    \n",
        "    return preprocessed_frame"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H90llygigkFK",
        "colab_type": "text"
      },
      "source": [
        "### stack_frames\n",
        "\n",
        "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
        "\n",
        "- First we preprocess frame\n",
        "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
        "- Finally we **build the stacked state**\n",
        "\n",
        "This is how work stack:\n",
        "- For the first frame, we feed 4 frames\n",
        "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
        "- And so on\n",
        "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1Wmvut1glHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stack_size = 4 # We stack 4 frames\n",
        "\n",
        "# Initialize deque with zero-images one array for each image\n",
        "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "    \n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "        \n",
        "        # Because we're in a new episode, copy the same frame 4x\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        \n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "\n",
        "    else:\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "    \n",
        "    return stacked_state, stacked_frames"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVvQCrAthKBN",
        "colab_type": "text"
      },
      "source": [
        "### discount_and_normalize_rewards üí∞\n",
        "This function is important, because we are in a Monte Carlo situation. <br>\n",
        "\n",
        "We need to **discount the rewards at the end of the episode**. This function takes, the reward discount it, and **then normalize them** (to avoid a big variability in rewards)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9s_5VzOhKco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discount_and_normalize_rewards(episode_rewards):\n",
        "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
        "    cumulative = 0.0\n",
        "    for i in reversed(range(len(episode_rewards))):\n",
        "        cumulative = cumulative * gamma + episode_rewards[i]\n",
        "        discounted_episode_rewards[i] = cumulative\n",
        "    \n",
        "    mean = np.mean(discounted_episode_rewards)\n",
        "    std = np.std(discounted_episode_rewards)\n",
        "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
        "\n",
        "    return discounted_episode_rewards"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8hJaAhahOk6",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
        "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
        "\n",
        "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
        "- Then, you'll add the training hyperparameters when you implement the training algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hiWtYD3hMWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ENVIRONMENT HYPERPARAMETERS\n",
        "state_size = [84,84,4] # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
        "action_size = game.get_available_buttons_size() # 3 possible actions: turn left, turn right, move forward\n",
        "stack_size = 4 # Defines how many frames are stacked together\n",
        "\n",
        "## TRAINING HYPERPARAMETERS\n",
        "learning_rate = 0.002\n",
        "num_epochs = 500 # Total epochs for training \n",
        "\n",
        "batch_size = 3000 # Timesteps for the a batch.\n",
        "gamma = 0.95      # Discounting rate\n",
        "\n",
        "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
        "training = True"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H09lz4XhTnv",
        "colab_type": "text"
      },
      "source": [
        "Quick note: Policy gradient methods like reinforce **are on-policy method which can not be updated from experience replay.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2IgZK-hVrr",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Create our Policy Gradient Neural Network model üß†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7WHomBJhWBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CreatePGNetwork:\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='PGNetwork'):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        with tf.variable_scope(name):\n",
        "            with tf.name_scope(\"inputs\"):\n",
        "                # We create the placeholders\n",
        "                # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
        "                # [None, 84, 84, 4]\n",
        "                self.inputs_= tf.placeholder(tf.float32, [None, *state_size], name=\"inputs_\")\n",
        "                self.actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
        "                self.discounted_episode_rewards_ = tf.placeholder(tf.float32, [None, ], name=\"discounted_episode_rewards_\")\n",
        "            \n",
        "                \n",
        "                # Add this placeholder for having this variable in tensorboard\n",
        "                self.mean_reward_ = tf.placeholder(tf.float32, name=\"mean_reward\")\n",
        "                \n",
        "            with tf.name_scope(\"conv1\"):\n",
        "                \"\"\"\n",
        "                First convnet:\n",
        "                CNN\n",
        "                BatchNormalization\n",
        "                ELU\n",
        "                \"\"\"\n",
        "                # Input is 84x84x4\n",
        "                self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
        "                                             filters = 32,\n",
        "                                             kernel_size = [8,8],\n",
        "                                             strides = [4,4],\n",
        "                                             padding = \"VALID\",\n",
        "                                              kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                             name = \"conv1\")\n",
        "\n",
        "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
        "                                                       training = True,\n",
        "                                                       epsilon = 1e-5,\n",
        "                                                         name = 'batch_norm1')\n",
        "\n",
        "                self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
        "                ## --> [20, 20, 32]\n",
        "            \n",
        "            with tf.name_scope(\"conv2\"):\n",
        "                \"\"\"\n",
        "                Second convnet:\n",
        "                CNN\n",
        "                BatchNormalization\n",
        "                ELU\n",
        "                \"\"\"\n",
        "                self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
        "                                     filters = 64,\n",
        "                                     kernel_size = [4,4],\n",
        "                                     strides = [2,2],\n",
        "                                     padding = \"VALID\",\n",
        "                                    kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                     name = \"conv2\")\n",
        "\n",
        "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
        "                                                       training = True,\n",
        "                                                       epsilon = 1e-5,\n",
        "                                                         name = 'batch_norm2')\n",
        "\n",
        "                self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
        "                ## --> [9, 9, 64]\n",
        "            \n",
        "            with tf.name_scope(\"conv3\"):\n",
        "                \"\"\"\n",
        "                Third convnet:\n",
        "                CNN\n",
        "                BatchNormalization\n",
        "                ELU\n",
        "                \"\"\"\n",
        "                self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
        "                                     filters = 128,\n",
        "                                     kernel_size = [4,4],\n",
        "                                     strides = [2,2],\n",
        "                                     padding = \"VALID\",\n",
        "                                    kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                     name = \"conv3\")\n",
        "\n",
        "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
        "                                                       training = True,\n",
        "                                                       epsilon = 1e-5,\n",
        "                                                         name = 'batch_norm3')\n",
        "\n",
        "                self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
        "                ## --> [3, 3, 128]\n",
        "            \n",
        "            with tf.name_scope(\"flatten\"):\n",
        "                self.flatten = tf.layers.flatten(self.conv3_out)\n",
        "                ## --> [1152]\n",
        "            \n",
        "            with tf.name_scope(\"fc1\"):\n",
        "                self.fc = tf.layers.dense(inputs = self.flatten,\n",
        "                                      units = 512,\n",
        "                                      activation = tf.nn.elu,\n",
        "                                      kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                    name=\"fc1\")\n",
        "            \n",
        "            with tf.name_scope(\"logits\"):\n",
        "                self.logits = tf.layers.dense(inputs = self.fc, \n",
        "                                              kernel_initializer=tf2.keras.initializers.GlorotUniform(),\n",
        "                                              units = 3, \n",
        "                                            activation=None)\n",
        "            \n",
        "            with tf.name_scope(\"softmax\"):\n",
        "                self.action_distribution = tf.nn.softmax(self.logits)\n",
        "                \n",
        "\n",
        "            with tf.name_scope(\"loss\"):\n",
        "                # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
        "                # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
        "                # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
        "                self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logits, labels = self.actions)\n",
        "                self.loss = tf.reduce_mean(self.neg_log_prob * self.discounted_episode_rewards_) \n",
        "        \n",
        "    \n",
        "            with tf.name_scope(\"train\"):\n",
        "                self.train_opt = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRu-r2uhhZCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Instantiate the PGNetwork\n",
        "PGNetwork = CreatePGNetwork(state_size, action_size, learning_rate)\n",
        "\n",
        "# Initialize Session\n",
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1zYu-SKh0dg",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Set up Tensorboard üìä\n",
        "\n",
        "To launch tensorboard : `tensorboard --logdir=/tensorboard/pg/1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUo4kbDkhbBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup TensorBoard Writer\n",
        "writer = tf.summary.FileWriter(\"/tensorboard/pg/test\")\n",
        "\n",
        "## Losses\n",
        "tf.summary.scalar(\"Loss\", PGNetwork.loss)\n",
        "\n",
        "## Reward mean\n",
        "tf.summary.scalar(\"Reward_mean\", PGNetwork.mean_reward_ )\n",
        "\n",
        "write_op = tf.summary.merge_all()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55vk8rDch5qh",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Train our Agent üèÉ‚Äç‚ôÇÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkt6ZvoGh97U",
        "colab_type": "text"
      },
      "source": [
        "Here we'll create batches.<br>\n",
        "These batches contains episodes **(their number depends on how many rewards we collect**: for instance if we have episodes with only 10 rewards we can put batch_size/10 episodes\n",
        "<br>\n",
        "    Algo: \n",
        "\n",
        "      Make a batch\n",
        "        For each step:\n",
        "            Choose action a\n",
        "            Perform action a\n",
        "            Store s, a, r\n",
        "            **If** done:\n",
        "                Calculate sum reward\n",
        "                Calculate gamma Gt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UcrHLi4h8WS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_batch(batch_size, stacked_frames):\n",
        "    # Initialize lists: states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards\n",
        "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
        "    \n",
        "    # Reward of batch is also a trick to keep track of how many timestep we made.\n",
        "    # We use to to verify at the end of each episode if > batch_size or not.\n",
        "    \n",
        "    # Keep track of how many episodes in our batch (useful when we'll need to calculate the average reward per episode)\n",
        "    episode_num  = 1\n",
        "    \n",
        "    # Launch a new episode\n",
        "    game.new_episode()\n",
        "        \n",
        "    # Get a new state\n",
        "    state = game.get_state().screen_buffer\n",
        "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "    while True:\n",
        "        # Run State Through Policy & Calculate Action\n",
        "        action_probability_distribution = sess.run(PGNetwork.action_distribution, \n",
        "                                                   feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
        "        \n",
        "        # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\n",
        "        # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\n",
        "        #30% chance that we take action a2)\n",
        "        action = np.random.choice(range(action_probability_distribution.shape[1]), \n",
        "                                  p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
        "        action = possible_actions[action]\n",
        "\n",
        "        # Perform action\n",
        "        reward = game.make_action(action)\n",
        "        done = game.is_episode_finished()\n",
        "\n",
        "        # Store results\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards_of_episode.append(reward)\n",
        "        \n",
        "        if done:\n",
        "            # The episode ends so no next state\n",
        "            next_state = np.zeros((84, 84), dtype=np.int)\n",
        "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "            \n",
        "            # Append the rewards_of_batch to reward_of_episode\n",
        "            rewards_of_batch.append(rewards_of_episode)\n",
        "            \n",
        "            # Calculate gamma Gt\n",
        "            discounted_rewards.append(discount_and_normalize_rewards(rewards_of_episode))\n",
        "           \n",
        "            # If the number of rewards_of_batch > batch_size stop the minibatch creation\n",
        "            # (Because we have sufficient number of episode mb)\n",
        "            # Remember that we put this condition here, because we want entire episode (Monte Carlo)\n",
        "            # so we can't check that condition for each step but only if an episode is finished\n",
        "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
        "                break\n",
        "                \n",
        "            # Reset the transition stores\n",
        "            rewards_of_episode = []\n",
        "            \n",
        "            # Add episode\n",
        "            episode_num += 1\n",
        "            \n",
        "            # Start a new episode\n",
        "            game.new_episode()\n",
        "\n",
        "            # First we need a state\n",
        "            state = game.get_state().screen_buffer\n",
        "\n",
        "            # Stack the frames\n",
        "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "         \n",
        "        else:\n",
        "            # If not done, the next_state become the current state\n",
        "            next_state = game.get_state().screen_buffer\n",
        "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "            state = next_state\n",
        "                         \n",
        "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNuaa91Ui1sM",
        "colab_type": "text"
      },
      "source": [
        "    Create the Neural Network\n",
        "    Initialize the weights\n",
        "    Init the environment\n",
        "    maxReward = 0 # Keep track of maximum reward\n",
        "    **For** epochs in range(num_epochs):\n",
        "        Get batches\n",
        "        Optimize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgUiRQnkjC1H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4424dcd5-edcd-40c6-aadc-7fbf934edbfd"
      },
      "source": [
        "# Keep track of all rewards total for each batch\n",
        "allRewards = []\n",
        "\n",
        "total_rewards = 0\n",
        "maximumRewardRecorded = 0\n",
        "mean_reward_total = []\n",
        "epoch = 1\n",
        "average_reward = []\n",
        "\n",
        "# Saver\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "if training:\n",
        "    # Load the model\n",
        "    # saver.restore(sess, \"./models/model.ckpt\")\n",
        "\n",
        "    while epoch < num_epochs + 1:\n",
        "        # Gather training data\n",
        "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(batch_size, stacked_frames)\n",
        "\n",
        "        ### These part is used for analytics\n",
        "        # Calculate the total reward ot the batch\n",
        "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
        "        allRewards.append(total_reward_of_that_batch)\n",
        "\n",
        "        # Calculate the mean reward of the batch\n",
        "        # Total rewards of batch / nb episodes in that batch\n",
        "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
        "        mean_reward_total.append(mean_reward_of_that_batch)\n",
        "\n",
        "        # Calculate the average reward of all training\n",
        "        # mean_reward_of_that_batch / epoch\n",
        "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
        "\n",
        "        # Calculate maximum reward recorded \n",
        "        maximumRewardRecorded = np.amax(allRewards)\n",
        "\n",
        "        print(\"==========================================\")\n",
        "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
        "        print(\"-----------\")\n",
        "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
        "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
        "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
        "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
        "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
        "\n",
        "        # Feedforward, gradient and backpropagation\n",
        "        loss_, _ = sess.run([PGNetwork.loss, PGNetwork.train_opt], feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
        "                                                            PGNetwork.actions: actions_mb,\n",
        "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb \n",
        "                                                                    })\n",
        "\n",
        "        print(\"Training Loss: {}\".format(loss_))\n",
        "\n",
        "        # Write TF Summaries\n",
        "        summary = sess.run(write_op, feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
        "                                                            PGNetwork.actions: actions_mb,\n",
        "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb,\n",
        "                                                                    PGNetwork.mean_reward_: mean_reward_of_that_batch\n",
        "                                                                    })\n",
        "\n",
        "        #summary = sess.run(write_op, feed_dict={x: s_.reshape(len(s_),84,84,1), y:a_, d_r: d_r_, r: r_, n: n_})\n",
        "        writer.add_summary(summary, epoch)\n",
        "        writer.flush()\n",
        "\n",
        "        # Save Model\n",
        "        if epoch % 10 == 0:\n",
        "            saver.save(sess, \"./models/model.ckpt\")\n",
        "            print(\"Model saved\")\n",
        "        epoch += 1"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========================================\n",
            "Epoch:  1 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2820.0\n",
            "Mean Reward of that batch 402.85714285714283\n",
            "Average Reward of all training: 402.85714285714283\n",
            "Max reward for a batch so far: 2820.0\n",
            "Training Loss: -0.05897117778658867\n",
            "==========================================\n",
            "Epoch:  2 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2408.0\n",
            "Mean Reward of that batch 401.3333333333333\n",
            "Average Reward of all training: 402.0952380952381\n",
            "Max reward for a batch so far: 2820.0\n",
            "Training Loss: 0.012303103692829609\n",
            "==========================================\n",
            "Epoch:  3 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2728.0\n",
            "Mean Reward of that batch 454.6666666666667\n",
            "Average Reward of all training: 419.61904761904765\n",
            "Max reward for a batch so far: 2820.0\n",
            "Training Loss: -0.06005857512354851\n",
            "==========================================\n",
            "Epoch:  4 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2884.0\n",
            "Mean Reward of that batch 412.0\n",
            "Average Reward of all training: 417.7142857142857\n",
            "Max reward for a batch so far: 2884.0\n",
            "Training Loss: -0.011664925143122673\n",
            "==========================================\n",
            "Epoch:  5 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2468.0\n",
            "Mean Reward of that batch 352.57142857142856\n",
            "Average Reward of all training: 404.6857142857143\n",
            "Max reward for a batch so far: 2884.0\n",
            "Training Loss: -0.12346036732196808\n",
            "==========================================\n",
            "Epoch:  6 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2852.0\n",
            "Mean Reward of that batch 407.42857142857144\n",
            "Average Reward of all training: 405.14285714285717\n",
            "Max reward for a batch so far: 2884.0\n",
            "Training Loss: -0.10806256532669067\n",
            "==========================================\n",
            "Epoch:  7 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2724.0\n",
            "Mean Reward of that batch 389.14285714285717\n",
            "Average Reward of all training: 402.85714285714295\n",
            "Max reward for a batch so far: 2884.0\n",
            "Training Loss: -0.07827316969633102\n",
            "==========================================\n",
            "Epoch:  8 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2760.0\n",
            "Mean Reward of that batch 460.0\n",
            "Average Reward of all training: 410.0\n",
            "Max reward for a batch so far: 2884.0\n",
            "Training Loss: -0.022457610815763474\n",
            "==========================================\n",
            "Epoch:  9 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 3400.0\n",
            "Mean Reward of that batch 566.6666666666666\n",
            "Average Reward of all training: 427.4074074074074\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: -0.006201973184943199\n",
            "==========================================\n",
            "Epoch:  10 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2568.0\n",
            "Mean Reward of that batch 428.0\n",
            "Average Reward of all training: 427.4666666666666\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: -0.15471939742565155\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  11 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2532.0\n",
            "Mean Reward of that batch 361.7142857142857\n",
            "Average Reward of all training: 421.4891774891774\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: -0.024803198873996735\n",
            "==========================================\n",
            "Epoch:  12 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2632.0\n",
            "Mean Reward of that batch 438.6666666666667\n",
            "Average Reward of all training: 422.9206349206349\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.06020502373576164\n",
            "==========================================\n",
            "Epoch:  13 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2656.0\n",
            "Mean Reward of that batch 332.0\n",
            "Average Reward of all training: 415.92673992673986\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.0652712881565094\n",
            "==========================================\n",
            "Epoch:  14 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2596.0\n",
            "Mean Reward of that batch 370.85714285714283\n",
            "Average Reward of all training: 412.70748299319723\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.001970710000023246\n",
            "==========================================\n",
            "Epoch:  15 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2756.0\n",
            "Mean Reward of that batch 393.7142857142857\n",
            "Average Reward of all training: 411.4412698412698\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.042615003883838654\n",
            "==========================================\n",
            "Epoch:  16 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2472.0\n",
            "Mean Reward of that batch 412.0\n",
            "Average Reward of all training: 411.4761904761905\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.029316863045096397\n",
            "==========================================\n",
            "Epoch:  17 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2468.0\n",
            "Mean Reward of that batch 352.57142857142856\n",
            "Average Reward of all training: 408.01120448179273\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.1414995640516281\n",
            "==========================================\n",
            "Epoch:  18 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2660.0\n",
            "Mean Reward of that batch 380.0\n",
            "Average Reward of all training: 406.4550264550264\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.046022314578294754\n",
            "==========================================\n",
            "Epoch:  19 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2532.0\n",
            "Mean Reward of that batch 361.7142857142857\n",
            "Average Reward of all training: 404.1002506265664\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.1776653528213501\n",
            "==========================================\n",
            "Epoch:  20 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2948.0\n",
            "Mean Reward of that batch 421.14285714285717\n",
            "Average Reward of all training: 404.9523809523809\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.10920614004135132\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  21 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2600.0\n",
            "Mean Reward of that batch 433.3333333333333\n",
            "Average Reward of all training: 406.30385487528343\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.11588221788406372\n",
            "==========================================\n",
            "Epoch:  22 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2536.0\n",
            "Mean Reward of that batch 422.6666666666667\n",
            "Average Reward of all training: 407.04761904761904\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.0794866532087326\n",
            "==========================================\n",
            "Epoch:  23 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 404.08281573498965\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.08645576238632202\n",
            "==========================================\n",
            "Epoch:  24 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2532.0\n",
            "Mean Reward of that batch 361.7142857142857\n",
            "Average Reward of all training: 402.3174603174604\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.08668515831232071\n",
            "==========================================\n",
            "Epoch:  25 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2660.0\n",
            "Mean Reward of that batch 380.0\n",
            "Average Reward of all training: 401.42476190476197\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.016952084377408028\n",
            "==========================================\n",
            "Epoch:  26 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2564.0\n",
            "Mean Reward of that batch 366.2857142857143\n",
            "Average Reward of all training: 400.07326007326014\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.0903455913066864\n",
            "==========================================\n",
            "Epoch:  27 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2436.0\n",
            "Mean Reward of that batch 348.0\n",
            "Average Reward of all training: 398.14462081128755\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.047443099319934845\n",
            "==========================================\n",
            "Epoch:  28 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2340.0\n",
            "Mean Reward of that batch 334.2857142857143\n",
            "Average Reward of all training: 395.8639455782313\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.09441779553890228\n",
            "==========================================\n",
            "Epoch:  29 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2592.0\n",
            "Mean Reward of that batch 324.0\n",
            "Average Reward of all training: 393.3858784893268\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.12117050588130951\n",
            "==========================================\n",
            "Epoch:  30 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2308.0\n",
            "Mean Reward of that batch 329.7142857142857\n",
            "Average Reward of all training: 391.2634920634921\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.07087894529104233\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  31 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2564.0\n",
            "Mean Reward of that batch 366.2857142857143\n",
            "Average Reward of all training: 390.457757296467\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.06192920729517937\n",
            "==========================================\n",
            "Epoch:  32 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2564.0\n",
            "Mean Reward of that batch 366.2857142857143\n",
            "Average Reward of all training: 389.70238095238096\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.06772647798061371\n",
            "==========================================\n",
            "Epoch:  33 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2692.0\n",
            "Mean Reward of that batch 384.57142857142856\n",
            "Average Reward of all training: 389.5468975468976\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.12168239802122116\n",
            "==========================================\n",
            "Epoch:  34 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 388.0560224089636\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.1181720718741417\n",
            "==========================================\n",
            "Epoch:  35 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2468.0\n",
            "Mean Reward of that batch 352.57142857142856\n",
            "Average Reward of all training: 387.04217687074834\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.0845474973320961\n",
            "==========================================\n",
            "Epoch:  36 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2528.0\n",
            "Mean Reward of that batch 316.0\n",
            "Average Reward of all training: 385.06878306878315\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.0012756700161844492\n",
            "==========================================\n",
            "Epoch:  37 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2436.0\n",
            "Mean Reward of that batch 348.0\n",
            "Average Reward of all training: 384.0669240669241\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.044521771371364594\n",
            "==========================================\n",
            "Epoch:  38 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2308.0\n",
            "Mean Reward of that batch 329.7142857142857\n",
            "Average Reward of all training: 382.6365914786968\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.07184727489948273\n",
            "==========================================\n",
            "Epoch:  39 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2468.0\n",
            "Mean Reward of that batch 352.57142857142856\n",
            "Average Reward of all training: 381.86568986568994\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.07813499122858047\n",
            "==========================================\n",
            "Epoch:  40 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2500.0\n",
            "Mean Reward of that batch 357.14285714285717\n",
            "Average Reward of all training: 381.247619047619\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.05988093465566635\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  41 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2408.0\n",
            "Mean Reward of that batch 401.3333333333333\n",
            "Average Reward of all training: 381.73751451800234\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.07439521700143814\n",
            "==========================================\n",
            "Epoch:  42 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2468.0\n",
            "Mean Reward of that batch 352.57142857142856\n",
            "Average Reward of all training: 381.0430839002268\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.06424669176340103\n",
            "==========================================\n",
            "Epoch:  43 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2340.0\n",
            "Mean Reward of that batch 334.2857142857143\n",
            "Average Reward of all training: 379.9557032115172\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.061072394251823425\n",
            "==========================================\n",
            "Epoch:  44 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2468.0\n",
            "Mean Reward of that batch 352.57142857142856\n",
            "Average Reward of all training: 379.33333333333337\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.011508440598845482\n",
            "==========================================\n",
            "Epoch:  45 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 3236.0\n",
            "Mean Reward of that batch 462.2857142857143\n",
            "Average Reward of all training: 381.1767195767196\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.10520820319652557\n",
            "==========================================\n",
            "Epoch:  46 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2468.0\n",
            "Mean Reward of that batch 352.57142857142856\n",
            "Average Reward of all training: 380.55486542443066\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.11000041663646698\n",
            "==========================================\n",
            "Epoch:  47 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2436.0\n",
            "Mean Reward of that batch 348.0\n",
            "Average Reward of all training: 379.86220871327254\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: -0.0011569608468562365\n",
            "==========================================\n",
            "Epoch:  48 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 379.00793650793656\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.03585987165570259\n",
            "==========================================\n",
            "Epoch:  49 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2592.0\n",
            "Mean Reward of that batch 324.0\n",
            "Average Reward of all training: 377.885325558795\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.016399439424276352\n",
            "==========================================\n",
            "Epoch:  50 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 377.1047619047619\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.1389712393283844\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  51 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2564.0\n",
            "Mean Reward of that batch 366.2857142857143\n",
            "Average Reward of all training: 376.89262371615314\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.0812959298491478\n",
            "==========================================\n",
            "Epoch:  52 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2408.0\n",
            "Mean Reward of that batch 401.3333333333333\n",
            "Average Reward of all training: 377.3626373626373\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.06843090802431107\n",
            "==========================================\n",
            "Epoch:  53 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2692.0\n",
            "Mean Reward of that batch 384.57142857142856\n",
            "Average Reward of all training: 377.4986522911051\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.12157024443149567\n",
            "==========================================\n",
            "Epoch:  54 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2468.0\n",
            "Mean Reward of that batch 352.57142857142856\n",
            "Average Reward of all training: 377.03703703703695\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.08949063718318939\n",
            "==========================================\n",
            "Epoch:  55 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 376.34285714285704\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.07748972624540329\n",
            "==========================================\n",
            "Epoch:  56 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 375.67346938775506\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.08003931492567062\n",
            "==========================================\n",
            "Epoch:  57 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 3300.0\n",
            "Mean Reward of that batch 471.42857142857144\n",
            "Average Reward of all training: 377.3533834586466\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.045156292617321014\n",
            "==========================================\n",
            "Epoch:  58 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2340.0\n",
            "Mean Reward of that batch 334.2857142857143\n",
            "Average Reward of all training: 376.6108374384236\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.1020706444978714\n",
            "==========================================\n",
            "Epoch:  59 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2308.0\n",
            "Mean Reward of that batch 329.7142857142857\n",
            "Average Reward of all training: 375.8159806295399\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.10737349838018417\n",
            "==========================================\n",
            "Epoch:  60 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2532.0\n",
            "Mean Reward of that batch 361.7142857142857\n",
            "Average Reward of all training: 375.58095238095234\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.034077856689691544\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  61 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2536.0\n",
            "Mean Reward of that batch 422.6666666666667\n",
            "Average Reward of all training: 376.3528493364559\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.05646156892180443\n",
            "==========================================\n",
            "Epoch:  62 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2440.0\n",
            "Mean Reward of that batch 406.6666666666667\n",
            "Average Reward of all training: 376.84178187403995\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.08630848675966263\n",
            "==========================================\n",
            "Epoch:  63 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2736.0\n",
            "Mean Reward of that batch 684.0\n",
            "Average Reward of all training: 381.71730914588056\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.036189548671245575\n",
            "==========================================\n",
            "Epoch:  64 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2472.0\n",
            "Mean Reward of that batch 412.0\n",
            "Average Reward of all training: 382.19047619047615\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.02304169535636902\n",
            "==========================================\n",
            "Epoch:  65 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2960.0\n",
            "Mean Reward of that batch 740.0\n",
            "Average Reward of all training: 387.69523809523804\n",
            "Max reward for a batch so far: 3400.0\n",
            "Training Loss: 0.0599132776260376\n",
            "==========================================\n",
            "Epoch:  66 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 4296.0\n",
            "Mean Reward of that batch 1074.0\n",
            "Average Reward of all training: 398.09379509379505\n",
            "Max reward for a batch so far: 4296.0\n",
            "Training Loss: 0.008523321710526943\n",
            "==========================================\n",
            "Epoch:  67 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4516.0\n",
            "Mean Reward of that batch 1505.3333333333333\n",
            "Average Reward of all training: 414.61975835110155\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: 0.011410840786993504\n",
            "==========================================\n",
            "Epoch:  68 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2672.0\n",
            "Mean Reward of that batch 668.0\n",
            "Average Reward of all training: 418.3459383753501\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: 0.0022933781147003174\n",
            "==========================================\n",
            "Epoch:  69 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3440.0\n",
            "Mean Reward of that batch 1720.0\n",
            "Average Reward of all training: 437.21048999309863\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: 0.03294973820447922\n",
            "==========================================\n",
            "Epoch:  70 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 4260.0\n",
            "Mean Reward of that batch 852.0\n",
            "Average Reward of all training: 443.1360544217687\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: 0.0041025057435035706\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  71 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2724.0\n",
            "Mean Reward of that batch 389.14285714285717\n",
            "Average Reward of all training: 442.37558685446004\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: -0.028170619159936905\n",
            "==========================================\n",
            "Epoch:  72 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3112.0\n",
            "Mean Reward of that batch 778.0\n",
            "Average Reward of all training: 447.037037037037\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: 0.008541014045476913\n",
            "==========================================\n",
            "Epoch:  73 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2896.0\n",
            "Mean Reward of that batch 724.0\n",
            "Average Reward of all training: 450.83105022831046\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: 0.016593320295214653\n",
            "==========================================\n",
            "Epoch:  74 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3916.0\n",
            "Mean Reward of that batch 783.2\n",
            "Average Reward of all training: 455.32252252252243\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: -0.01867946796119213\n",
            "==========================================\n",
            "Epoch:  75 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3492.0\n",
            "Mean Reward of that batch 698.4\n",
            "Average Reward of all training: 458.5635555555555\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: -0.03810376673936844\n",
            "==========================================\n",
            "Epoch:  76 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3568.0\n",
            "Mean Reward of that batch 1784.0\n",
            "Average Reward of all training: 476.0035087719298\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: 0.061448000371456146\n",
            "==========================================\n",
            "Epoch:  77 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 3656.0\n",
            "Mean Reward of that batch 609.3333333333334\n",
            "Average Reward of all training: 477.73506493506494\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: -0.01595114730298519\n",
            "==========================================\n",
            "Epoch:  78 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2992.0\n",
            "Mean Reward of that batch 748.0\n",
            "Average Reward of all training: 481.2\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: 0.05761536583304405\n",
            "==========================================\n",
            "Epoch:  79 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 4200.0\n",
            "Mean Reward of that batch 2100.0\n",
            "Average Reward of all training: 501.6911392405063\n",
            "Max reward for a batch so far: 4516.0\n",
            "Training Loss: -0.026720941066741943\n",
            "==========================================\n",
            "Epoch:  80 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4804.0\n",
            "Mean Reward of that batch 1601.3333333333333\n",
            "Average Reward of all training: 515.4366666666666\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.018348008394241333\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  81 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3248.0\n",
            "Mean Reward of that batch 1624.0\n",
            "Average Reward of all training: 529.1226337448559\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.003477879799902439\n",
            "==========================================\n",
            "Epoch:  82 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3340.0\n",
            "Mean Reward of that batch 1113.3333333333333\n",
            "Average Reward of all training: 536.2471544715447\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.019663101062178612\n",
            "==========================================\n",
            "Epoch:  83 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3660.0\n",
            "Mean Reward of that batch 1220.0\n",
            "Average Reward of all training: 544.485140562249\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.054151829332113266\n",
            "==========================================\n",
            "Epoch:  84 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3412.0\n",
            "Mean Reward of that batch 1137.3333333333333\n",
            "Average Reward of all training: 551.5428571428571\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.007121444679796696\n",
            "==========================================\n",
            "Epoch:  85 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4172.0\n",
            "Mean Reward of that batch 1390.6666666666667\n",
            "Average Reward of all training: 561.4149019607843\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.029865819960832596\n",
            "==========================================\n",
            "Epoch:  86 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2664.0\n",
            "Mean Reward of that batch 444.0\n",
            "Average Reward of all training: 560.0496124031007\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.010138592682778835\n",
            "==========================================\n",
            "Epoch:  87 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2608.0\n",
            "Mean Reward of that batch 652.0\n",
            "Average Reward of all training: 561.1065134099616\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.1569794863462448\n",
            "==========================================\n",
            "Epoch:  88 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2728.0\n",
            "Mean Reward of that batch 454.6666666666667\n",
            "Average Reward of all training: 559.8969696969697\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.010793820023536682\n",
            "==========================================\n",
            "Epoch:  89 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2500.0\n",
            "Mean Reward of that batch 357.14285714285717\n",
            "Average Reward of all training: 557.6188336008561\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.05861149728298187\n",
            "==========================================\n",
            "Epoch:  90 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2560.0\n",
            "Mean Reward of that batch 320.0\n",
            "Average Reward of all training: 554.9786243386243\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.06949687004089355\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  91 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2404.0\n",
            "Mean Reward of that batch 343.42857142857144\n",
            "Average Reward of all training: 552.6538984824699\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.02545347437262535\n",
            "==========================================\n",
            "Epoch:  92 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2656.0\n",
            "Mean Reward of that batch 332.0\n",
            "Average Reward of all training: 550.255486542443\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.022463468834757805\n",
            "==========================================\n",
            "Epoch:  93 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2472.0\n",
            "Mean Reward of that batch 412.0\n",
            "Average Reward of all training: 548.7688684075781\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.02277376689016819\n",
            "==========================================\n",
            "Epoch:  94 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2564.0\n",
            "Mean Reward of that batch 366.2857142857143\n",
            "Average Reward of all training: 546.8275582573456\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.06109536811709404\n",
            "==========================================\n",
            "Epoch:  95 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 544.638395989975\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.009684816002845764\n",
            "==========================================\n",
            "Epoch:  96 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2468.0\n",
            "Mean Reward of that batch 352.57142857142856\n",
            "Average Reward of all training: 542.6376984126983\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.0016616013599559665\n",
            "==========================================\n",
            "Epoch:  97 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2540.0\n",
            "Mean Reward of that batch 508.0\n",
            "Average Reward of all training: 542.2806087383407\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.4393657445907593\n",
            "==========================================\n",
            "Epoch:  98 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2920.0\n",
            "Mean Reward of that batch 486.6666666666667\n",
            "Average Reward of all training: 541.7131195335277\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.07743652164936066\n",
            "==========================================\n",
            "Epoch:  99 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3184.0\n",
            "Mean Reward of that batch 796.0\n",
            "Average Reward of all training: 544.2816738816738\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.13428692519664764\n",
            "==========================================\n",
            "Epoch:  100 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 3368.0\n",
            "Mean Reward of that batch 561.3333333333334\n",
            "Average Reward of all training: 544.4521904761905\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.5360846519470215\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  101 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2952.0\n",
            "Mean Reward of that batch 492.0\n",
            "Average Reward of all training: 543.9328618576143\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.07465491443872452\n",
            "==========================================\n",
            "Epoch:  102 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2308.0\n",
            "Mean Reward of that batch 329.7142857142857\n",
            "Average Reward of all training: 541.832679738562\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.8219265937805176\n",
            "==========================================\n",
            "Epoch:  103 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2532.0\n",
            "Mean Reward of that batch 361.7142857142857\n",
            "Average Reward of all training: 540.0839574664816\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.1527615338563919\n",
            "==========================================\n",
            "Epoch:  104 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2468.0\n",
            "Mean Reward of that batch 352.57142857142856\n",
            "Average Reward of all training: 538.2809523809524\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.01870539039373398\n",
            "==========================================\n",
            "Epoch:  105 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2308.0\n",
            "Mean Reward of that batch 329.7142857142857\n",
            "Average Reward of all training: 536.2946031746031\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.04998249188065529\n",
            "==========================================\n",
            "Epoch:  106 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2340.0\n",
            "Mean Reward of that batch 334.2857142857143\n",
            "Average Reward of all training: 534.3888589398023\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.020361468195915222\n",
            "==========================================\n",
            "Epoch:  107 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2432.0\n",
            "Mean Reward of that batch 304.0\n",
            "Average Reward of all training: 532.2356920338228\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.2199837328807916e-05\n",
            "==========================================\n",
            "Epoch:  108 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2308.0\n",
            "Mean Reward of that batch 329.7142857142857\n",
            "Average Reward of all training: 530.3604938271604\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.01306651346385479\n",
            "==========================================\n",
            "Epoch:  109 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2624.0\n",
            "Mean Reward of that batch 328.0\n",
            "Average Reward of all training: 528.5039755351681\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.014588657766580582\n",
            "==========================================\n",
            "Epoch:  110 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2496.0\n",
            "Mean Reward of that batch 312.0\n",
            "Average Reward of all training: 526.5357575757575\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.014494280330836773\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  111 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2340.0\n",
            "Mean Reward of that batch 334.2857142857143\n",
            "Average Reward of all training: 524.8037752037752\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.02186395786702633\n",
            "==========================================\n",
            "Epoch:  112 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2304.0\n",
            "Mean Reward of that batch 288.0\n",
            "Average Reward of all training: 522.6894557823128\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.01194540224969387\n",
            "==========================================\n",
            "Epoch:  113 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2340.0\n",
            "Mean Reward of that batch 334.2857142857143\n",
            "Average Reward of all training: 521.0221660345554\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.04896126687526703\n",
            "==========================================\n",
            "Epoch:  114 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2560.0\n",
            "Mean Reward of that batch 320.0\n",
            "Average Reward of all training: 519.258813700919\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.011441988870501518\n",
            "==========================================\n",
            "Epoch:  115 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2340.0\n",
            "Mean Reward of that batch 334.2857142857143\n",
            "Average Reward of all training: 517.6503519668737\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.010811541229486465\n",
            "==========================================\n",
            "Epoch:  116 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2528.0\n",
            "Mean Reward of that batch 316.0\n",
            "Average Reward of all training: 515.911986863711\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.556881776807131e-06\n",
            "==========================================\n",
            "Epoch:  117 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 514.3986975986976\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 5.824098025186686e-06\n",
            "==========================================\n",
            "Epoch:  118 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2528.0\n",
            "Mean Reward of that batch 316.0\n",
            "Average Reward of all training: 512.7173527037934\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.1423954826314e-06\n",
            "==========================================\n",
            "Epoch:  119 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2592.0\n",
            "Mean Reward of that batch 324.0\n",
            "Average Reward of all training: 511.13149259703886\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.01163777057081461\n",
            "==========================================\n",
            "Epoch:  120 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2560.0\n",
            "Mean Reward of that batch 320.0\n",
            "Average Reward of all training: 509.5387301587302\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.4171592890052125e-05\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  121 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2336.0\n",
            "Mean Reward of that batch 292.0\n",
            "Average Reward of all training: 507.74088941361674\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.715651084552519e-05\n",
            "==========================================\n",
            "Epoch:  122 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2528.0\n",
            "Mean Reward of that batch 316.0\n",
            "Average Reward of all training: 506.1692427790789\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.011564209125936031\n",
            "==========================================\n",
            "Epoch:  123 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2464.0\n",
            "Mean Reward of that batch 308.0\n",
            "Average Reward of all training: 504.55811072396443\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.2213118427316658e-05\n",
            "==========================================\n",
            "Epoch:  124 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2592.0\n",
            "Mean Reward of that batch 324.0\n",
            "Average Reward of all training: 503.10199692780344\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.9967283151345327e-05\n",
            "==========================================\n",
            "Epoch:  125 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2432.0\n",
            "Mean Reward of that batch 304.0\n",
            "Average Reward of all training: 501.509180952381\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.1144978745724075e-05\n",
            "==========================================\n",
            "Epoch:  126 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2308.0\n",
            "Mean Reward of that batch 329.7142857142857\n",
            "Average Reward of all training: 500.1457294028723\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.5014560631243512e-05\n",
            "==========================================\n",
            "Epoch:  127 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2464.0\n",
            "Mean Reward of that batch 308.0\n",
            "Average Reward of all training: 498.63277090363704\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.7027734429575503e-05\n",
            "==========================================\n",
            "Epoch:  128 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2368.0\n",
            "Mean Reward of that batch 296.0\n",
            "Average Reward of all training: 497.0497023809524\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.008094626478850842\n",
            "==========================================\n",
            "Epoch:  129 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2624.0\n",
            "Mean Reward of that batch 328.0\n",
            "Average Reward of all training: 495.73923957179767\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.626709126867354e-05\n",
            "==========================================\n",
            "Epoch:  130 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2592.0\n",
            "Mean Reward of that batch 324.0\n",
            "Average Reward of all training: 494.4181684981685\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.0795201635337435e-05\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  131 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2496.0\n",
            "Mean Reward of that batch 312.0\n",
            "Average Reward of all training: 493.025663395129\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.7691874745651148e-05\n",
            "==========================================\n",
            "Epoch:  132 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2400.0\n",
            "Mean Reward of that batch 300.0\n",
            "Average Reward of all training: 491.56334776334774\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.0010806539794430137\n",
            "==========================================\n",
            "Epoch:  133 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2624.0\n",
            "Mean Reward of that batch 328.0\n",
            "Average Reward of all training: 490.3335481561045\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.009652620181441307\n",
            "==========================================\n",
            "Epoch:  134 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2464.0\n",
            "Mean Reward of that batch 308.0\n",
            "Average Reward of all training: 488.97285003553657\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.005228592548519373\n",
            "==========================================\n",
            "Epoch:  135 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2528.0\n",
            "Mean Reward of that batch 316.0\n",
            "Average Reward of all training: 487.69156966490294\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.0474630218814127e-05\n",
            "==========================================\n",
            "Epoch:  136 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2340.0\n",
            "Mean Reward of that batch 334.2857142857143\n",
            "Average Reward of all training: 486.5635854341736\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.949476765934378e-05\n",
            "==========================================\n",
            "Epoch:  137 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2688.0\n",
            "Mean Reward of that batch 336.0\n",
            "Average Reward of all training: 485.46458116093146\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.0050424616783857346\n",
            "==========================================\n",
            "Epoch:  138 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2592.0\n",
            "Mean Reward of that batch 324.0\n",
            "Average Reward of all training: 484.2945479641131\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.0873750397877302e-05\n",
            "==========================================\n",
            "Epoch:  139 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2528.0\n",
            "Mean Reward of that batch 316.0\n",
            "Average Reward of all training: 483.0837958204864\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 6.75389455864206e-05\n",
            "==========================================\n",
            "Epoch:  140 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2560.0\n",
            "Mean Reward of that batch 320.0\n",
            "Average Reward of all training: 481.9189115646258\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.2982759876176715e-05\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  141 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2432.0\n",
            "Mean Reward of that batch 304.0\n",
            "Average Reward of all training: 480.6570753123944\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 6.18373178440379e-06\n",
            "==========================================\n",
            "Epoch:  142 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2464.0\n",
            "Mean Reward of that batch 308.0\n",
            "Average Reward of all training: 479.4411804158282\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 6.344729172269581e-06\n",
            "==========================================\n",
            "Epoch:  143 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2368.0\n",
            "Mean Reward of that batch 296.0\n",
            "Average Reward of all training: 478.1583749583749\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.2218367373861838e-05\n",
            "==========================================\n",
            "Epoch:  144 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2308.0\n",
            "Mean Reward of that batch 329.7142857142857\n",
            "Average Reward of all training: 477.1275132275132\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.7327725294744596e-05\n",
            "==========================================\n",
            "Epoch:  145 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2368.0\n",
            "Mean Reward of that batch 296.0\n",
            "Average Reward of all training: 475.87835796387515\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.0290380487276707e-05\n",
            "==========================================\n",
            "Epoch:  146 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2464.0\n",
            "Mean Reward of that batch 308.0\n",
            "Average Reward of all training: 474.7285061969993\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.00044323777547106147\n",
            "==========================================\n",
            "Epoch:  147 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2464.0\n",
            "Mean Reward of that batch 308.0\n",
            "Average Reward of all training: 473.59429867184963\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.013491869904100895\n",
            "==========================================\n",
            "Epoch:  148 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2308.0\n",
            "Mean Reward of that batch 329.7142857142857\n",
            "Average Reward of all training: 472.6221364221364\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.2394118786905892e-05\n",
            "==========================================\n",
            "Epoch:  149 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2464.0\n",
            "Mean Reward of that batch 308.0\n",
            "Average Reward of all training: 471.51728986896774\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -4.159653326496482e-05\n",
            "==========================================\n",
            "Epoch:  150 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2464.0\n",
            "Mean Reward of that batch 308.0\n",
            "Average Reward of all training: 470.4271746031746\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -7.14729176252149e-05\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  151 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2528.0\n",
            "Mean Reward of that batch 316.0\n",
            "Average Reward of all training: 469.40447808262377\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.00017119206313509494\n",
            "==========================================\n",
            "Epoch:  152 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 468.54561403508774\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.0011091002961620688\n",
            "==========================================\n",
            "Epoch:  153 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2592.0\n",
            "Mean Reward of that batch 324.0\n",
            "Average Reward of all training: 467.600871459695\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.17717692255973816\n",
            "==========================================\n",
            "Epoch:  154 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2528.0\n",
            "Mean Reward of that batch 316.0\n",
            "Average Reward of all training: 466.61645021645023\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.1447749137878418\n",
            "==========================================\n",
            "Epoch:  155 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2696.0\n",
            "Mean Reward of that batch 449.3333333333333\n",
            "Average Reward of all training: 466.50494623655914\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.23054037988185883\n",
            "==========================================\n",
            "Epoch:  156 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 465.68669108669116\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.14228199422359467\n",
            "==========================================\n",
            "Epoch:  157 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2500.0\n",
            "Mean Reward of that batch 357.14285714285717\n",
            "Average Reward of all training: 464.9953290870488\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.8303889036178589\n",
            "==========================================\n",
            "Epoch:  158 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2824.0\n",
            "Mean Reward of that batch 470.6666666666667\n",
            "Average Reward of all training: 465.031223628692\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.007944864220917225\n",
            "==========================================\n",
            "Epoch:  159 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 464.2376759508835\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.0063239336013794\n",
            "==========================================\n",
            "Epoch:  160 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2536.0\n",
            "Mean Reward of that batch 422.6666666666667\n",
            "Average Reward of all training: 463.9778571428571\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.4872695207595825\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  161 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2632.0\n",
            "Mean Reward of that batch 438.6666666666667\n",
            "Average Reward of all training: 463.8206447796509\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.48381027579307556\n",
            "==========================================\n",
            "Epoch:  162 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2564.0\n",
            "Mean Reward of that batch 366.2857142857143\n",
            "Average Reward of all training: 463.21857730746615\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.197607398033142\n",
            "==========================================\n",
            "Epoch:  163 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3020.0\n",
            "Mean Reward of that batch 604.0\n",
            "Average Reward of all training: 464.0822670172363\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.8735874891281128\n",
            "==========================================\n",
            "Epoch:  164 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 4368.0\n",
            "Mean Reward of that batch 1092.0\n",
            "Average Reward of all training: 467.91103368176533\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.45516952872276306\n",
            "==========================================\n",
            "Epoch:  165 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2720.0\n",
            "Mean Reward of that batch 340.0\n",
            "Average Reward of all training: 467.13581529581523\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.38477349281311035\n",
            "==========================================\n",
            "Epoch:  166 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2536.0\n",
            "Mean Reward of that batch 422.6666666666667\n",
            "Average Reward of all training: 466.86792885829027\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -5.52888822555542\n",
            "==========================================\n",
            "Epoch:  167 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2404.0\n",
            "Mean Reward of that batch 343.42857142857144\n",
            "Average Reward of all training: 466.1287710293698\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -5.403000831604004\n",
            "==========================================\n",
            "Epoch:  168 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2528.0\n",
            "Mean Reward of that batch 316.0\n",
            "Average Reward of all training: 465.23514739229023\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.9967679977416992\n",
            "==========================================\n",
            "Epoch:  169 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2404.0\n",
            "Mean Reward of that batch 343.42857142857144\n",
            "Average Reward of all training: 464.51439842209066\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -3.3126399517059326\n",
            "==========================================\n",
            "Epoch:  170 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2704.0\n",
            "Mean Reward of that batch 676.0\n",
            "Average Reward of all training: 465.75843137254896\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -4.746747970581055\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  171 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2572.0\n",
            "Mean Reward of that batch 514.4\n",
            "Average Reward of all training: 466.04288499025336\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -23.576568603515625\n",
            "==========================================\n",
            "Epoch:  172 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2708.0\n",
            "Mean Reward of that batch 902.6666666666666\n",
            "Average Reward of all training: 468.5813953488372\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 9.820538520812988\n",
            "==========================================\n",
            "Epoch:  173 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2664.0\n",
            "Mean Reward of that batch 444.0\n",
            "Average Reward of all training: 468.4393063583815\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -4.623202323913574\n",
            "==========================================\n",
            "Epoch:  174 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2956.0\n",
            "Mean Reward of that batch 591.2\n",
            "Average Reward of all training: 469.1448275862068\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -16.87632179260254\n",
            "==========================================\n",
            "Epoch:  175 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2828.0\n",
            "Mean Reward of that batch 942.6666666666666\n",
            "Average Reward of all training: 471.8506666666666\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.26501649618148804\n",
            "==========================================\n",
            "Epoch:  176 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3796.0\n",
            "Mean Reward of that batch 1265.3333333333333\n",
            "Average Reward of all training: 476.35909090909087\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.1839492321014404\n",
            "==========================================\n",
            "Epoch:  177 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3272.0\n",
            "Mean Reward of that batch 818.0\n",
            "Average Reward of all training: 478.2892655367231\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -18.68589210510254\n",
            "==========================================\n",
            "Epoch:  178 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 3976.0\n",
            "Mean Reward of that batch 662.6666666666666\n",
            "Average Reward of all training: 479.32509363295884\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -34.20308303833008\n",
            "==========================================\n",
            "Epoch:  179 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3696.0\n",
            "Mean Reward of that batch 924.0\n",
            "Average Reward of all training: 481.80931098696465\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -51.52024459838867\n",
            "==========================================\n",
            "Epoch:  180 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2956.0\n",
            "Mean Reward of that batch 985.3333333333334\n",
            "Average Reward of all training: 484.6066666666666\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -9.685006141662598\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  181 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2404.0\n",
            "Mean Reward of that batch 343.42857142857144\n",
            "Average Reward of all training: 483.82667719021305\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 12.679116249084473\n",
            "==========================================\n",
            "Epoch:  182 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 4036.0\n",
            "Mean Reward of that batch 807.2\n",
            "Average Reward of all training: 485.60345368916796\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -13.524271011352539\n",
            "==========================================\n",
            "Epoch:  183 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2508.0\n",
            "Mean Reward of that batch 501.6\n",
            "Average Reward of all training: 485.6908665105386\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 9.86314868927002\n",
            "==========================================\n",
            "Epoch:  184 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2508.0\n",
            "Mean Reward of that batch 501.6\n",
            "Average Reward of all training: 485.7773291925466\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -42.40146255493164\n",
            "==========================================\n",
            "Epoch:  185 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2740.0\n",
            "Mean Reward of that batch 913.3333333333334\n",
            "Average Reward of all training: 488.0884427284427\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 10.748906135559082\n",
            "==========================================\n",
            "Epoch:  186 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3052.0\n",
            "Mean Reward of that batch 1017.3333333333334\n",
            "Average Reward of all training: 490.93384536610347\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.8896207809448242\n",
            "==========================================\n",
            "Epoch:  187 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2928.0\n",
            "Mean Reward of that batch 732.0\n",
            "Average Reward of all training: 492.2229691876751\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.036014262586832047\n",
            "==========================================\n",
            "Epoch:  188 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3084.0\n",
            "Mean Reward of that batch 616.8\n",
            "Average Reward of all training: 492.8856129685917\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 12.028701782226562\n",
            "==========================================\n",
            "Epoch:  189 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2440.0\n",
            "Mean Reward of that batch 406.6666666666667\n",
            "Average Reward of all training: 492.42942806752336\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 10.421202659606934\n",
            "==========================================\n",
            "Epoch:  190 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2828.0\n",
            "Mean Reward of that batch 565.6\n",
            "Average Reward of all training: 492.81453634085216\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -3.0159189701080322\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  191 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2724.0\n",
            "Mean Reward of that batch 389.14285714285717\n",
            "Average Reward of all training: 492.2717526801296\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.3352887034416199\n",
            "==========================================\n",
            "Epoch:  192 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2852.0\n",
            "Mean Reward of that batch 407.42857142857144\n",
            "Average Reward of all training: 491.8298611111111\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 11.420138359069824\n",
            "==========================================\n",
            "Epoch:  193 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2660.0\n",
            "Mean Reward of that batch 380.0\n",
            "Average Reward of all training: 491.25043177892917\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.164154529571533\n",
            "==========================================\n",
            "Epoch:  194 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3240.0\n",
            "Mean Reward of that batch 810.0\n",
            "Average Reward of all training: 492.893470790378\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 5.842581748962402\n",
            "==========================================\n",
            "Epoch:  195 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2832.0\n",
            "Mean Reward of that batch 708.0\n",
            "Average Reward of all training: 493.9965811965812\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.198209047317505\n",
            "==========================================\n",
            "Epoch:  196 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 493.2050534499514\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.0699666738510132\n",
            "==========================================\n",
            "Epoch:  197 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2528.0\n",
            "Mean Reward of that batch 316.0\n",
            "Average Reward of all training: 492.30553541213436\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.7487947940826416\n",
            "==========================================\n",
            "Epoch:  198 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2636.0\n",
            "Mean Reward of that batch 527.2\n",
            "Average Reward of all training: 492.48177008177004\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 5.162295818328857\n",
            "==========================================\n",
            "Epoch:  199 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2572.0\n",
            "Mean Reward of that batch 514.4\n",
            "Average Reward of all training: 492.5919119406557\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -5.353800296783447\n",
            "==========================================\n",
            "Epoch:  200 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2560.0\n",
            "Mean Reward of that batch 320.0\n",
            "Average Reward of all training: 491.7289523809524\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -9.658113479614258\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  201 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2528.0\n",
            "Mean Reward of that batch 316.0\n",
            "Average Reward of all training: 490.8546789860223\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.8251900672912598\n",
            "==========================================\n",
            "Epoch:  202 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2632.0\n",
            "Mean Reward of that batch 438.6666666666667\n",
            "Average Reward of all training: 490.59632248939175\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -5.521909236907959\n",
            "==========================================\n",
            "Epoch:  203 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 3012.0\n",
            "Mean Reward of that batch 430.2857142857143\n",
            "Average Reward of all training: 490.2992258972554\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.901137828826904\n",
            "==========================================\n",
            "Epoch:  204 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2504.0\n",
            "Mean Reward of that batch 417.3333333333333\n",
            "Average Reward of all training: 489.9415499533147\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.630982875823975\n",
            "==========================================\n",
            "Epoch:  205 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2888.0\n",
            "Mean Reward of that batch 481.3333333333333\n",
            "Average Reward of all training: 489.8995586527294\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.588766098022461\n",
            "==========================================\n",
            "Epoch:  206 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2604.0\n",
            "Mean Reward of that batch 520.8\n",
            "Average Reward of all training: 490.04956079519184\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -9.104982376098633\n",
            "==========================================\n",
            "Epoch:  207 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2692.0\n",
            "Mean Reward of that batch 384.57142857142856\n",
            "Average Reward of all training: 489.54000460087417\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.173811674118042\n",
            "==========================================\n",
            "Epoch:  208 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2340.0\n",
            "Mean Reward of that batch 334.2857142857143\n",
            "Average Reward of all training: 488.79358974358973\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.7581045627593994\n",
            "==========================================\n",
            "Epoch:  209 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2536.0\n",
            "Mean Reward of that batch 422.6666666666667\n",
            "Average Reward of all training: 488.47719298245613\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -9.044822692871094\n",
            "==========================================\n",
            "Epoch:  210 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2404.0\n",
            "Mean Reward of that batch 343.42857142857144\n",
            "Average Reward of all training: 487.786485260771\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.224343776702881\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  211 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2632.0\n",
            "Mean Reward of that batch 438.6666666666667\n",
            "Average Reward of all training: 487.5536899119838\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -12.90029239654541\n",
            "==========================================\n",
            "Epoch:  212 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2568.0\n",
            "Mean Reward of that batch 428.0\n",
            "Average Reward of all training: 487.27277628032346\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 5.128429412841797\n",
            "==========================================\n",
            "Epoch:  213 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2664.0\n",
            "Mean Reward of that batch 444.0\n",
            "Average Reward of all training: 487.0696177062374\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.7661564350128174\n",
            "==========================================\n",
            "Epoch:  214 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2472.0\n",
            "Mean Reward of that batch 412.0\n",
            "Average Reward of all training: 486.71882510013353\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.3080966472625732\n",
            "==========================================\n",
            "Epoch:  215 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2568.0\n",
            "Mean Reward of that batch 428.0\n",
            "Average Reward of all training: 486.4457142857143\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.0022058486938477\n",
            "==========================================\n",
            "Epoch:  216 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2504.0\n",
            "Mean Reward of that batch 417.3333333333333\n",
            "Average Reward of all training: 486.12574955908286\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.9607019424438477\n",
            "==========================================\n",
            "Epoch:  217 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2756.0\n",
            "Mean Reward of that batch 393.7142857142857\n",
            "Average Reward of all training: 485.69989027869207\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.882777214050293\n",
            "==========================================\n",
            "Epoch:  218 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3212.0\n",
            "Mean Reward of that batch 642.4\n",
            "Average Reward of all training: 486.4186981214504\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.2723405361175537\n",
            "==========================================\n",
            "Epoch:  219 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2540.0\n",
            "Mean Reward of that batch 508.0\n",
            "Average Reward of all training: 486.5172428788867\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.7013198137283325\n",
            "==========================================\n",
            "Epoch:  220 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2800.0\n",
            "Mean Reward of that batch 700.0\n",
            "Average Reward of all training: 487.48761904761903\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.800277829170227\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  221 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3152.0\n",
            "Mean Reward of that batch 788.0\n",
            "Average Reward of all training: 488.84740357681534\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.3504369258880615\n",
            "==========================================\n",
            "Epoch:  222 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2668.0\n",
            "Mean Reward of that batch 533.6\n",
            "Average Reward of all training: 489.0489918489918\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.37059083580970764\n",
            "==========================================\n",
            "Epoch:  223 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2760.0\n",
            "Mean Reward of that batch 460.0\n",
            "Average Reward of all training: 488.91872731155235\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.1345576047897339\n",
            "==========================================\n",
            "Epoch:  224 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3084.0\n",
            "Mean Reward of that batch 616.8\n",
            "Average Reward of all training: 489.4896258503401\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.5574830770492554\n",
            "==========================================\n",
            "Epoch:  225 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2604.0\n",
            "Mean Reward of that batch 520.8\n",
            "Average Reward of all training: 489.62878306878304\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.23596297204494476\n",
            "==========================================\n",
            "Epoch:  226 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2920.0\n",
            "Mean Reward of that batch 486.6666666666667\n",
            "Average Reward of all training: 489.6156763590391\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.7136548757553101\n",
            "==========================================\n",
            "Epoch:  227 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3848.0\n",
            "Mean Reward of that batch 962.0\n",
            "Average Reward of all training: 491.6966645689112\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.0851311683654785\n",
            "==========================================\n",
            "Epoch:  228 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3632.0\n",
            "Mean Reward of that batch 908.0\n",
            "Average Reward of all training: 493.52255639097734\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.7276926040649414\n",
            "==========================================\n",
            "Epoch:  229 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2996.0\n",
            "Mean Reward of that batch 998.6666666666666\n",
            "Average Reward of all training: 495.7284258681638\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.3959437310695648\n",
            "==========================================\n",
            "Epoch:  230 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2568.0\n",
            "Mean Reward of that batch 428.0\n",
            "Average Reward of all training: 495.4339544513457\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.2026121616363525\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  231 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3156.0\n",
            "Mean Reward of that batch 1052.0\n",
            "Average Reward of all training: 497.8433312719026\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.155057907104492\n",
            "==========================================\n",
            "Epoch:  232 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4012.0\n",
            "Mean Reward of that batch 1337.3333333333333\n",
            "Average Reward of all training: 501.4618226600985\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.3226539194583893\n",
            "==========================================\n",
            "Epoch:  233 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2860.0\n",
            "Mean Reward of that batch 953.3333333333334\n",
            "Average Reward of all training: 503.4011853668506\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.9608012437820435\n",
            "==========================================\n",
            "Epoch:  234 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2496.0\n",
            "Mean Reward of that batch 312.0\n",
            "Average Reward of all training: 502.58323158323157\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.220561146736145\n",
            "==========================================\n",
            "Epoch:  235 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2828.0\n",
            "Mean Reward of that batch 565.6\n",
            "Average Reward of all training: 502.85138804457955\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.28228721022605896\n",
            "==========================================\n",
            "Epoch:  236 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3152.0\n",
            "Mean Reward of that batch 788.0\n",
            "Average Reward of all training: 504.0596448748991\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.4901452362537384\n",
            "==========================================\n",
            "Epoch:  237 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3020.0\n",
            "Mean Reward of that batch 604.0\n",
            "Average Reward of all training: 504.48133413703033\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.8636778593063354\n",
            "==========================================\n",
            "Epoch:  238 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3152.0\n",
            "Mean Reward of that batch 1576.0\n",
            "Average Reward of all training: 508.98351340536215\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.5128552317619324\n",
            "==========================================\n",
            "Epoch:  239 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2440.0\n",
            "Mean Reward of that batch 406.6666666666667\n",
            "Average Reward of all training: 508.55540944411234\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.933357834815979\n",
            "==========================================\n",
            "Epoch:  240 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2572.0\n",
            "Mean Reward of that batch 514.4\n",
            "Average Reward of all training: 508.57976190476194\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.4969818592071533\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  241 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3212.0\n",
            "Mean Reward of that batch 1070.6666666666667\n",
            "Average Reward of all training: 510.9120727129026\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.77760124206543\n",
            "==========================================\n",
            "Epoch:  242 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2532.0\n",
            "Mean Reward of that batch 361.7142857142857\n",
            "Average Reward of all training: 510.2955529319166\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.165083408355713\n",
            "==========================================\n",
            "Epoch:  243 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2692.0\n",
            "Mean Reward of that batch 384.57142857142856\n",
            "Average Reward of all training: 509.7781697040956\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.2545858323574066\n",
            "==========================================\n",
            "Epoch:  244 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2440.0\n",
            "Mean Reward of that batch 406.6666666666667\n",
            "Average Reward of all training: 509.3555815768931\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.4020541906356812\n",
            "==========================================\n",
            "Epoch:  245 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2440.0\n",
            "Mean Reward of that batch 406.6666666666667\n",
            "Average Reward of all training: 508.93644314868806\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.666159749031067\n",
            "==========================================\n",
            "Epoch:  246 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2632.0\n",
            "Mean Reward of that batch 438.6666666666667\n",
            "Average Reward of all training: 508.6507936507937\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -3.4440722465515137\n",
            "==========================================\n",
            "Epoch:  247 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2504.0\n",
            "Mean Reward of that batch 417.3333333333333\n",
            "Average Reward of all training: 508.281087333719\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.5572599172592163\n",
            "==========================================\n",
            "Epoch:  248 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2568.0\n",
            "Mean Reward of that batch 428.0\n",
            "Average Reward of all training: 507.9573732718894\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.3094007968902588\n",
            "==========================================\n",
            "Epoch:  249 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 4112.0\n",
            "Mean Reward of that batch 1028.0\n",
            "Average Reward of all training: 510.04589787722324\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.19293518364429474\n",
            "==========================================\n",
            "Epoch:  250 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3592.0\n",
            "Mean Reward of that batch 898.0\n",
            "Average Reward of all training: 511.59771428571435\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.6968092322349548\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  251 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4580.0\n",
            "Mean Reward of that batch 1526.6666666666667\n",
            "Average Reward of all training: 515.6418136975906\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.8214584589004517\n",
            "==========================================\n",
            "Epoch:  252 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4684.0\n",
            "Mean Reward of that batch 1561.3333333333333\n",
            "Average Reward of all training: 519.7913832199547\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.6376303434371948\n",
            "==========================================\n",
            "Epoch:  253 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3256.0\n",
            "Mean Reward of that batch 1628.0\n",
            "Average Reward of all training: 524.1716544325241\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 6.813598155975342\n",
            "==========================================\n",
            "Epoch:  254 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3348.0\n",
            "Mean Reward of that batch 1116.0\n",
            "Average Reward of all training: 526.5016872890889\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.695085048675537\n",
            "==========================================\n",
            "Epoch:  255 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2728.0\n",
            "Mean Reward of that batch 454.6666666666667\n",
            "Average Reward of all training: 526.2199813258637\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.7220912575721741\n",
            "==========================================\n",
            "Epoch:  256 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3020.0\n",
            "Mean Reward of that batch 1006.6666666666666\n",
            "Average Reward of all training: 528.0967261904761\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.8651889562606812\n",
            "==========================================\n",
            "Epoch:  257 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2636.0\n",
            "Mean Reward of that batch 527.2\n",
            "Average Reward of all training: 528.0932369835093\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.20598633587360382\n",
            "==========================================\n",
            "Epoch:  258 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2724.0\n",
            "Mean Reward of that batch 389.14285714285717\n",
            "Average Reward of all training: 527.554669619786\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.026106119155884\n",
            "==========================================\n",
            "Epoch:  259 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3248.0\n",
            "Mean Reward of that batch 812.0\n",
            "Average Reward of all training: 528.6529141386285\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.170579433441162\n",
            "==========================================\n",
            "Epoch:  260 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2796.0\n",
            "Mean Reward of that batch 559.2\n",
            "Average Reward of all training: 528.770402930403\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.16487383842468262\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  261 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3448.0\n",
            "Mean Reward of that batch 1724.0\n",
            "Average Reward of all training: 533.3498266739646\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.8809990882873535\n",
            "==========================================\n",
            "Epoch:  262 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2724.0\n",
            "Mean Reward of that batch 389.14285714285717\n",
            "Average Reward of all training: 532.7994183933115\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.11564593762159348\n",
            "==========================================\n",
            "Epoch:  263 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2924.0\n",
            "Mean Reward of that batch 584.8\n",
            "Average Reward of all training: 532.9971392359225\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.6386076807975769\n",
            "==========================================\n",
            "Epoch:  264 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2956.0\n",
            "Mean Reward of that batch 591.2\n",
            "Average Reward of all training: 533.2176046176046\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.567145049571991\n",
            "==========================================\n",
            "Epoch:  265 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2724.0\n",
            "Mean Reward of that batch 389.14285714285717\n",
            "Average Reward of all training: 532.6739263252471\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.8406671285629272\n",
            "==========================================\n",
            "Epoch:  266 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 3016.0\n",
            "Mean Reward of that batch 502.6666666666667\n",
            "Average Reward of all training: 532.5611170784103\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.14317980408668518\n",
            "==========================================\n",
            "Epoch:  267 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2860.0\n",
            "Mean Reward of that batch 953.3333333333334\n",
            "Average Reward of all training: 534.1370429819867\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.2852281332015991\n",
            "==========================================\n",
            "Epoch:  268 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3532.0\n",
            "Mean Reward of that batch 1177.3333333333333\n",
            "Average Reward of all training: 536.5370291400142\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.08459847420454025\n",
            "==========================================\n",
            "Epoch:  269 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3628.0\n",
            "Mean Reward of that batch 725.6\n",
            "Average Reward of all training: 537.2398654629138\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.1393425464630127\n",
            "==========================================\n",
            "Epoch:  270 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2832.0\n",
            "Mean Reward of that batch 708.0\n",
            "Average Reward of all training: 537.8723104056438\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.5874836444854736\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  271 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3372.0\n",
            "Mean Reward of that batch 1124.0\n",
            "Average Reward of all training: 540.0351432085749\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.2526838779449463\n",
            "==========================================\n",
            "Epoch:  272 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2372.0\n",
            "Mean Reward of that batch 338.85714285714283\n",
            "Average Reward of all training: 539.2955182072828\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.8551385402679443\n",
            "==========================================\n",
            "Epoch:  273 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3508.0\n",
            "Mean Reward of that batch 1169.3333333333333\n",
            "Average Reward of all training: 541.6033490319204\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.4632481336593628\n",
            "==========================================\n",
            "Epoch:  274 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2996.0\n",
            "Mean Reward of that batch 998.6666666666666\n",
            "Average Reward of all training: 543.2714633298575\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.1704349517822266\n",
            "==========================================\n",
            "Epoch:  275 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2768.0\n",
            "Mean Reward of that batch 692.0\n",
            "Average Reward of all training: 543.8122943722943\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.5542987585067749\n",
            "==========================================\n",
            "Epoch:  276 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2464.0\n",
            "Mean Reward of that batch 308.0\n",
            "Average Reward of all training: 542.9579020013803\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.2668568789958954\n",
            "==========================================\n",
            "Epoch:  277 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3756.0\n",
            "Mean Reward of that batch 1252.0\n",
            "Average Reward of all training: 545.5176207667182\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.012890814803540707\n",
            "==========================================\n",
            "Epoch:  278 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3248.0\n",
            "Mean Reward of that batch 1624.0\n",
            "Average Reward of all training: 549.397053785543\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.039364174008369446\n",
            "==========================================\n",
            "Epoch:  279 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3116.0\n",
            "Mean Reward of that batch 1038.6666666666667\n",
            "Average Reward of all training: 551.1507083119985\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.31267619132995605\n",
            "==========================================\n",
            "Epoch:  280 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2468.0\n",
            "Mean Reward of that batch 352.57142857142856\n",
            "Average Reward of all training: 550.4414965986393\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.809732437133789\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  281 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2740.0\n",
            "Mean Reward of that batch 913.3333333333334\n",
            "Average Reward of all training: 551.7329266226063\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.6519626975059509\n",
            "==========================================\n",
            "Epoch:  282 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3080.0\n",
            "Mean Reward of that batch 770.0\n",
            "Average Reward of all training: 552.5069233367105\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.23107975721359253\n",
            "==========================================\n",
            "Epoch:  283 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3532.0\n",
            "Mean Reward of that batch 1177.3333333333333\n",
            "Average Reward of all training: 554.7147905098435\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.5847181081771851\n",
            "==========================================\n",
            "Epoch:  284 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2888.0\n",
            "Mean Reward of that batch 481.3333333333333\n",
            "Average Reward of all training: 554.4564050972502\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.3726324439048767\n",
            "==========================================\n",
            "Epoch:  285 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2860.0\n",
            "Mean Reward of that batch 572.0\n",
            "Average Reward of all training: 554.5179615705931\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.7195032238960266\n",
            "==========================================\n",
            "Epoch:  286 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2440.0\n",
            "Mean Reward of that batch 406.6666666666667\n",
            "Average Reward of all training: 554.000999000999\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.1555129736661911\n",
            "==========================================\n",
            "Epoch:  287 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2436.0\n",
            "Mean Reward of that batch 348.0\n",
            "Average Reward of all training: 553.283225485316\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.27456900477409363\n",
            "==========================================\n",
            "Epoch:  288 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2880.0\n",
            "Mean Reward of that batch 360.0\n",
            "Average Reward of all training: 552.6121031746031\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.12101871520280838\n",
            "==========================================\n",
            "Epoch:  289 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2960.0\n",
            "Mean Reward of that batch 740.0\n",
            "Average Reward of all training: 553.2605042016806\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.17751014232635498\n",
            "==========================================\n",
            "Epoch:  290 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3084.0\n",
            "Mean Reward of that batch 616.8\n",
            "Average Reward of all training: 553.47960591133\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.2238481044769287\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  291 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2440.0\n",
            "Mean Reward of that batch 406.6666666666667\n",
            "Average Reward of all training: 552.9750940926199\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.1550857573747635\n",
            "==========================================\n",
            "Epoch:  292 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2560.0\n",
            "Mean Reward of that batch 320.0\n",
            "Average Reward of all training: 552.1772341813438\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.3499530255794525\n",
            "==========================================\n",
            "Epoch:  293 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2440.0\n",
            "Mean Reward of that batch 406.6666666666667\n",
            "Average Reward of all training: 551.680611084024\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.12244265526533127\n",
            "==========================================\n",
            "Epoch:  294 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2596.0\n",
            "Mean Reward of that batch 370.85714285714283\n",
            "Average Reward of all training: 551.0655652737286\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.06465855985879898\n",
            "==========================================\n",
            "Epoch:  295 / 500\n",
            "-----------\n",
            "Number of training episodes: 8\n",
            "Total reward: 2560.0\n",
            "Mean Reward of that batch 320.0\n",
            "Average Reward of all training: 550.2822921711057\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -16.142967224121094\n",
            "==========================================\n",
            "Epoch:  296 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2824.0\n",
            "Mean Reward of that batch 470.6666666666667\n",
            "Average Reward of all training: 550.0133204633205\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -4.322126865386963\n",
            "==========================================\n",
            "Epoch:  297 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2852.0\n",
            "Mean Reward of that batch 407.42857142857144\n",
            "Average Reward of all training: 549.5332371332371\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -33.40669250488281\n",
            "==========================================\n",
            "Epoch:  298 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3028.0\n",
            "Mean Reward of that batch 1009.3333333333334\n",
            "Average Reward of all training: 551.0761904761904\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -7.51810884475708\n",
            "==========================================\n",
            "Epoch:  299 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 4200.0\n",
            "Mean Reward of that batch 2100.0\n",
            "Average Reward of all training: 556.2565376652333\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 8.721148490905762\n",
            "==========================================\n",
            "Epoch:  300 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2568.0\n",
            "Mean Reward of that batch 428.0\n",
            "Average Reward of all training: 555.8290158730158\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.022603988647461\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  301 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2888.0\n",
            "Mean Reward of that batch 481.3333333333333\n",
            "Average Reward of all training: 555.58152191109\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.852416038513184\n",
            "==========================================\n",
            "Epoch:  302 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3732.0\n",
            "Mean Reward of that batch 1244.0\n",
            "Average Reward of all training: 557.8610532954904\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -4.606379508972168\n",
            "==========================================\n",
            "Epoch:  303 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 4200.0\n",
            "Mean Reward of that batch 2100.0\n",
            "Average Reward of all training: 562.9506207763634\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.9617786407470703\n",
            "==========================================\n",
            "Epoch:  304 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2768.0\n",
            "Mean Reward of that batch 692.0\n",
            "Average Reward of all training: 563.3751253132832\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 5.432161808013916\n",
            "==========================================\n",
            "Epoch:  305 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2472.0\n",
            "Mean Reward of that batch 412.0\n",
            "Average Reward of all training: 562.8788134270102\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.3242207765579224\n",
            "==========================================\n",
            "Epoch:  306 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2736.0\n",
            "Mean Reward of that batch 684.0\n",
            "Average Reward of all training: 563.2746342981637\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -6.934518814086914\n",
            "==========================================\n",
            "Epoch:  307 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3276.0\n",
            "Mean Reward of that batch 1092.0\n",
            "Average Reward of all training: 564.9968667597332\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -12.420336723327637\n",
            "==========================================\n",
            "Epoch:  308 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 4200.0\n",
            "Mean Reward of that batch 2100.0\n",
            "Average Reward of all training: 569.9806431663575\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 6.531651020050049\n",
            "==========================================\n",
            "Epoch:  309 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3916.0\n",
            "Mean Reward of that batch 1305.3333333333333\n",
            "Average Reward of all training: 572.3604253351825\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.1482630968093872\n",
            "==========================================\n",
            "Epoch:  310 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3024.0\n",
            "Mean Reward of that batch 1512.0\n",
            "Average Reward of all training: 575.3915207373271\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -12.430313110351562\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  311 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3000.0\n",
            "Mean Reward of that batch 1500.0\n",
            "Average Reward of all training: 578.364538355535\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.9519174098968506\n",
            "==========================================\n",
            "Epoch:  312 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2956.0\n",
            "Mean Reward of that batch 985.3333333333334\n",
            "Average Reward of all training: 579.6689255189256\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 7.853176593780518\n",
            "==========================================\n",
            "Epoch:  313 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2956.0\n",
            "Mean Reward of that batch 985.3333333333334\n",
            "Average Reward of all training: 580.9649779400578\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 10.0764799118042\n",
            "==========================================\n",
            "Epoch:  314 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2568.0\n",
            "Mean Reward of that batch 428.0\n",
            "Average Reward of all training: 580.4778283287837\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -63.466285705566406\n",
            "==========================================\n",
            "Epoch:  315 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3408.0\n",
            "Mean Reward of that batch 852.0\n",
            "Average Reward of all training: 581.3398034769464\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 9.562522888183594\n",
            "==========================================\n",
            "Epoch:  316 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2504.0\n",
            "Mean Reward of that batch 417.3333333333333\n",
            "Average Reward of all training: 580.8207956600361\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 5.313990592956543\n",
            "==========================================\n",
            "Epoch:  317 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3284.0\n",
            "Mean Reward of that batch 1094.6666666666667\n",
            "Average Reward of all training: 582.4417605528016\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -6.028435230255127\n",
            "==========================================\n",
            "Epoch:  318 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2924.0\n",
            "Mean Reward of that batch 584.8\n",
            "Average Reward of all training: 582.4491764001198\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.727198600769043\n",
            "==========================================\n",
            "Epoch:  319 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3368.0\n",
            "Mean Reward of that batch 842.0\n",
            "Average Reward of all training: 583.2628153455739\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.9275403022766113\n",
            "==========================================\n",
            "Epoch:  320 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3152.0\n",
            "Mean Reward of that batch 788.0\n",
            "Average Reward of all training: 583.902619047619\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.6616246104240417\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  321 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3148.0\n",
            "Mean Reward of that batch 1049.3333333333333\n",
            "Average Reward of all training: 585.3525589675122\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 5.664856910705566\n",
            "==========================================\n",
            "Epoch:  322 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3508.0\n",
            "Mean Reward of that batch 1169.3333333333333\n",
            "Average Reward of all training: 587.1661638568471\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.605432033538818\n",
            "==========================================\n",
            "Epoch:  323 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 3112.0\n",
            "Mean Reward of that batch 518.6666666666666\n",
            "Average Reward of all training: 586.9540911101283\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.559875011444092\n",
            "==========================================\n",
            "Epoch:  324 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3596.0\n",
            "Mean Reward of that batch 719.2\n",
            "Average Reward of all training: 587.3622574955907\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.038576722145080566\n",
            "==========================================\n",
            "Epoch:  325 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2824.0\n",
            "Mean Reward of that batch 470.6666666666667\n",
            "Average Reward of all training: 587.0031941391942\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.011559963226318\n",
            "==========================================\n",
            "Epoch:  326 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3280.0\n",
            "Mean Reward of that batch 820.0\n",
            "Average Reward of all training: 587.7179082676015\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.4441682994365692\n",
            "==========================================\n",
            "Epoch:  327 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2632.0\n",
            "Mean Reward of that batch 438.6666666666667\n",
            "Average Reward of all training: 587.2620940731033\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.725719690322876\n",
            "==========================================\n",
            "Epoch:  328 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2724.0\n",
            "Mean Reward of that batch 389.14285714285717\n",
            "Average Reward of all training: 586.6580720092916\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -7.106481075286865\n",
            "==========================================\n",
            "Epoch:  329 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3780.0\n",
            "Mean Reward of that batch 756.0\n",
            "Average Reward of all training: 587.1727891156463\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -7.4630560874938965\n",
            "==========================================\n",
            "Epoch:  330 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3084.0\n",
            "Mean Reward of that batch 1028.0\n",
            "Average Reward of all training: 588.5086291486292\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -3.85085129737854\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  331 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3696.0\n",
            "Mean Reward of that batch 1848.0\n",
            "Average Reward of all training: 592.3137390303554\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -3.784921407699585\n",
            "==========================================\n",
            "Epoch:  332 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3024.0\n",
            "Mean Reward of that batch 756.0\n",
            "Average Reward of all training: 592.8067699368904\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 5.962920188903809\n",
            "==========================================\n",
            "Epoch:  333 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3472.0\n",
            "Mean Reward of that batch 868.0\n",
            "Average Reward of all training: 593.633176033176\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 8.744169235229492\n",
            "==========================================\n",
            "Epoch:  334 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3472.0\n",
            "Mean Reward of that batch 868.0\n",
            "Average Reward of all training: 594.4546335899629\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 7.247914791107178\n",
            "==========================================\n",
            "Epoch:  335 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3124.0\n",
            "Mean Reward of that batch 1041.3333333333333\n",
            "Average Reward of all training: 595.7885998578536\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.672197103500366\n",
            "==========================================\n",
            "Epoch:  336 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 3048.0\n",
            "Mean Reward of that batch 508.0\n",
            "Average Reward of all training: 595.5273242630385\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.7329690456390381\n",
            "==========================================\n",
            "Epoch:  337 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3860.0\n",
            "Mean Reward of that batch 1286.6666666666667\n",
            "Average Reward of all training: 597.5781828458387\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.7714646458625793\n",
            "==========================================\n",
            "Epoch:  338 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2856.0\n",
            "Mean Reward of that batch 476.0\n",
            "Average Reward of all training: 597.2184840800226\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.2535436749458313\n",
            "==========================================\n",
            "Epoch:  339 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2868.0\n",
            "Mean Reward of that batch 956.0\n",
            "Average Reward of all training: 598.2768366343588\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.4406487047672272\n",
            "==========================================\n",
            "Epoch:  340 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3440.0\n",
            "Mean Reward of that batch 860.0\n",
            "Average Reward of all training: 599.0466106442577\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.4990345537662506\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  341 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2824.0\n",
            "Mean Reward of that batch 470.6666666666667\n",
            "Average Reward of all training: 598.6701298701299\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -3.886018753051758\n",
            "==========================================\n",
            "Epoch:  342 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3056.0\n",
            "Mean Reward of that batch 764.0\n",
            "Average Reward of all training: 599.1535505430243\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.184084177017212\n",
            "==========================================\n",
            "Epoch:  343 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2928.0\n",
            "Mean Reward of that batch 732.0\n",
            "Average Reward of all training: 599.5408579758434\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.012674331665039\n",
            "==========================================\n",
            "Epoch:  344 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2700.0\n",
            "Mean Reward of that batch 540.0\n",
            "Average Reward of all training: 599.3677740863787\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.145129919052124\n",
            "==========================================\n",
            "Epoch:  345 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2540.0\n",
            "Mean Reward of that batch 508.0\n",
            "Average Reward of all training: 599.1029399585922\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -5.9439544677734375\n",
            "==========================================\n",
            "Epoch:  346 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2668.0\n",
            "Mean Reward of that batch 533.6\n",
            "Average Reward of all training: 598.9136251032205\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.503138542175293\n",
            "==========================================\n",
            "Epoch:  347 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3120.0\n",
            "Mean Reward of that batch 1560.0\n",
            "Average Reward of all training: 601.6833264717991\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.3781895935535431\n",
            "==========================================\n",
            "Epoch:  348 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 4200.0\n",
            "Mean Reward of that batch 2100.0\n",
            "Average Reward of all training: 605.9888341543514\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.652803361415863\n",
            "==========================================\n",
            "Epoch:  349 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2632.0\n",
            "Mean Reward of that batch 438.6666666666667\n",
            "Average Reward of all training: 605.5094010096876\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.6093724370002747\n",
            "==========================================\n",
            "Epoch:  350 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3052.0\n",
            "Mean Reward of that batch 1017.3333333333334\n",
            "Average Reward of all training: 606.6860408163266\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.2790716886520386\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  351 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 4036.0\n",
            "Mean Reward of that batch 807.2\n",
            "Average Reward of all training: 607.2573056573057\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 4.391871929168701\n",
            "==========================================\n",
            "Epoch:  352 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4740.0\n",
            "Mean Reward of that batch 1580.0\n",
            "Average Reward of all training: 610.0207792207792\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.026910148561000824\n",
            "==========================================\n",
            "Epoch:  353 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3052.0\n",
            "Mean Reward of that batch 1017.3333333333334\n",
            "Average Reward of all training: 611.1746391474436\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.077592372894287\n",
            "==========================================\n",
            "Epoch:  354 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4396.0\n",
            "Mean Reward of that batch 1465.3333333333333\n",
            "Average Reward of all training: 613.5875168146354\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.6676577925682068\n",
            "==========================================\n",
            "Epoch:  355 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2864.0\n",
            "Mean Reward of that batch 716.0\n",
            "Average Reward of all training: 613.8760026827632\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.5679131150245667\n",
            "==========================================\n",
            "Epoch:  356 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3120.0\n",
            "Mean Reward of that batch 1560.0\n",
            "Average Reward of all training: 616.5336543606205\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.6611635088920593\n",
            "==========================================\n",
            "Epoch:  357 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2860.0\n",
            "Mean Reward of that batch 953.3333333333334\n",
            "Average Reward of all training: 617.4770708283313\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.07729572057723999\n",
            "==========================================\n",
            "Epoch:  358 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 2960.0\n",
            "Mean Reward of that batch 1480.0\n",
            "Average Reward of all training: 619.8863527533919\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.4377245604991913\n",
            "==========================================\n",
            "Epoch:  359 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3280.0\n",
            "Mean Reward of that batch 1640.0\n",
            "Average Reward of all training: 622.7278949462793\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.8131727576255798\n",
            "==========================================\n",
            "Epoch:  360 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 4200.0\n",
            "Mean Reward of that batch 2100.0\n",
            "Average Reward of all training: 626.8314285714285\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.761259913444519\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  361 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 2904.0\n",
            "Mean Reward of that batch 1452.0\n",
            "Average Reward of all training: 629.1172140878513\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.332712173461914\n",
            "==========================================\n",
            "Epoch:  362 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2704.0\n",
            "Mean Reward of that batch 676.0\n",
            "Average Reward of all training: 629.2467245461721\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.2236671447753906\n",
            "==========================================\n",
            "Epoch:  363 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 4200.0\n",
            "Mean Reward of that batch 2100.0\n",
            "Average Reward of all training: 633.2983864620229\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.8304780125617981\n",
            "==========================================\n",
            "Epoch:  364 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3760.0\n",
            "Mean Reward of that batch 1880.0\n",
            "Average Reward of all training: 636.7233908948195\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.525835394859314\n",
            "==========================================\n",
            "Epoch:  365 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3056.0\n",
            "Mean Reward of that batch 1528.0\n",
            "Average Reward of all training: 639.1652446183954\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.6239743828773499\n",
            "==========================================\n",
            "Epoch:  366 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 4164.0\n",
            "Mean Reward of that batch 832.8\n",
            "Average Reward of all training: 639.6943013270882\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 3.294971466064453\n",
            "==========================================\n",
            "Epoch:  367 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3144.0\n",
            "Mean Reward of that batch 786.0\n",
            "Average Reward of all training: 640.0929544569872\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.9356272220611572\n",
            "==========================================\n",
            "Epoch:  368 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4676.0\n",
            "Mean Reward of that batch 1558.6666666666667\n",
            "Average Reward of all training: 642.5890786749483\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -5.3411359786987305\n",
            "==========================================\n",
            "Epoch:  369 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 4104.0\n",
            "Mean Reward of that batch 1026.0\n",
            "Average Reward of all training: 643.6281326622791\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -3.2186849117279053\n",
            "==========================================\n",
            "Epoch:  370 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3468.0\n",
            "Mean Reward of that batch 1156.0\n",
            "Average Reward of all training: 645.0129214929216\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.9182920455932617\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  371 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 4552.0\n",
            "Mean Reward of that batch 1138.0\n",
            "Average Reward of all training: 646.3417276344501\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.919806718826294\n",
            "==========================================\n",
            "Epoch:  372 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3852.0\n",
            "Mean Reward of that batch 1284.0\n",
            "Average Reward of all training: 648.0558627752176\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.8258473873138428\n",
            "==========================================\n",
            "Epoch:  373 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 4200.0\n",
            "Mean Reward of that batch 2100.0\n",
            "Average Reward of all training: 651.9484744031662\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.7272790670394897\n",
            "==========================================\n",
            "Epoch:  374 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3116.0\n",
            "Mean Reward of that batch 1038.6666666666667\n",
            "Average Reward of all training: 652.9824802648332\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.3771584033966064\n",
            "==========================================\n",
            "Epoch:  375 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3120.0\n",
            "Mean Reward of that batch 780.0\n",
            "Average Reward of all training: 653.3211936507936\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.4040443897247314\n",
            "==========================================\n",
            "Epoch:  376 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2960.0\n",
            "Mean Reward of that batch 740.0\n",
            "Average Reward of all training: 653.551722391084\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.883599042892456\n",
            "==========================================\n",
            "Epoch:  377 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 4296.0\n",
            "Mean Reward of that batch 1074.0\n",
            "Average Reward of all training: 654.6669698117973\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.7838171720504761\n",
            "==========================================\n",
            "Epoch:  378 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4484.0\n",
            "Mean Reward of that batch 1494.6666666666667\n",
            "Average Reward of all training: 656.8891912320483\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.7623374462127686\n",
            "==========================================\n",
            "Epoch:  379 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2824.0\n",
            "Mean Reward of that batch 470.6666666666667\n",
            "Average Reward of all training: 656.397838924488\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.07751545310020447\n",
            "==========================================\n",
            "Epoch:  380 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2572.0\n",
            "Mean Reward of that batch 514.4\n",
            "Average Reward of all training: 656.0241604010024\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.041918992996216\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  381 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3724.0\n",
            "Mean Reward of that batch 744.8\n",
            "Average Reward of all training: 656.2571678540183\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.6472365856170654\n",
            "==========================================\n",
            "Epoch:  382 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2536.0\n",
            "Mean Reward of that batch 422.6666666666667\n",
            "Average Reward of all training: 655.6456743954126\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.1918541193008423\n",
            "==========================================\n",
            "Epoch:  383 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3440.0\n",
            "Mean Reward of that batch 860.0\n",
            "Average Reward of all training: 656.1792366032574\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.6773709058761597\n",
            "==========================================\n",
            "Epoch:  384 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3020.0\n",
            "Mean Reward of that batch 1006.6666666666666\n",
            "Average Reward of all training: 657.0919642857142\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.268921971321106\n",
            "==========================================\n",
            "Epoch:  385 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 4264.0\n",
            "Mean Reward of that batch 1066.0\n",
            "Average Reward of all training: 658.1540630797772\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.653853178024292\n",
            "==========================================\n",
            "Epoch:  386 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2860.0\n",
            "Mean Reward of that batch 572.0\n",
            "Average Reward of all training: 657.9308660251664\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.8966711759567261\n",
            "==========================================\n",
            "Epoch:  387 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4108.0\n",
            "Mean Reward of that batch 1369.3333333333333\n",
            "Average Reward of all training: 659.7691152946967\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.08987762778997421\n",
            "==========================================\n",
            "Epoch:  388 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 2992.0\n",
            "Mean Reward of that batch 1496.0\n",
            "Average Reward of all training: 661.9243495336278\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.023709354922175407\n",
            "==========================================\n",
            "Epoch:  389 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2860.0\n",
            "Mean Reward of that batch 953.3333333333334\n",
            "Average Reward of all training: 662.673472885298\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.604789674282074\n",
            "==========================================\n",
            "Epoch:  390 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 4016.0\n",
            "Mean Reward of that batch 2008.0\n",
            "Average Reward of all training: 666.123028083028\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.739281177520752\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  391 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3084.0\n",
            "Mean Reward of that batch 1028.0\n",
            "Average Reward of all training: 667.0485446352453\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.6540980339050293\n",
            "==========================================\n",
            "Epoch:  392 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 4768.0\n",
            "Mean Reward of that batch 1192.0\n",
            "Average Reward of all training: 668.3877065111758\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.3408850431442261\n",
            "==========================================\n",
            "Epoch:  393 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3596.0\n",
            "Mean Reward of that batch 1198.6666666666667\n",
            "Average Reward of all training: 669.7370168423603\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.10156555473804474\n",
            "==========================================\n",
            "Epoch:  394 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3508.0\n",
            "Mean Reward of that batch 1169.3333333333333\n",
            "Average Reward of all training: 671.0050277979211\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.5655338764190674\n",
            "==========================================\n",
            "Epoch:  395 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 4200.0\n",
            "Mean Reward of that batch 2100.0\n",
            "Average Reward of all training: 674.6227365883061\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.5380619764328003\n",
            "==========================================\n",
            "Epoch:  396 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3092.0\n",
            "Mean Reward of that batch 1030.6666666666667\n",
            "Average Reward of all training: 675.5218374218374\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.4438741207122803\n",
            "==========================================\n",
            "Epoch:  397 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2772.0\n",
            "Mean Reward of that batch 924.0\n",
            "Average Reward of all training: 676.14772700012\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.941195487976074\n",
            "==========================================\n",
            "Epoch:  398 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3344.0\n",
            "Mean Reward of that batch 1672.0\n",
            "Average Reward of all training: 678.6498683895669\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -2.086216926574707\n",
            "==========================================\n",
            "Epoch:  399 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2708.0\n",
            "Mean Reward of that batch 902.6666666666666\n",
            "Average Reward of all training: 679.2113139992839\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.7634813189506531\n",
            "==========================================\n",
            "Epoch:  400 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2892.0\n",
            "Mean Reward of that batch 578.4\n",
            "Average Reward of all training: 678.9592857142858\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.2557910978794098\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  401 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2928.0\n",
            "Mean Reward of that batch 732.0\n",
            "Average Reward of all training: 679.0915568222302\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.1224020719528198\n",
            "==========================================\n",
            "Epoch:  402 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3052.0\n",
            "Mean Reward of that batch 610.4\n",
            "Average Reward of all training: 678.9206823027719\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.3499429523944855\n",
            "==========================================\n",
            "Epoch:  403 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2636.0\n",
            "Mean Reward of that batch 527.2\n",
            "Average Reward of all training: 678.5442041829139\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.22000744938850403\n",
            "==========================================\n",
            "Epoch:  404 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2636.0\n",
            "Mean Reward of that batch 527.2\n",
            "Average Reward of all training: 678.1695898161244\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 2.2420854568481445\n",
            "==========================================\n",
            "Epoch:  405 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2732.0\n",
            "Mean Reward of that batch 546.4\n",
            "Average Reward of all training: 677.8442328042328\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 1.6759859323501587\n",
            "==========================================\n",
            "Epoch:  406 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2536.0\n",
            "Mean Reward of that batch 422.6666666666667\n",
            "Average Reward of all training: 677.2157166314801\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.23176825046539307\n",
            "==========================================\n",
            "Epoch:  407 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2640.0\n",
            "Mean Reward of that batch 660.0\n",
            "Average Reward of all training: 677.1734175734174\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.6067439913749695\n",
            "==========================================\n",
            "Epoch:  408 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2772.0\n",
            "Mean Reward of that batch 924.0\n",
            "Average Reward of all training: 677.7783846872081\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -3.005232095718384\n",
            "==========================================\n",
            "Epoch:  409 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3764.0\n",
            "Mean Reward of that batch 1254.6666666666667\n",
            "Average Reward of all training: 679.188869484224\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.5405844449996948\n",
            "==========================================\n",
            "Epoch:  410 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2740.0\n",
            "Mean Reward of that batch 913.3333333333334\n",
            "Average Reward of all training: 679.7599535423924\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.016132796183228493\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  411 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3636.0\n",
            "Mean Reward of that batch 1212.0\n",
            "Average Reward of all training: 681.0549414899779\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.9433771967887878\n",
            "==========================================\n",
            "Epoch:  412 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3820.0\n",
            "Mean Reward of that batch 1273.3333333333333\n",
            "Average Reward of all training: 682.4925104022191\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -0.5629111528396606\n",
            "==========================================\n",
            "Epoch:  413 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2796.0\n",
            "Mean Reward of that batch 559.2\n",
            "Average Reward of all training: 682.193981321342\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.48301976919174194\n",
            "==========================================\n",
            "Epoch:  414 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3192.0\n",
            "Mean Reward of that batch 1596.0\n",
            "Average Reward of all training: 684.4012422360247\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: 0.6791184544563293\n",
            "==========================================\n",
            "Epoch:  415 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3660.0\n",
            "Mean Reward of that batch 1220.0\n",
            "Average Reward of all training: 685.6918416523234\n",
            "Max reward for a batch so far: 4804.0\n",
            "Training Loss: -1.688023328781128\n",
            "==========================================\n",
            "Epoch:  416 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4964.0\n",
            "Mean Reward of that batch 1654.6666666666667\n",
            "Average Reward of all training: 688.0211080586081\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -0.8797840476036072\n",
            "==========================================\n",
            "Epoch:  417 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3188.0\n",
            "Mean Reward of that batch 1062.6666666666667\n",
            "Average Reward of all training: 688.9195386547904\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 1.0072959661483765\n",
            "==========================================\n",
            "Epoch:  418 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2704.0\n",
            "Mean Reward of that batch 676.0\n",
            "Average Reward of all training: 688.888630667578\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -1.213632583618164\n",
            "==========================================\n",
            "Epoch:  419 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2836.0\n",
            "Mean Reward of that batch 945.3333333333334\n",
            "Average Reward of all training: 689.5006705307421\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 0.6535160541534424\n",
            "==========================================\n",
            "Epoch:  420 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3792.0\n",
            "Mean Reward of that batch 948.0\n",
            "Average Reward of all training: 690.1161451247166\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -4.417914390563965\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  421 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3508.0\n",
            "Mean Reward of that batch 1169.3333333333333\n",
            "Average Reward of all training: 691.2544282321004\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 0.9047157168388367\n",
            "==========================================\n",
            "Epoch:  422 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3280.0\n",
            "Mean Reward of that batch 1640.0\n",
            "Average Reward of all training: 693.5026404874747\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 0.39602452516555786\n",
            "==========================================\n",
            "Epoch:  423 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3372.0\n",
            "Mean Reward of that batch 1124.0\n",
            "Average Reward of all training: 694.5203647416413\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 0.3870224356651306\n",
            "==========================================\n",
            "Epoch:  424 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4580.0\n",
            "Mean Reward of that batch 1526.6666666666667\n",
            "Average Reward of all training: 696.4829739442947\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 0.5430640578269958\n",
            "==========================================\n",
            "Epoch:  425 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3184.0\n",
            "Mean Reward of that batch 796.0\n",
            "Average Reward of all training: 696.7171316526611\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -1.2676712274551392\n",
            "==========================================\n",
            "Epoch:  426 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 4244.0\n",
            "Mean Reward of that batch 1414.6666666666667\n",
            "Average Reward of all training: 698.4024591996423\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -3.622763156890869\n",
            "==========================================\n",
            "Epoch:  427 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2756.0\n",
            "Mean Reward of that batch 393.7142857142857\n",
            "Average Reward of all training: 697.6889037582247\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -4.106334686279297\n",
            "==========================================\n",
            "Epoch:  428 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3220.0\n",
            "Mean Reward of that batch 1073.3333333333333\n",
            "Average Reward of all training: 698.566577659101\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 0.36612042784690857\n",
            "==========================================\n",
            "Epoch:  429 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3760.0\n",
            "Mean Reward of that batch 940.0\n",
            "Average Reward of all training: 699.1293595293595\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 0.9390506744384766\n",
            "==========================================\n",
            "Epoch:  430 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3220.0\n",
            "Mean Reward of that batch 1073.3333333333333\n",
            "Average Reward of all training: 699.9996013289036\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -9.422137260437012\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  431 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3344.0\n",
            "Mean Reward of that batch 836.0\n",
            "Average Reward of all training: 700.315147497514\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -2.4820330142974854\n",
            "==========================================\n",
            "Epoch:  432 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 4260.0\n",
            "Mean Reward of that batch 852.0\n",
            "Average Reward of all training: 700.6662698412699\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 1.4190196990966797\n",
            "==========================================\n",
            "Epoch:  433 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3344.0\n",
            "Mean Reward of that batch 836.0\n",
            "Average Reward of all training: 700.9788188716595\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 0.6227574944496155\n",
            "==========================================\n",
            "Epoch:  434 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 4612.0\n",
            "Mean Reward of that batch 922.4\n",
            "Average Reward of all training: 701.4890059249506\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -0.851223886013031\n",
            "==========================================\n",
            "Epoch:  435 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3572.0\n",
            "Mean Reward of that batch 1190.6666666666667\n",
            "Average Reward of all training: 702.6135522714834\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -0.7789576053619385\n",
            "==========================================\n",
            "Epoch:  436 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2704.0\n",
            "Mean Reward of that batch 676.0\n",
            "Average Reward of all training: 702.5525120139799\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 1.2898845672607422\n",
            "==========================================\n",
            "Epoch:  437 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2800.0\n",
            "Mean Reward of that batch 700.0\n",
            "Average Reward of all training: 702.5466710253896\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 3.4985480308532715\n",
            "==========================================\n",
            "Epoch:  438 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2732.0\n",
            "Mean Reward of that batch 546.4\n",
            "Average Reward of all training: 702.1901717764731\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 0.8304133415222168\n",
            "==========================================\n",
            "Epoch:  439 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3092.0\n",
            "Mean Reward of that batch 1030.6666666666667\n",
            "Average Reward of all training: 702.9384098058358\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -0.7235779762268066\n",
            "==========================================\n",
            "Epoch:  440 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3572.0\n",
            "Mean Reward of that batch 1190.6666666666667\n",
            "Average Reward of all training: 704.046883116883\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -3.4148998260498047\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  441 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2740.0\n",
            "Mean Reward of that batch 913.3333333333334\n",
            "Average Reward of all training: 704.5214555663536\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -0.7675896883010864\n",
            "==========================================\n",
            "Epoch:  442 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 3432.0\n",
            "Mean Reward of that batch 572.0\n",
            "Average Reward of all training: 704.2216332686921\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 5.263281345367432\n",
            "==========================================\n",
            "Epoch:  443 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2668.0\n",
            "Mean Reward of that batch 533.6\n",
            "Average Reward of all training: 703.8364828549929\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -6.8784613609313965\n",
            "==========================================\n",
            "Epoch:  444 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2896.0\n",
            "Mean Reward of that batch 724.0\n",
            "Average Reward of all training: 703.8818961818961\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -0.2130771428346634\n",
            "==========================================\n",
            "Epoch:  445 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3724.0\n",
            "Mean Reward of that batch 744.8\n",
            "Average Reward of all training: 703.973846976993\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -1.995896816253662\n",
            "==========================================\n",
            "Epoch:  446 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2832.0\n",
            "Mean Reward of that batch 708.0\n",
            "Average Reward of all training: 703.9828742259235\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 0.5830129981040955\n",
            "==========================================\n",
            "Epoch:  447 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3920.0\n",
            "Mean Reward of that batch 980.0\n",
            "Average Reward of all training: 704.6003622030466\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -13.925601959228516\n",
            "==========================================\n",
            "Epoch:  448 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2892.0\n",
            "Mean Reward of that batch 578.4\n",
            "Average Reward of all training: 704.3186649659864\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -10.181557655334473\n",
            "==========================================\n",
            "Epoch:  449 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2504.0\n",
            "Mean Reward of that batch 417.3333333333333\n",
            "Average Reward of all training: 703.6794994166931\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -3.7330844402313232\n",
            "==========================================\n",
            "Epoch:  450 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3120.0\n",
            "Mean Reward of that batch 780.0\n",
            "Average Reward of all training: 703.8491005291005\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 1.1703948974609375\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  451 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2704.0\n",
            "Mean Reward of that batch 676.0\n",
            "Average Reward of all training: 703.7873508605215\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -1.3477909564971924\n",
            "==========================================\n",
            "Epoch:  452 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3244.0\n",
            "Mean Reward of that batch 648.8\n",
            "Average Reward of all training: 703.6656974294142\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -3.7845675945281982\n",
            "==========================================\n",
            "Epoch:  453 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2600.0\n",
            "Mean Reward of that batch 433.3333333333333\n",
            "Average Reward of all training: 703.0689372437716\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -2.9013915061950684\n",
            "==========================================\n",
            "Epoch:  454 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3380.0\n",
            "Mean Reward of that batch 1126.6666666666667\n",
            "Average Reward of all training: 704.0019718900775\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 1.252271056175232\n",
            "==========================================\n",
            "Epoch:  455 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3316.0\n",
            "Mean Reward of that batch 1105.3333333333333\n",
            "Average Reward of all training: 704.8840188383045\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 8.090697288513184\n",
            "==========================================\n",
            "Epoch:  456 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 4324.0\n",
            "Mean Reward of that batch 864.8\n",
            "Average Reward of all training: 705.2347117794486\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 6.047611713409424\n",
            "==========================================\n",
            "Epoch:  457 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3372.0\n",
            "Mean Reward of that batch 674.4\n",
            "Average Reward of all training: 705.1672397624257\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -6.0418925285339355\n",
            "==========================================\n",
            "Epoch:  458 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2800.0\n",
            "Mean Reward of that batch 700.0\n",
            "Average Reward of all training: 705.1559575795383\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 2.585984230041504\n",
            "==========================================\n",
            "Epoch:  459 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2704.0\n",
            "Mean Reward of that batch 676.0\n",
            "Average Reward of all training: 705.0924369747898\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -4.4120635986328125\n",
            "==========================================\n",
            "Epoch:  460 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3752.0\n",
            "Mean Reward of that batch 938.0\n",
            "Average Reward of all training: 705.5987577639751\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 5.6691670417785645\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  461 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2832.0\n",
            "Mean Reward of that batch 708.0\n",
            "Average Reward of all training: 705.6039665323829\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 4.559360504150391\n",
            "==========================================\n",
            "Epoch:  462 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 4680.0\n",
            "Mean Reward of that batch 1170.0\n",
            "Average Reward of all training: 706.6091527520098\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -5.451582908630371\n",
            "==========================================\n",
            "Epoch:  463 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3280.0\n",
            "Mean Reward of that batch 820.0\n",
            "Average Reward of all training: 706.8540573896944\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -16.745006561279297\n",
            "==========================================\n",
            "Epoch:  464 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2860.0\n",
            "Mean Reward of that batch 953.3333333333334\n",
            "Average Reward of all training: 707.38526272578\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 5.0387492179870605\n",
            "==========================================\n",
            "Epoch:  465 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3852.0\n",
            "Mean Reward of that batch 770.4\n",
            "Average Reward of all training: 707.5207782898106\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -0.7138836979866028\n",
            "==========================================\n",
            "Epoch:  466 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2468.0\n",
            "Mean Reward of that batch 352.57142857142856\n",
            "Average Reward of all training: 706.7590844062947\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 7.375453948974609\n",
            "==========================================\n",
            "Epoch:  467 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 3692.0\n",
            "Mean Reward of that batch 738.4\n",
            "Average Reward of all training: 706.8268379728764\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -1.4600512981414795\n",
            "==========================================\n",
            "Epoch:  468 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2600.0\n",
            "Mean Reward of that batch 433.3333333333333\n",
            "Average Reward of all training: 706.2424501424501\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -3.158076524734497\n",
            "==========================================\n",
            "Epoch:  469 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3464.0\n",
            "Mean Reward of that batch 866.0\n",
            "Average Reward of all training: 706.5830845771144\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 4.853970050811768\n",
            "==========================================\n",
            "Epoch:  470 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3916.0\n",
            "Mean Reward of that batch 1305.3333333333333\n",
            "Average Reward of all training: 707.8570212765957\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -0.6560002565383911\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  471 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2992.0\n",
            "Mean Reward of that batch 748.0\n",
            "Average Reward of all training: 707.9422505307855\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -5.716123104095459\n",
            "==========================================\n",
            "Epoch:  472 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 3660.0\n",
            "Mean Reward of that batch 1220.0\n",
            "Average Reward of all training: 709.0271186440677\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 6.132138252258301\n",
            "==========================================\n",
            "Epoch:  473 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 4304.0\n",
            "Mean Reward of that batch 1076.0\n",
            "Average Reward of all training: 709.8029598308668\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -1.4812686443328857\n",
            "==========================================\n",
            "Epoch:  474 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2700.0\n",
            "Mean Reward of that batch 540.0\n",
            "Average Reward of all training: 709.4447257383966\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 0.544260561466217\n",
            "==========================================\n",
            "Epoch:  475 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 3184.0\n",
            "Mean Reward of that batch 1592.0\n",
            "Average Reward of all training: 711.3027368421052\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -14.752193450927734\n",
            "==========================================\n",
            "Epoch:  476 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2768.0\n",
            "Mean Reward of that batch 692.0\n",
            "Average Reward of all training: 711.2621848739495\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 4.847720146179199\n",
            "==========================================\n",
            "Epoch:  477 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 3272.0\n",
            "Mean Reward of that batch 545.3333333333334\n",
            "Average Reward of all training: 710.914325646401\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -9.36593246459961\n",
            "==========================================\n",
            "Epoch:  478 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2408.0\n",
            "Mean Reward of that batch 401.3333333333333\n",
            "Average Reward of all training: 710.2666666666667\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -5.559749603271484\n",
            "==========================================\n",
            "Epoch:  479 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2764.0\n",
            "Mean Reward of that batch 552.8\n",
            "Average Reward of all training: 709.9379262352122\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 4.495769023895264\n",
            "==========================================\n",
            "Epoch:  480 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 3048.0\n",
            "Mean Reward of that batch 508.0\n",
            "Average Reward of all training: 709.5172222222224\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -1.7850109338760376\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  481 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2504.0\n",
            "Mean Reward of that batch 417.3333333333333\n",
            "Average Reward of all training: 708.9097713097713\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -21.050912857055664\n",
            "==========================================\n",
            "Epoch:  482 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2408.0\n",
            "Mean Reward of that batch 401.3333333333333\n",
            "Average Reward of all training: 708.2716459197787\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -17.454864501953125\n",
            "==========================================\n",
            "Epoch:  483 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2664.0\n",
            "Mean Reward of that batch 444.0\n",
            "Average Reward of all training: 707.7244996549344\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -35.28024673461914\n",
            "==========================================\n",
            "Epoch:  484 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2892.0\n",
            "Mean Reward of that batch 578.4\n",
            "Average Reward of all training: 707.4573002754822\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -7.21636962890625\n",
            "==========================================\n",
            "Epoch:  485 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 3176.0\n",
            "Mean Reward of that batch 529.3333333333334\n",
            "Average Reward of all training: 707.0900343642611\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 5.11922550201416\n",
            "==========================================\n",
            "Epoch:  486 / 500\n",
            "-----------\n",
            "Number of training episodes: 7\n",
            "Total reward: 2436.0\n",
            "Mean Reward of that batch 348.0\n",
            "Average Reward of all training: 706.3511659807955\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -25.908235549926758\n",
            "==========================================\n",
            "Epoch:  487 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2568.0\n",
            "Mean Reward of that batch 428.0\n",
            "Average Reward of all training: 705.7796030116358\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -75.16023254394531\n",
            "==========================================\n",
            "Epoch:  488 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3376.0\n",
            "Mean Reward of that batch 844.0\n",
            "Average Reward of all training: 706.0628415300547\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -129.31813049316406\n",
            "==========================================\n",
            "Epoch:  489 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3312.0\n",
            "Mean Reward of that batch 828.0\n",
            "Average Reward of all training: 706.3122017723246\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -0.2659112215042114\n",
            "==========================================\n",
            "Epoch:  490 / 500\n",
            "-----------\n",
            "Number of training episodes: 3\n",
            "Total reward: 2900.0\n",
            "Mean Reward of that batch 966.6666666666666\n",
            "Average Reward of all training: 706.843537414966\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -0.36194151639938354\n",
            "Model saved\n",
            "==========================================\n",
            "Epoch:  491 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2768.0\n",
            "Mean Reward of that batch 692.0\n",
            "Average Reward of all training: 706.8133061778684\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -1.4887967109680176\n",
            "==========================================\n",
            "Epoch:  492 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3408.0\n",
            "Mean Reward of that batch 852.0\n",
            "Average Reward of all training: 707.1084010840109\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 2.8711209297180176\n",
            "==========================================\n",
            "Epoch:  493 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 3216.0\n",
            "Mean Reward of that batch 804.0\n",
            "Average Reward of all training: 707.3049357674105\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -3.646683692932129\n",
            "==========================================\n",
            "Epoch:  494 / 500\n",
            "-----------\n",
            "Number of training episodes: 2\n",
            "Total reward: 2968.0\n",
            "Mean Reward of that batch 1484.0\n",
            "Average Reward of all training: 708.8771929824562\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 0.46970459818840027\n",
            "==========================================\n",
            "Epoch:  495 / 500\n",
            "-----------\n",
            "Number of training episodes: 5\n",
            "Total reward: 2956.0\n",
            "Mean Reward of that batch 591.2\n",
            "Average Reward of all training: 708.6394612794612\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -16.872251510620117\n",
            "==========================================\n",
            "Epoch:  496 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2664.0\n",
            "Mean Reward of that batch 444.0\n",
            "Average Reward of all training: 708.1059139784946\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -28.428062438964844\n",
            "==========================================\n",
            "Epoch:  497 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 3176.0\n",
            "Mean Reward of that batch 529.3333333333334\n",
            "Average Reward of all training: 707.7462105969149\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 13.385193824768066\n",
            "==========================================\n",
            "Epoch:  498 / 500\n",
            "-----------\n",
            "Number of training episodes: 6\n",
            "Total reward: 2984.0\n",
            "Mean Reward of that batch 497.3333333333333\n",
            "Average Reward of all training: 707.3236947791165\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: -11.109652519226074\n",
            "==========================================\n",
            "Epoch:  499 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2768.0\n",
            "Mean Reward of that batch 692.0\n",
            "Average Reward of all training: 707.292985971944\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 2.6850178241729736\n",
            "==========================================\n",
            "Epoch:  500 / 500\n",
            "-----------\n",
            "Number of training episodes: 4\n",
            "Total reward: 2928.0\n",
            "Mean Reward of that batch 732.0\n",
            "Average Reward of all training: 707.3424\n",
            "Max reward for a batch so far: 4964.0\n",
            "Training Loss: 17.15376091003418\n",
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq2AhS9IEiEM",
        "colab_type": "text"
      },
      "source": [
        "## Watch our agent play the game üëÄ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvFvwpAurPmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "game, possible_actions = create_environment()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lra2WTzh6thw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "05276f54-ce01-4074-fa17-57b9906ae1ea"
      },
      "source": [
        "# Saver\n",
        "saver = tf.train.Saver()\n",
        "allframes = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  game, possible_actions = create_environment()\n",
        "\n",
        "  saver.restore(sess, \"./models/model.ckpt\")\n",
        "\n",
        "  game.init()\n",
        "  game.new_episode()\n",
        "  total_rewards = 0\n",
        "\n",
        "  state = game.get_state().screen_buffer\n",
        "  state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "\n",
        "  while not game.is_episode_finished():\n",
        "      allframes.append(game.get_state().screen_buffer)\n",
        "\n",
        "      # Run State Through Policy & Calculate Action\n",
        "      action_probability_distribution = sess.run(PGNetwork.action_distribution, \n",
        "                                                  feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
        "\n",
        "      # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\n",
        "      # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\n",
        "      #30% chance that we take action a2)\n",
        "      action = np.random.choice(range(action_probability_distribution.shape[1]), \n",
        "                                p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
        "      action = possible_actions[action]\n",
        "\n",
        "      # Perform action\n",
        "      reward = game.make_action(action)\n",
        "      done = game.is_episode_finished()\n",
        "      total_rewards += reward\n",
        "\n",
        "      if done:\n",
        "          print(\"Total Reward: \", reward)\n",
        "          break\n",
        "      else:\n",
        "          # If not done, the next_state become the current state\n",
        "          next_state = game.get_state().screen_buffer\n",
        "          next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "          state = next_state\n",
        "\n",
        "  game.close()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
            "Total Reward:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IleHislfBVz",
        "colab_type": "text"
      },
      "source": [
        "### Saving frame data to images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYwpYSFWi0CQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import glob\n",
        "!rm -rf frames\n",
        "!mkdir -p frames\n",
        "for idx, frame in enumerate(allframes):\n",
        "  cv2.imwrite('frames/{:05}.jpg'.format(idx), frame)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPOCQaj9o7fw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37ddb0f8-2bb6-4695-c4b7-83423444f137"
      },
      "source": [
        "from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "from PIL import Image\n",
        "\n",
        "fourcc = VideoWriter_fourcc(*'MJPG')\n",
        "outputfn = \"out.avi\"\n",
        "fps = 24 # reset manually\n",
        "\n",
        "filenames = glob.glob('frames/*.jpg')\n",
        "filenames = sorted(filenames)\n",
        "\n",
        "im = Image.open(filenames[0])\n",
        "vw = VideoWriter(outputfn, fourcc, fps, im.size)\n",
        "\n",
        "\n",
        "for im in filenames:\n",
        "  im = cv2.imread(im)\n",
        "  vw.write(im.astype(np.uint8))\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "vw.release()\n",
        "print(vw)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<VideoWriter 0x7f49b0542a50>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHN3VZDFSSrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Downlaod file in Colab\n",
        "from google.colab import files\n",
        "\n",
        "files.download('out.avi')"
      ],
      "execution_count": 76,
      "outputs": []
    }
  ]
}